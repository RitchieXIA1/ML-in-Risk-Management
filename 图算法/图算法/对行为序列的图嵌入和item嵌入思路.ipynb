{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e105574",
   "metadata": {},
   "source": [
    "# 构建图的一些思路"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0b509",
   "metadata": {},
   "source": [
    "用ip地址前三位，客户经理，地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3630ddae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph\n",
    "import pandas as pd\n",
    "import networkx\n",
    "import gc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import igraph\n",
    "import warnings\n",
    "import networkx as nx\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "from ge import Node2Vec#可能需要额外安装包\n",
    "\n",
    "\n",
    "#vocab = list(model.wv.vocab.keys())\n",
    "def get_emb(model,sentences,size=128):\n",
    "    \n",
    "    emb=np.zeros(size)\n",
    "    n=len(sentences)\n",
    "    for word in sentences:\n",
    "        try:\n",
    "            emb+=model.wv.word_vec(word)\n",
    "        except(KeyError):\n",
    "            n-=1\n",
    "    return emb/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713c9a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data_op[['user','ip_3']].dropna().drop_duplicates().groupby('user')['ip_3'].apply(list).reset_index()\n",
    "df.ip_3=df.ip_3.str.join(',')\n",
    "cv=CountVectorizer()\n",
    "results=cv.fit_transform(df.ip_3)\n",
    "#这一步统计的是每个id下，各ip分别的出现次数\n",
    "result=results*results.T#建立领接矩阵\n",
    "result[np.diag_indices_from(result)]=0#主对角线归0\n",
    "conn_indices = result.nonzero()#得到非0元素位置\n",
    "weights = result[conn_indices]#非0的元素\n",
    "weights=weights[0].tolist() ##注意一定要转list##############\n",
    "\n",
    "##########然后就可以转pandas了\n",
    "edges = zip(*conn_indices)\n",
    "G = igraph.Graph(edges=edges, directed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c4c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tempg = G.get_edgelist()\n",
    "g = networkx.Graph(G.get_edgelist()) \n",
    "del tempg;gc.collect()\n",
    "fig = plt.figure(figsize=(15,15))\n",
    "nx.draw(g)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "G.es['weight'] = weights[0]\n",
    "G.vs['user']=df.user#设置主键\n",
    "lpa=G.community_label_propagation()\n",
    "df['lpa']=lpa.membership\n",
    "df['pagerank']=G.pagerank(directed=False,weights='weight')\n",
    "louvain=G.community_multilevel(weights='weight') ##louvain社区的发现\n",
    "df['louvain']=louvain.membership\n",
    "df['degree']=G.degree()\n",
    "infomap=G.community_infomap(edge_weights='weight') #infomap社区的发现\n",
    "df['infomap']=infomap.membership\n",
    "df['closeness']=G.closeness()\n",
    "df['eccentricity']=G.eccentricity()#离心率\n",
    "df.to_csv('df_user_ip3.csv',index=False)\n",
    "\n",
    "#需要注意的是对两个社区发现算法发现出来的社区可以继续进行聚合操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3fbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=df.ip_3.str.split(',').values.tolist()\n",
    "start = datetime.datetime.now()\n",
    "#可以看做教科书级别的图嵌入和序列嵌入参数设置\n",
    "model = Word2Vec(sentences = words, \n",
    "                 iter = 50, \n",
    "                 min_count = 1,# 必要设置\n",
    "                 size = 128, \n",
    "                 workers = 4, \n",
    "                 sg = 1, \n",
    "                 hs = 0, \n",
    "                 negative = 5,#>0 则会采用negativesamping，用于设置多少个noise words\n",
    "                 window = 9999999)#表示当前词与预测词在一个句子中的最大距离是多少\n",
    "\n",
    "print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "model.save('ip_3_word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c806cfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average(sentence):\n",
    "    n=len(sentence)\n",
    "    embedding=np.zeros(200)\n",
    "    for word in sentence:\n",
    "        \n",
    "        try:\n",
    "            embedding+=model.wv[word]\n",
    "        except(KeyError):\n",
    "            n-=1\n",
    "    \n",
    "    return embedding/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a84eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把嵌入的向量作为特征拼接进原始数据\n",
    "sentences=[]\n",
    "for sentence in words:\n",
    "    sentences.append(get_average(sentence))\n",
    "sentences=pd.DataFrame(sentences)\n",
    "sentences.columns=['ip_3_emb_'+str(item) for item in range(128)]\n",
    "for col in sentences.columns:\n",
    "    df[col]=sentences[col].values\n",
    "df.to_csv('user_ip_3.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#把上面的代码进行集成\n",
    "def graph(data,cols=['user','ip_3']):\n",
    "    df=data[cols].dropna().drop_duplicates().groupby(cols[0])[cols[1]].apply(list).reset_index()#group成list\n",
    "    df[cols[1]]=df[cols[1]].str.join(',')#用逗号分割并做成str格式\n",
    "    cv=CountVectorizer(dtype=np.int8)\n",
    "    results=cv.fit_transform(df[cols[1]])\n",
    "    result=results.astype(np.int8).dot(results.astype(np.int8).T)\n",
    "    result[np.diag_indices_from(result)]=0\n",
    "    conn_indices = result.nonzero()\n",
    "    weights = result[conn_indices]\n",
    "    weights=weights[0].tolist() ##注意一定要转list##############\n",
    "\n",
    "    edges = zip(*conn_indices)\n",
    "    G = igraph.Graph(edges=edges, directed=False)\n",
    "    G.es['weight'] = weights[0]\n",
    "    G.vs['user']=df.user#这个地方是主键\n",
    "    lpa=G.community_label_propagation()\n",
    "    df[cols[1]+'_''lpa']=lpa.membership\n",
    "    df[cols[1]+'_''pagerank']=G.pagerank(directed=False,weights='weight')\n",
    "    louvain=G.community_multilevel(weights='weight') ##louvain社区的发现\n",
    "    df[cols[1]+'_''louvain']=louvain.membership\n",
    "    df[cols[1]+'_''degree']=G.degree()\n",
    "    words=df[cols[1]].str.split(',').values.tolist()\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "\n",
    "    model = Word2Vec(sentences = words, \n",
    "                     iter = 50, \n",
    "                     min_count = 1,\n",
    "                     size = 128, \n",
    "                     workers = 4, \n",
    "                     sg = 1, \n",
    "                     hs = 0, \n",
    "                     negative = 5, \n",
    "                     window = 9999999)\n",
    "\n",
    "    print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "    model.save(cols[1]+'_word2vec')\n",
    "    sentences=[]\n",
    "\n",
    "\n",
    "    vocab = list(model.wv.vocab.keys())\n",
    "    for sentence in words:\n",
    "        sentences.append(get_emb(model,sentence))\n",
    "    sentences=pd.DataFrame(sentences)\n",
    "    sentences.columns=[cols[1]+'_emb_'+str(item) for item in range(128)]\n",
    "    for col in sentences.columns:\n",
    "        df[col]=sentences[col].values\n",
    "    df.to_csv(cols[0]+'_'+cols[1]+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10207f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#这里用ge包的node2vec来快速实现\n",
    "def just_emb(data,cols=['user','ip_3']):\n",
    "    df=data[cols].dropna().drop_duplicates().groupby(cols[0])[cols[1]].apply(list).reset_index()\n",
    "    df[cols[1]]=df[cols[1]].str.join(',')\n",
    "    cv=CountVectorizer(dtype=np.int8)\n",
    "    results=cv.fit_transform(df[cols[1]])\n",
    "    result=[]\n",
    "    for i in range(results.shape[0]):\n",
    "        result.append(results[i]*results.T)\n",
    "   # result=results.astype(np.int8).dot(results.astype(np.int8).T)\n",
    "    result=np.concatenate(result)\n",
    "    result[np.diag_indices_from(result)]=0\n",
    "    conn_indices = result.nonzero()\n",
    "    weights = result[conn_indices]\n",
    "    weights=weights[0].tolist() ##注意一定要转list##############\n",
    "    edges = zip(*conn_indices)\n",
    "    \n",
    "    g=pd.DataFrame(edges)\n",
    "    g.columns=['source','target']\n",
    "    g['weight']=weights[0]\n",
    "    G = nx.from_pandas_edgelist(g,source= 'source',target= 'target', edge_attr='weight')\n",
    "\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    model = Node2Vec(G, walk_length = 10, num_walks = 5,p = 0.25, q = 4, workers = 4,use_rejection_sampling=1)#init model\n",
    "    model.sentences=pd.DataFrame(model.sentences).astype(str).values.tolist()\n",
    "    model.train(window_size=5,iter=20)\n",
    "\n",
    "    print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "#    model.save(cols[1]+'_word2vec')\n",
    "    sentences=[]\n",
    "\n",
    "\n",
    "    for sentence in df[cols[1]].str.split(',').values.tolist():\n",
    "        sentences.append(get_emb(model.w2v_model,sentence))\n",
    "    sentences=pd.DataFrame(sentences)\n",
    "    sentences.columns=[cols[1]+'_net_emb_'+str(item) for item in range(128)]\n",
    "    for col in sentences.columns:\n",
    "        df[col]=sentences[col].values\n",
    "    df.to_csv(cols[0]+'_'+cols[1]+'_net.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e6b60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def just_emb(data,cols=['user','ip_3']):\n",
    "    df=data[cols].dropna().drop_duplicates().groupby(cols[0])[cols[1]].apply(list).reset_index()\n",
    "    df[cols[1]]=df[cols[1]].str.join(',')\n",
    "    cv=CountVectorizer(dtype=np.int8)\n",
    "    results=cv.fit_transform(df[cols[1]])\n",
    "    result=[]\n",
    "    for i in range(results.shape[0]):\n",
    "        result.append(results[i]*results.T)\n",
    "   # result=results.astype(np.int8).dot(results.astype(np.int8).T)\n",
    "    result=np.concatenate(result)\n",
    "    result[np.diag_indices_from(result)]=0\n",
    "    conn_indices = result.nonzero()\n",
    "    weights = result[conn_indices]\n",
    "    weights=weights[0].tolist() ##注意一定要转list##############\n",
    "    edges = zip(*conn_indices)\n",
    "    \n",
    "    g=pd.DataFrame(edges)\n",
    "    g.columns=['source','target']\n",
    "    g['weight']=weights[0]\n",
    "    G = nx.from_pandas_edgelist(g,source= 'source',target= 'target', edge_attr='weight')\n",
    "\n",
    "    \n",
    "    start = datetime.datetime.now()\n",
    "    model = Node2Vec(G, walk_length = 10, num_walks = 5,p = 0.25, q = 4, workers = 4,use_rejection_sampling=1)#init model\n",
    "    model.sentences=pd.DataFrame(model.sentences).astype(str).values.tolist()\n",
    "    model.train(window_size=5,iter=20)\n",
    "\n",
    "    print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "#    model.save(cols[1]+'_word2vec')\n",
    "    sentences=[]\n",
    "\n",
    "\n",
    "    for sentence in df[cols[1]].str.split(',').values.tolist():\n",
    "        sentences.append(get_emb(model.w2v_model,sentence))\n",
    "    sentences=pd.DataFrame(sentences)\n",
    "    sentences.columns=[cols[1]+'_net_emb_'+str(item) for item in range(128)]\n",
    "    for col in sentences.columns:\n",
    "        df[col]=sentences[col].values\n",
    "    df.to_csv(cols[0]+'_'+cols[1]+'_net.csv',index=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f94547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-10T09:25:30.572160Z",
     "start_time": "2022-03-10T09:25:30.195139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bird', 'cat', 'dog', 'fish']\n",
      "{'dog': 2, 'cat': 1, 'fish': 3, 'bird': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird'] # “dog cat fish” 为输入列表元素,即代表一个文章的字符串\n",
    "cv = CountVectorizer()#创建词袋数据结构\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "#上述代码等价于下面两行\n",
    "#cv.fit(texts)\n",
    "#cv_fit=cv.transform(texts)\n",
    "\n",
    "print(cv.get_feature_names())    #['bird', 'cat', 'dog', 'fish'] 列表形式呈现文章生成的词典\n",
    "\n",
    "print(cv.vocabulary_\t)              # {‘dog’:2,'cat':1,'fish':3,'bird':0} 字典形式呈现，key：词，value:词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b562acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
