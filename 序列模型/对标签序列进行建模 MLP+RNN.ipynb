{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "536d1e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Flatten, Embedding, Dropout, PReLU,ReLU\n",
    "from keras.layers import Bidirectional, SpatialDropout1D, CuDNNGRU,CuDNNLSTM, Conv1D,Conv2D,MaxPool2D,Reshape\n",
    "from keras.layers import GlobalAvgPool1D, GlobalMaxPool1D, concatenate,GlobalMaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.regularizers import l2,l1\n",
    "from keras.layers.normalization import batch_normalization\n",
    "from keras.layers.core import Flatten\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Optimizer\n",
    "from keras import backend as K, initializers, regularizers, constraints\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation,BatchNormalization\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b2e9e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_v3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_shape=(10102,)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))   \n",
    " #   model.add(BatchNormalization())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))    \n",
    "#    model.add(BatchNormalization())\n",
    "#\n",
    "    model.add(Dense(6))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='Nadam',\n",
    "                  metrics=['auc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdeb3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_dict = dict(zip(df2.appid.unique().tolist(),np.arange(len(df2.appid.unique()))))\n",
    "x_train = [[x for x in apps if  x in app_dict] for apps in x_train.app_list]\n",
    "x_test = [[x for x in apps if  x in app_dict] for apps in x_test.app_list]\n",
    "x_train = [\" \".join(app) for app in x_train]\n",
    "x_test = [\" \".join(app) for app in x_test]\n",
    "c_vec1 = CountVectorizer(lowercase=False,ngram_range=(1,1),dtype=np.int8)\n",
    "c_vec1.fit(x_train + x_test)\n",
    "x_train = c_vec1.transform(x_train).toarray()\n",
    "x_test = c_vec1.transform(x_test).toarray()\n",
    "gc.collect()\n",
    "train.drop(['uId','age_group'],axis=1,inplace=True)\n",
    "test.drop('uId',axis=1,inplace=True)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "train = train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test = test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(pd.concat([train,test],axis=0))\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train = memory_preprocess._memory_process(pd.DataFrame(train))\n",
    "test = memory_preprocess._memory_process(pd.DataFrame(test))\n",
    "gc.collect()\n",
    "\n",
    "x_train = np.hstack((x_train,train.values))\n",
    "x_test = np.hstack((x_test,test.values))\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=False)\n",
    "y_test = np.zeros((x_test.shape[0],6))\n",
    "y_val = np.zeros((x_train.shape[0],6))\n",
    "for i, (train_index, valid_index) in enumerate(kfold.split(x_train, np.argmax(y_train,axis=1))):\n",
    "    X_train, X_val, Y_train, Y_val = x_train[train_index],x_train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "    filepath=\"weights_best2.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    model = mlp_v3()\n",
    "    if i == 0:print(model.summary()) \n",
    "    model.fit(X_train, Y_train, batch_size=128, epochs=5, validation_data=(X_val, Y_val), verbose=1, callbacks=callbacks, \n",
    "             )\n",
    "    model.load_weights(filepath)\n",
    "\n",
    "    \n",
    "    y_val[valid_index] = model.predict(X_val, batch_size=128, verbose=1)\n",
    "    y_test +=  np.array(model.predict(x_test, batch_size=128, verbose=1))/5\n",
    "    \n",
    "y_val = pd.DataFrame(y_val,index=train_uId)\n",
    "y_val.to_csv('../../data/prob_file/act_all_train_mlp.csv')\n",
    "y_test = pd.DataFrame(y_test,index=test_uId)\n",
    "y_test.to_csv('../../data/prob_file/act_all_test_mlp.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4aa5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RnnVersion1( n_recurrent=50, n_filters=30, dropout_rate=0.2, l2_penalty=0.0001,n_capsule = 10, n_routings = 5, capsule_dim = 16):\n",
    "    K.clear_session()\n",
    "    def conv_block(x, n, kernel_size):\n",
    "        x = Conv1D(n, kernel_size, activation='relu') (x)\n",
    "        x = Conv1D(n_filters, kernel_size, activation='relu') (x)\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAveragePooling1D()(x)\n",
    "        x_max = GlobalMaxPooling1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])  \n",
    "    def att_max_avg_pooling(x):\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAveragePooling1D()(x)\n",
    "        x_max = GlobalMaxPooling1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])\n",
    "\n",
    "    inputs = Input(shape=(100,))\n",
    "    emb = Embedding(9399, 300,  trainable=True)(inputs)\n",
    "\n",
    "    # model 0\n",
    "    x0 = BatchNormalization()(emb)\n",
    "    x0 = SpatialDropout1D(dropout_rate)(x0)\n",
    "    \n",
    "    x0 = Bidirectional(\n",
    "        CuDNNGRU(n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x0)\n",
    "    x0 = Conv1D(n_filters, kernel_size=3)(x0)\n",
    "    x0 = PReLU()(x0)\n",
    "#     x0 = Dropout(dropout_rate)(x0)\n",
    "    x0 = att_max_avg_pooling(x0)\n",
    "\n",
    "    # model 1\n",
    "    x1 = SpatialDropout1D(dropout_rate)(emb)\n",
    "    x1 = Bidirectional(\n",
    "        CuDNNGRU(2*n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x1)\n",
    "    x1 = Conv1D(2*n_filters, kernel_size=2)(x1)\n",
    "    x1 = PReLU()(x1)\n",
    "#     x1 = Dropout(dropout_rate)(x1)\n",
    "    x1 = att_max_avg_pooling(x1)\n",
    "\n",
    "    x = concatenate([x0, x1],name='concatenate')\n",
    "    \n",
    "    fc = Dense(128, activation='relu')(x)\n",
    "    outputs = Dense(6, activation='softmax')(fc)#   , kernel_regularizer=l2(l2_penalty), activity_regularizer=l2(l2_penalty)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='Nadam',metrics =['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23adaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RnnVersion2(n_recurrent=50, n_dense=50, word_embedding_matrix= None, n_filters=50,dropout_rate=0.2, l2_penalty=0.0001,\n",
    "                n_capsule = 10, n_routings = 5, capsule_dim = 16,max_len = 170, emb_size = 21099):\n",
    "    K.clear_session()\n",
    "    \n",
    "    def conv_block(x, n, kernel_size):\n",
    "        x = Conv1D(n, kernel_size, activation='relu') (x)\n",
    "        x = Conv1D(n_filters, kernel_size, activation='relu') (x)\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAvgPool1D()(x)\n",
    "        x_max = GlobalMaxPool1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])   \n",
    "    def att_max_avg_pooling(x):\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAvgPool1D()(x)\n",
    "        x_max = GlobalMaxPool1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])\n",
    "\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    emb = Embedding(emb_size, 300,trainable=True)(inputs)\n",
    "\n",
    "    # model 0\n",
    "    x0 = SpatialDropout1D(dropout_rate)(emb)\n",
    "    s0 = Bidirectional(\n",
    "        CuDNNGRU(2*n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x0)\n",
    "    x0 = att_max_avg_pooling(s0)\n",
    "\n",
    "    # model 1\n",
    "    x1 = SpatialDropout1D(dropout_rate)(emb)\n",
    "    s1 = Bidirectional(\n",
    "        CuDNNGRU(2*n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x1)\n",
    "    x1 = att_max_avg_pooling(s1)\n",
    "    \n",
    "    # combine sequence output\n",
    "    x = concatenate([s0, s1])\n",
    "#     x = att_max_avg_pooling(x)\n",
    "    x = Bidirectional(\n",
    "        CuDNNGRU(n_recurrent, return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x)\n",
    "    x = att_max_avg_pooling(x)\n",
    "    \n",
    "\n",
    "\n",
    "    # combine it all\n",
    "    x = concatenate([x,x0, x1],name = 'concatenate')\n",
    "\n",
    "    outputs = Dense(6, activation='softmax')(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam',metrics =['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68321805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RnnVersion3(n_recurrent=50, n_dense=50, word_embedding_matrix= None, n_filters=50,dropout_rate=0.2, l2_penalty=0.0001,\n",
    "               n_capsule = 10, n_routings = 5, capsule_dim = 16,):\n",
    "    K.clear_session()\n",
    "    \n",
    "    def conv_block(x, n, kernel_size):\n",
    "        x = Conv1D(n, kernel_size, activation='relu') (x)\n",
    "        x = Conv1D(n_filters, kernel_size, activation='relu') (x)\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAvgPool1D()(x)\n",
    "        x_max = GlobalMaxPool1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])   \n",
    "    def att_max_avg_pooling(x):\n",
    "        x_att = AttentionWithContext()(x)\n",
    "        x_avg = GlobalAvgPool1D()(x)\n",
    "        x_max = GlobalMaxPool1D()(x)\n",
    "        return concatenate([x_att, x_avg, x_max])\n",
    "\n",
    "    input1_= Input(shape=(170, ), name='input1')\n",
    "    input2_ = Input(shape=(433, ), name='input2')\n",
    "    emb = Embedding(21099, 300,trainable=True)(input1_)\n",
    "\n",
    "    # model 0\n",
    "    x0 = SpatialDropout1D(dropout_rate)(emb)\n",
    "    s0 = Bidirectional(\n",
    "        CuDNNGRU(2*n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x0)\n",
    "    x0 = att_max_avg_pooling(s0)\n",
    "\n",
    "    # model 1\n",
    "    x1 = SpatialDropout1D(dropout_rate)(emb)\n",
    "    s1 = Bidirectional(\n",
    "        CuDNNGRU(2*n_recurrent, return_sequences=True,\n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x1)\n",
    "    x1 = att_max_avg_pooling(s1)\n",
    "    \n",
    "    # combine sequence output\n",
    "    x = concatenate([s0, s1])\n",
    "#     x = att_max_avg_pooling(x)\n",
    "    x = Bidirectional(\n",
    "        CuDNNGRU(n_recurrent, return_sequences=True, \n",
    "                 kernel_regularizer=l2(l2_penalty),\n",
    "                 recurrent_regularizer=l2(l2_penalty)))(x)\n",
    "    x = att_max_avg_pooling(x)\n",
    "    \n",
    "\n",
    "    # combine it all\n",
    "    x = concatenate([x,x0, x1,input2_],name = 'concatenate')\n",
    "    \n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    \n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(dropout_rate)(x)       \n",
    " #   fc = Dense(120, activation='relu')(x)\n",
    "    outputs = Dense(6, activation='softmax')(x)\n",
    "    model = Model(inputs=[input1_,input2_], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='nadam',metrics =['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05374c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBest(Callback):\n",
    "    \"\"\"Get the best model at the end of training.\n",
    "\t# Arguments\n",
    "        monitor: quantity to monitor.\n",
    "        verbose: verbosity mode, 0 or 1.\n",
    "        mode: one of {auto, min, max}.\n",
    "            The decision\n",
    "            to overwrite the current stored weights is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc. In `auto` mode, the direction is\n",
    "            automatically inferred from the name of the monitored quantity.\n",
    "        period: Interval (number of epochs) between checkpoints.\n",
    "\t# Example\n",
    "\t\tcallbacks = [GetBest(monitor='val_acc', verbose=1, mode='max')]\n",
    "\t\tmode.fit(X, y, validation_data=(X_eval, Y_eval),\n",
    "                 callbacks=callbacks)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, monitor='val_loss', verbose=0,\n",
    "                 mode='auto', period=1):\n",
    "        super(GetBest, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.period = period\n",
    "        self.best_epochs = 0\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('GetBest mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "                \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            #filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is None:\n",
    "                warnings.warn('Can pick best model only with %s available, '\n",
    "                              'skipping.' % (self.monitor), RuntimeWarning)\n",
    "            else:\n",
    "                if self.monitor_op(current, self.best):\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                              ' storing weights.'\n",
    "                              % (epoch + 1, self.monitor, self.best,\n",
    "                                 current))\n",
    "                    self.best = current\n",
    "                    self.best_epochs = epoch + 1\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s did not improve' %\n",
    "                              (epoch + 1, self.monitor))            \n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0:\n",
    "            print('Using epoch %05d with %s: %0.5f' % (self.best_epochs, self.monitor,\n",
    "                                                       self.best))\n",
    "        self.model.set_weights(self.best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29702f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "app_list = sequence.pad_sequences(app_list, maxlen=170)\n",
    "app_test = sequence.pad_sequences(app_test, maxlen=170)\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "train = train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "test = test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(pd.concat([train,test],axis=0))\n",
    "train = scaler.transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "train = memory_preprocess._memory_process(pd.DataFrame(train))\n",
    "test = memory_preprocess._memory_process(pd.DataFrame(test))\n",
    "gc.collect()\n",
    "train = train.values\n",
    "test = test.values\n",
    "from keras.utils.np_utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=None)\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "kfold = StratifiedKFold(n_splits=5, random_state=10, shuffle=False)\n",
    "y_testb = np.zeros((x_test.shape[0],6))\n",
    "y_valb = np.zeros((x_train.shape[0],6))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fbee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (train_index, valid_index) in enumerate(kfold.split(app_list, np.argmax(y_train,axis=1))):\n",
    "    X_train1, X_val1, X_train2, X_val2,Y_train, Y_val = app_list[train_index],app_list[valid_index], train[train_index], train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "    filepath=\"weights_best5.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=1, min_lr=0.0001, verbose=2)\n",
    "    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n",
    "    callbacks = [checkpoint, reduce_lr]\n",
    "    model = RnnVersion3()\n",
    "    model.fit([X_train1,X_train2], Y_train, batch_size=128, epochs=4, validation_data=([X_val1,X_val2], Y_val), verbose=1, callbacks=callbacks, \n",
    "             )\n",
    "    model.load_weights(filepath)\n",
    "    \n",
    "    y_valb[valid_index] = model.predict([X_val1,X_val2], batch_size=128, verbose=1)\n",
    "    y_testb +=  np.array(model.predict([app_test,test], batch_size=128, verbose=1))/5    \n",
    "    \n",
    "y_valb = pd.DataFrame(y_valb,index=train_uId)\n",
    "y_valb.to_csv('../../data/prob_file/act_use_all_rnn_train_v2.csv')\n",
    "y_testb = pd.DataFrame(y_testb,index=test_uId)\n",
    "y_testb.to_csv('../../data/prob_file/act_use_all_rnn_test_v2.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbbf8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523f8b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
