{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b5cb06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:40:47.182880Z",
     "start_time": "2022-02-14T09:40:46.451247Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "import gc\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    " \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "067e89eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:42:50.724771Z",
     "start_time": "2022-02-14T09:42:35.298822Z"
    }
   },
   "outputs": [],
   "source": [
    "#读取数据集\n",
    "\n",
    "test_data = pd.read_csv('test_format1.csv')\n",
    "train_data = pd.read_csv('train_format1.csv')\n",
    "\n",
    "user_info = pd.read_csv('user_info_format1.csv')\n",
    "user_log = pd.read_csv('user_log_format1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5175ae9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:43:24.652219Z",
     "start_time": "2022-02-14T09:43:24.645207Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_csv(file_name, num_rows):\n",
    "    return pd.read_csv(file_name, nrows=num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e434a01c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:43:06.765504Z",
     "start_time": "2022-02-14T09:43:06.754502Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce memory\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "                    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d9f3212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:46:58.287049Z",
     "start_time": "2022-02-14T09:46:57.437858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage after optimization is: 1.74 MB\n",
      "Decreased by 70.8%\n",
      "Memory usage after optimization is: 3.49 MB\n",
      "Decreased by 41.7%\n",
      "Memory usage after optimization is: 3.24 MB\n",
      "Decreased by 66.7%\n",
      "Memory usage after optimization is: 32.43 MB\n",
      "Decreased by 69.6%\n"
     ]
    }
   ],
   "source": [
    "num_rows = None\n",
    "num_rows = 200 * 10000 # 1000条测试代码使用\n",
    "# num_rows = 1000\n",
    "\n",
    "train_file = 'train_format1.csv'\n",
    "test_file = 'test_format1.csv'\n",
    "\n",
    "user_info_file = 'user_info_format1.csv'\n",
    "user_log_file = 'user_log_format1.csv'\n",
    "\n",
    "train_data = reduce_mem_usage(read_csv(train_file, num_rows))\n",
    "test_data = reduce_mem_usage(read_csv(test_file, num_rows))\n",
    "\n",
    "user_info = reduce_mem_usage(read_csv(user_info_file, num_rows))\n",
    "user_log = reduce_mem_usage(read_csv(user_log_file, num_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8acf1ec6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:43:58.501946Z",
     "start_time": "2022-02-14T09:43:58.482942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 260864 entries, 0 to 260863\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype\n",
      "---  ------       --------------   -----\n",
      " 0   user_id      260864 non-null  int32\n",
      " 1   merchant_id  260864 non-null  int16\n",
      " 2   label        260864 non-null  int8 \n",
      "dtypes: int16(1), int32(1), int8(1)\n",
      "memory usage: 1.7 MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f113929c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:44:04.705658Z",
     "start_time": "2022-02-14T09:44:04.685654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 261477 entries, 0 to 261476\n",
      "Data columns (total 3 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   user_id      261477 non-null  int32  \n",
      " 1   merchant_id  261477 non-null  int16  \n",
      " 2   prob         0 non-null       float64\n",
      "dtypes: float64(1), int16(1), int32(1)\n",
      "memory usage: 3.5 MB\n"
     ]
    }
   ],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a7278bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:44:09.824272Z",
     "start_time": "2022-02-14T09:44:09.809269Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424170 entries, 0 to 424169\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count   Dtype  \n",
      "---  ------     --------------   -----  \n",
      " 0   user_id    424170 non-null  int32  \n",
      " 1   age_range  421953 non-null  float16\n",
      " 2   gender     417734 non-null  float16\n",
      "dtypes: float16(2), int32(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "user_info.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6227fc6b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:44:14.762954Z",
     "start_time": "2022-02-14T09:44:14.751484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Data columns (total 7 columns):\n",
      " #   Column       Dtype  \n",
      "---  ------       -----  \n",
      " 0   user_id      int32  \n",
      " 1   item_id      int32  \n",
      " 2   cat_id       int16  \n",
      " 3   seller_id    int16  \n",
      " 4   brand_id     float16\n",
      " 5   time_stamp   int16  \n",
      " 6   action_type  int8   \n",
      "dtypes: float16(1), int16(3), int32(2), int8(1)\n",
      "memory usage: 32.4 MB\n"
     ]
    }
   ],
   "source": [
    "user_log.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93383dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:01.765306Z",
     "start_time": "2022-02-14T09:47:01.575254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_data['prob']\n",
    "all_data = train_data.append(test_data)\n",
    "all_data = all_data.merge(user_info,on=['user_id'],how='left')\n",
    "del train_data, test_data, user_info\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee00542d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:05.081144Z",
     "start_time": "2022-02-14T09:47:05.065982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>34176</td>\n",
       "      <td>3906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34176</td>\n",
       "      <td>121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34176</td>\n",
       "      <td>4356</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34176</td>\n",
       "      <td>2217</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230784</td>\n",
       "      <td>4818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender\n",
       "0    34176         3906    0.0        6.0     0.0\n",
       "1    34176          121    0.0        6.0     0.0\n",
       "2    34176         4356    1.0        6.0     0.0\n",
       "3    34176         2217    0.0        6.0     0.0\n",
       "4   230784         4818    0.0        0.0     0.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72b18624",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:06.778061Z",
     "start_time": "2022-02-14T09:47:06.492685Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "按时间排序\n",
    "\"\"\"\n",
    "user_log = user_log.sort_values(['user_id','time_stamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "138fa148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:07.280691Z",
     "start_time": "2022-02-14T09:47:07.262682Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>time_stamp</th>\n",
       "      <th>action_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>61975</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61976</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61977</th>\n",
       "      <td>16</td>\n",
       "      <td>980982</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61978</th>\n",
       "      <td>16</td>\n",
       "      <td>962763</td>\n",
       "      <td>19</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61979</th>\n",
       "      <td>16</td>\n",
       "      <td>391126</td>\n",
       "      <td>437</td>\n",
       "      <td>650</td>\n",
       "      <td>4276.0</td>\n",
       "      <td>914</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  cat_id  seller_id  brand_id  time_stamp  action_type\n",
       "61975       16   980982     437        650    4276.0         914            0\n",
       "61976       16   980982     437        650    4276.0         914            0\n",
       "61977       16   980982     437        650    4276.0         914            0\n",
       "61978       16   962763      19        650    4276.0         914            0\n",
       "61979       16   391126     437        650    4276.0         914            0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02a6df71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:09.626103Z",
     "start_time": "2022-02-14T09:47:09.609102Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "合并数据\n",
    "\"\"\"\n",
    "list_join_func = lambda x: \" \".join([str(i) for i in x])\n",
    "\n",
    "\n",
    "agg_dict = {\n",
    "            'item_id' : list_join_func,\t\n",
    "            'cat_id' : list_join_func,\n",
    "            'seller_id' : list_join_func,\n",
    "            'brand_id' : list_join_func,\n",
    "            'time_stamp' : list_join_func,\n",
    "            'action_type' : list_join_func\n",
    "        }\n",
    "\n",
    "rename_dict = {\n",
    "            'item_id' : 'item_path',\n",
    "            'cat_id' : 'cat_path',\n",
    "            'seller_id' : 'seller_path',\n",
    "            'brand_id' : 'brand_path',\n",
    "            'time_stamp' : 'time_stamp_path',\n",
    "            'action_type' : 'action_type_path'\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bede511f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:23.361132Z",
     "start_time": "2022-02-14T09:47:20.694272Z"
    }
   },
   "outputs": [],
   "source": [
    "#相当于是客户的行为顺序，可以换成还款金额顺序和用款金额顺序，余额顺序之类的\n",
    "user_log_path = user_log.groupby('user_id').agg(agg_dict).reset_index().rename(columns=rename_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c990b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:47:42.999810Z",
     "start_time": "2022-02-14T09:47:42.984808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>980982 980982 980982 962763 391126 827174 6731...</td>\n",
       "      <td>437 437 437 19 437 437 437 437 895 19 437 437 ...</td>\n",
       "      <td>650 650 650 650 650 650 650 650 3948 650 650 6...</td>\n",
       "      <td>4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...</td>\n",
       "      <td>914 914 914 914 914 914 914 914 914 914 914 91...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>388018 388018 88673 88673 88673 88673 846066 5...</td>\n",
       "      <td>949 949 614 614 614 614 420 1401 948 948 513 1...</td>\n",
       "      <td>2772 2772 4066 4066 4066 4066 4951 4951 2872 2...</td>\n",
       "      <td>2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...</td>\n",
       "      <td>710 710 711 711 711 711 908 908 1105 1105 1105...</td>\n",
       "      <td>0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>60215 1004605 60215 60215 60215 60215 628525 5...</td>\n",
       "      <td>1308 1308 1308 1308 1308 1308 1271 656 656 656...</td>\n",
       "      <td>2128 3207 2128 2128 2128 2128 3142 4618 4618 4...</td>\n",
       "      <td>3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...</td>\n",
       "      <td>521 521 521 521 521 522 529 828 828 828 828 82...</td>\n",
       "      <td>0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>889499 528459 765746 553259 889499 22435 40047...</td>\n",
       "      <td>662 1075 662 1577 662 11 184 1604 11 11 177 11...</td>\n",
       "      <td>4048 601 3104 3828 4048 4766 2419 2768 2565 26...</td>\n",
       "      <td>5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...</td>\n",
       "      <td>517 520 525 528 602 602 610 610 610 610 610 61...</td>\n",
       "      <td>3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>979639 890128 981780 211366 211366 797946 4567...</td>\n",
       "      <td>267 1271 1505 267 267 1075 1075 407 407 1075 4...</td>\n",
       "      <td>2429 4785 3784 800 800 1595 1418 2662 2662 315...</td>\n",
       "      <td>2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...</td>\n",
       "      <td>529 529 602 604 604 607 607 607 607 607 607 60...</td>\n",
       "      <td>0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13178</th>\n",
       "      <td>424025</td>\n",
       "      <td>1064392 261421 261421 770894 753017 297015 475...</td>\n",
       "      <td>1518 1518 1518 1357 245 1518 1401 1401 971 295...</td>\n",
       "      <td>4287 4287 4287 3144 3144 467 3214 3214 270 376...</td>\n",
       "      <td>6348.0 6348.0 6348.0 4026.0 4026.0 4708.0 1887...</td>\n",
       "      <td>608 608 608 608 608 608 619 622 623 716 717 71...</td>\n",
       "      <td>0 3 0 0 0 0 0 0 3 2 0 0 0 0 0 0 3 0 3 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13179</th>\n",
       "      <td>424060</td>\n",
       "      <td>1014256 665996 901216 754284 769108 250243 396...</td>\n",
       "      <td>1397 1397 1397 1095 302 662 662 662 662 662 15...</td>\n",
       "      <td>625 625 1123 3760 3760 3760 3760 3760 3760 376...</td>\n",
       "      <td>752.0 752.0 4028.0 3738.0 3738.0 3738.0 3738.0...</td>\n",
       "      <td>604 604 604 628 628 628 628 628 628 628 628 62...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13180</th>\n",
       "      <td>424070</td>\n",
       "      <td>305287 840017 840017 1076460 645176 183925 183...</td>\n",
       "      <td>84 1213 1213 1271 1075 1075 1075 1075 1075 107...</td>\n",
       "      <td>1274 4562 4562 3392 4585 4585 4585 4585 4585 4...</td>\n",
       "      <td>2800.0 3972.0 3972.0 2730.0 8012.0 8012.0 8012...</td>\n",
       "      <td>601 810 810 810 922 922 922 922 1013 1013 1103...</td>\n",
       "      <td>3 3 2 3 3 3 0 0 0 0 3 0 3 0 3 0 3 3 0 0 0 0 3 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13181</th>\n",
       "      <td>424083</td>\n",
       "      <td>615903 142904 815349 472387 325596 815349 1095...</td>\n",
       "      <td>1505 602 602 602 602 602 602 1553 530 1505 146...</td>\n",
       "      <td>3760 1058 1058 1058 1058 1058 1058 4175 3017 1...</td>\n",
       "      <td>3738.0 8096.0 3368.0 3368.0 8096.0 3368.0 88.0...</td>\n",
       "      <td>627 702 702 702 702 702 702 905 905 908 1003 1...</td>\n",
       "      <td>0 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13182</th>\n",
       "      <td>424164</td>\n",
       "      <td>578568 206584 914885 578568 93415 906976 96057...</td>\n",
       "      <td>1095 1095 1095 1095 662 1095 662 662 1095 662 ...</td>\n",
       "      <td>1340 2894 1340 1340 2894 2894 2894 2894 1340 2...</td>\n",
       "      <td>nan 4096.0 1981.0 1981.0 4096.0 4096.0 4096.0 ...</td>\n",
       "      <td>513 605 605 605 605 605 605 605 605 605 608 60...</td>\n",
       "      <td>1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13183 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id                                          item_path  \\\n",
       "0           16  980982 980982 980982 962763 391126 827174 6731...   \n",
       "1           19  388018 388018 88673 88673 88673 88673 846066 5...   \n",
       "2           41  60215 1004605 60215 60215 60215 60215 628525 5...   \n",
       "3           56  889499 528459 765746 553259 889499 22435 40047...   \n",
       "4          155  979639 890128 981780 211366 211366 797946 4567...   \n",
       "...        ...                                                ...   \n",
       "13178   424025  1064392 261421 261421 770894 753017 297015 475...   \n",
       "13179   424060  1014256 665996 901216 754284 769108 250243 396...   \n",
       "13180   424070  305287 840017 840017 1076460 645176 183925 183...   \n",
       "13181   424083  615903 142904 815349 472387 325596 815349 1095...   \n",
       "13182   424164  578568 206584 914885 578568 93415 906976 96057...   \n",
       "\n",
       "                                                cat_path  \\\n",
       "0      437 437 437 19 437 437 437 437 895 19 437 437 ...   \n",
       "1      949 949 614 614 614 614 420 1401 948 948 513 1...   \n",
       "2      1308 1308 1308 1308 1308 1308 1271 656 656 656...   \n",
       "3      662 1075 662 1577 662 11 184 1604 11 11 177 11...   \n",
       "4      267 1271 1505 267 267 1075 1075 407 407 1075 4...   \n",
       "...                                                  ...   \n",
       "13178  1518 1518 1518 1357 245 1518 1401 1401 971 295...   \n",
       "13179  1397 1397 1397 1095 302 662 662 662 662 662 15...   \n",
       "13180  84 1213 1213 1271 1075 1075 1075 1075 1075 107...   \n",
       "13181  1505 602 602 602 602 602 602 1553 530 1505 146...   \n",
       "13182  1095 1095 1095 1095 662 1095 662 662 1095 662 ...   \n",
       "\n",
       "                                             seller_path  \\\n",
       "0      650 650 650 650 650 650 650 650 3948 650 650 6...   \n",
       "1      2772 2772 4066 4066 4066 4066 4951 4951 2872 2...   \n",
       "2      2128 3207 2128 2128 2128 2128 3142 4618 4618 4...   \n",
       "3      4048 601 3104 3828 4048 4766 2419 2768 2565 26...   \n",
       "4      2429 4785 3784 800 800 1595 1418 2662 2662 315...   \n",
       "...                                                  ...   \n",
       "13178  4287 4287 4287 3144 3144 467 3214 3214 270 376...   \n",
       "13179  625 625 1123 3760 3760 3760 3760 3760 3760 376...   \n",
       "13180  1274 4562 4562 3392 4585 4585 4585 4585 4585 4...   \n",
       "13181  3760 1058 1058 1058 1058 1058 1058 4175 3017 1...   \n",
       "13182  1340 2894 1340 1340 2894 2894 2894 2894 1340 2...   \n",
       "\n",
       "                                              brand_path  \\\n",
       "0      4276.0 4276.0 4276.0 4276.0 4276.0 4276.0 4276...   \n",
       "1      2112.0 2112.0 1552.0 1552.0 1552.0 1552.0 5200...   \n",
       "2      3848.0 3848.0 3848.0 3848.0 3848.0 3848.0 1014...   \n",
       "3      5360.0 1040.0 8240.0 1446.0 5360.0 4360.0 3428...   \n",
       "4      2276.0 1422.0 5692.0 6328.0 6328.0 5800.0 7140...   \n",
       "...                                                  ...   \n",
       "13178  6348.0 6348.0 6348.0 4026.0 4026.0 4708.0 1887...   \n",
       "13179  752.0 752.0 4028.0 3738.0 3738.0 3738.0 3738.0...   \n",
       "13180  2800.0 3972.0 3972.0 2730.0 8012.0 8012.0 8012...   \n",
       "13181  3738.0 8096.0 3368.0 3368.0 8096.0 3368.0 88.0...   \n",
       "13182  nan 4096.0 1981.0 1981.0 4096.0 4096.0 4096.0 ...   \n",
       "\n",
       "                                         time_stamp_path  \\\n",
       "0      914 914 914 914 914 914 914 914 914 914 914 91...   \n",
       "1      710 710 711 711 711 711 908 908 1105 1105 1105...   \n",
       "2      521 521 521 521 521 522 529 828 828 828 828 82...   \n",
       "3      517 520 525 528 602 602 610 610 610 610 610 61...   \n",
       "4      529 529 602 604 604 607 607 607 607 607 607 60...   \n",
       "...                                                  ...   \n",
       "13178  608 608 608 608 608 608 619 622 623 716 717 71...   \n",
       "13179  604 604 604 628 628 628 628 628 628 628 628 62...   \n",
       "13180  601 810 810 810 922 922 922 922 1013 1013 1103...   \n",
       "13181  627 702 702 702 702 702 702 905 905 908 1003 1...   \n",
       "13182  513 605 605 605 605 605 605 605 605 605 608 60...   \n",
       "\n",
       "                                        action_type_path  \n",
       "0      0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 2 0 ...  \n",
       "1      0 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "2      0 0 0 2 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "3      3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 ...  \n",
       "4      0 0 0 2 2 0 0 0 0 0 0 2 0 0 0 0 0 0 0 2 0 0 2 ...  \n",
       "...                                                  ...  \n",
       "13178  0 3 0 0 0 0 0 0 3 2 0 0 0 0 0 0 3 0 3 0 0 0 0 ...  \n",
       "13179  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 ...  \n",
       "13180  3 3 2 3 3 3 0 0 0 0 3 0 3 0 3 0 3 3 0 0 0 0 3 ...  \n",
       "13181  0 0 0 0 0 0 0 0 0 0 2 2 2 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "13182  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "\n",
       "[13183 rows x 7 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_log_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe210c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:17.237517Z",
     "start_time": "2022-02-14T09:50:17.144479Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_path = all_data.merge(user_log_path,on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5e4af43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:19.974279Z",
     "start_time": "2022-02-14T09:50:19.951999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16859</th>\n",
       "      <td>120191</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>793882 288225 288225 288225 195714 195714 1957...</td>\n",
       "      <td>387 35 35 35 1213 1213 1213 1213 1075 447 1213...</td>\n",
       "      <td>1146 696 696 696 1200 1200 1200 1200 2702 4279...</td>\n",
       "      <td>8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...</td>\n",
       "      <td>512 516 516 516 606 606 606 606 606 606 606 60...</td>\n",
       "      <td>2 2 2 2 2 0 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16860</th>\n",
       "      <td>121727</td>\n",
       "      <td>4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544029 562170 544029 562170 544029 544029 5621...</td>\n",
       "      <td>1505 662 1505 662 1505 1505 662 1505 1505 1505...</td>\n",
       "      <td>795 1910 795 1910 795 795 1910 795 795 795 411...</td>\n",
       "      <td>3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...</td>\n",
       "      <td>628 628 628 628 628 628 628 628 628 628 710 71...</td>\n",
       "      <td>0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16861</th>\n",
       "      <td>385919</td>\n",
       "      <td>3912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187936 187936 657875 969054 462255 985073 1602...</td>\n",
       "      <td>602 602 1389 1505 1228 1604 1228 662 1228 662 ...</td>\n",
       "      <td>661 661 643 643 4738 643 3740 643 4738 643 643...</td>\n",
       "      <td>1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...</td>\n",
       "      <td>512 512 513 513 524 524 524 524 524 524 526 60...</td>\n",
       "      <td>2 2 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16862</th>\n",
       "      <td>215423</td>\n",
       "      <td>4356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>215423</td>\n",
       "      <td>1840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16864 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  merchant_id  label  age_range  gender  \\\n",
       "0       105600         1487    0.0        6.0     1.0   \n",
       "1       110976          159    0.0        5.0     0.0   \n",
       "2       374400          302    0.0        5.0     1.0   \n",
       "3       189312         1760    0.0        4.0     0.0   \n",
       "4       189312         2511    0.0        4.0     0.0   \n",
       "...        ...          ...    ...        ...     ...   \n",
       "16859   120191         1899    NaN        4.0     0.0   \n",
       "16860   121727         4044    NaN        4.0     0.0   \n",
       "16861   385919         3912    NaN        0.0     0.0   \n",
       "16862   215423         4356    NaN        5.0     0.0   \n",
       "16863   215423         1840    NaN        5.0     0.0   \n",
       "\n",
       "                                               item_path  \\\n",
       "0      986160 681407 681407 910680 681407 592698 3693...   \n",
       "1      396970 961553 627712 926681 1012423 825576 149...   \n",
       "2      256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3      290583 166235 556025 217894 166235 556025 5589...   \n",
       "4      290583 166235 556025 217894 166235 556025 5589...   \n",
       "...                                                  ...   \n",
       "16859  793882 288225 288225 288225 195714 195714 1957...   \n",
       "16860  544029 562170 544029 562170 544029 544029 5621...   \n",
       "16861  187936 187936 657875 969054 462255 985073 1602...   \n",
       "16862  885364 938282 966141 174392 885364 821661 3473...   \n",
       "16863  885364 938282 966141 174392 885364 821661 3473...   \n",
       "\n",
       "                                                cat_path  \\\n",
       "0      35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1      1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2      1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "...                                                  ...   \n",
       "16859  387 35 35 35 1213 1213 1213 1213 1075 447 1213...   \n",
       "16860  1505 662 1505 662 1505 1505 662 1505 1505 1505...   \n",
       "16861  602 602 1389 1505 1228 1604 1228 662 1228 662 ...   \n",
       "16862  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "16863  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "\n",
       "                                             seller_path  \\\n",
       "0      4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1      1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2      805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "...                                                  ...   \n",
       "16859  1146 696 696 696 1200 1200 1200 1200 2702 4279...   \n",
       "16860  795 1910 795 1910 795 795 1910 795 795 795 411...   \n",
       "16861  661 661 643 643 4738 643 3740 643 4738 643 643...   \n",
       "16862  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "16863  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "\n",
       "                                              brand_path  \\\n",
       "0      127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1      5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2      1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "...                                                  ...   \n",
       "16859  8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...   \n",
       "16860  3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...   \n",
       "16861  1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...   \n",
       "16862  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "16863  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "\n",
       "                                         time_stamp_path  \\\n",
       "0      518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1      517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2      517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3      924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4      924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "...                                                  ...   \n",
       "16859  512 516 516 516 606 606 606 606 606 606 606 60...   \n",
       "16860  628 628 628 628 628 628 628 628 628 628 710 71...   \n",
       "16861  512 512 513 513 524 524 524 524 524 524 526 60...   \n",
       "16862  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "16863  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "\n",
       "                                        action_type_path  \n",
       "0      2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1      2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2      2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3      0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4      0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "...                                                  ...  \n",
       "16859  2 2 2 2 2 0 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 ...  \n",
       "16860  0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 ...  \n",
       "16861  2 2 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "16862  0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...  \n",
       "16863  0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...  \n",
       "\n",
       "[16864 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa2061fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:34.419449Z",
     "start_time": "2022-02-14T09:50:34.337196Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2386"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "删除不需要的数据\n",
    "\"\"\"\n",
    "del user_log\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d2371bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:39.979772Z",
     "start_time": "2022-02-14T09:50:39.967778Z"
    }
   },
   "outputs": [],
   "source": [
    "def cnt_(x):\n",
    "    try:\n",
    "        return len(x.split(' '))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d192a44a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:45.306541Z",
     "start_time": "2022-02-14T09:50:45.297448Z"
    }
   },
   "outputs": [],
   "source": [
    "def nunique_(x):\n",
    "    try:\n",
    "        return len(set(x.split(' ')))\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1526e8ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:49.725139Z",
     "start_time": "2022-02-14T09:50:49.713136Z"
    }
   },
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    try:\n",
    "        return np.max([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f41730f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:55.319508Z",
     "start_time": "2022-02-14T09:50:55.308499Z"
    }
   },
   "outputs": [],
   "source": [
    "def min_(x):\n",
    "    try:\n",
    "        return np.min([int(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8108cb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:50:59.647630Z",
     "start_time": "2022-02-14T09:50:59.639529Z"
    }
   },
   "outputs": [],
   "source": [
    "def std_(x):\n",
    "    try:\n",
    "        return np.std([float(i) for i in x.split(' ')])\n",
    "    except:\n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "80d3c17b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:51:05.874026Z",
     "start_time": "2022-02-14T09:51:05.871424Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][0]\n",
    "    except:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "33d634ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:51:12.138987Z",
     "start_time": "2022-02-14T09:51:12.127985Z"
    }
   },
   "outputs": [],
   "source": [
    "def most_n_cnt(x, n):\n",
    "    try:\n",
    "        return Counter(x.split(' ')).most_common(n)[n-1][1]\n",
    "    except:\n",
    "        return -1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbdb57fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:51:25.862897Z",
     "start_time": "2022-02-14T09:51:25.847909Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "def user_cnt(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(cnt_)\n",
    "    return df_data\n",
    "\n",
    "def user_nunique(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(nunique_)\n",
    "    return df_data\n",
    "    \n",
    "def user_max(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(max_)\n",
    "    return df_data\n",
    "\n",
    "def user_min(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(min_)\n",
    "    return df_data\n",
    "    \n",
    "def user_std(df_data, single_col, name):\n",
    "    df_data[name] = df_data[single_col].apply(std_)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n",
    "\n",
    "def user_most_n_cnt(df_data, single_col, name, n=1):\n",
    "    func = lambda x: most_n_cnt(x, n)\n",
    "    df_data[name] = df_data[single_col].apply(func)\n",
    "    return df_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "962d8e72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:51:54.062994Z",
     "start_time": "2022-02-14T09:51:54.046989Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16859</th>\n",
       "      <td>120191</td>\n",
       "      <td>1899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>793882 288225 288225 288225 195714 195714 1957...</td>\n",
       "      <td>387 35 35 35 1213 1213 1213 1213 1075 447 1213...</td>\n",
       "      <td>1146 696 696 696 1200 1200 1200 1200 2702 4279...</td>\n",
       "      <td>8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...</td>\n",
       "      <td>512 516 516 516 606 606 606 606 606 606 606 60...</td>\n",
       "      <td>2 2 2 2 2 0 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16860</th>\n",
       "      <td>121727</td>\n",
       "      <td>4044</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>544029 562170 544029 562170 544029 544029 5621...</td>\n",
       "      <td>1505 662 1505 662 1505 1505 662 1505 1505 1505...</td>\n",
       "      <td>795 1910 795 1910 795 795 1910 795 795 795 411...</td>\n",
       "      <td>3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...</td>\n",
       "      <td>628 628 628 628 628 628 628 628 628 628 710 71...</td>\n",
       "      <td>0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16861</th>\n",
       "      <td>385919</td>\n",
       "      <td>3912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>187936 187936 657875 969054 462255 985073 1602...</td>\n",
       "      <td>602 602 1389 1505 1228 1604 1228 662 1228 662 ...</td>\n",
       "      <td>661 661 643 643 4738 643 3740 643 4738 643 643...</td>\n",
       "      <td>1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...</td>\n",
       "      <td>512 512 513 513 524 524 524 524 524 524 526 60...</td>\n",
       "      <td>2 2 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16862</th>\n",
       "      <td>215423</td>\n",
       "      <td>4356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>215423</td>\n",
       "      <td>1840</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>885364 938282 966141 174392 885364 821661 3473...</td>\n",
       "      <td>1389 662 1095 1095 1389 662 1577 821 662 1389 ...</td>\n",
       "      <td>2602 2602 2602 2602 2602 2602 3128 2602 2602 2...</td>\n",
       "      <td>1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...</td>\n",
       "      <td>521 521 521 521 521 521 521 521 521 521 521 52...</td>\n",
       "      <td>0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16864 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  merchant_id  label  age_range  gender  \\\n",
       "0       105600         1487    0.0        6.0     1.0   \n",
       "1       110976          159    0.0        5.0     0.0   \n",
       "2       374400          302    0.0        5.0     1.0   \n",
       "3       189312         1760    0.0        4.0     0.0   \n",
       "4       189312         2511    0.0        4.0     0.0   \n",
       "...        ...          ...    ...        ...     ...   \n",
       "16859   120191         1899    NaN        4.0     0.0   \n",
       "16860   121727         4044    NaN        4.0     0.0   \n",
       "16861   385919         3912    NaN        0.0     0.0   \n",
       "16862   215423         4356    NaN        5.0     0.0   \n",
       "16863   215423         1840    NaN        5.0     0.0   \n",
       "\n",
       "                                               item_path  \\\n",
       "0      986160 681407 681407 910680 681407 592698 3693...   \n",
       "1      396970 961553 627712 926681 1012423 825576 149...   \n",
       "2      256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3      290583 166235 556025 217894 166235 556025 5589...   \n",
       "4      290583 166235 556025 217894 166235 556025 5589...   \n",
       "...                                                  ...   \n",
       "16859  793882 288225 288225 288225 195714 195714 1957...   \n",
       "16860  544029 562170 544029 562170 544029 544029 5621...   \n",
       "16861  187936 187936 657875 969054 462255 985073 1602...   \n",
       "16862  885364 938282 966141 174392 885364 821661 3473...   \n",
       "16863  885364 938282 966141 174392 885364 821661 3473...   \n",
       "\n",
       "                                                cat_path  \\\n",
       "0      35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1      1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2      1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4      601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "...                                                  ...   \n",
       "16859  387 35 35 35 1213 1213 1213 1213 1075 447 1213...   \n",
       "16860  1505 662 1505 662 1505 1505 662 1505 1505 1505...   \n",
       "16861  602 602 1389 1505 1228 1604 1228 662 1228 662 ...   \n",
       "16862  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "16863  1389 662 1095 1095 1389 662 1577 821 662 1389 ...   \n",
       "\n",
       "                                             seller_path  \\\n",
       "0      4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1      1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2      805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4      3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "...                                                  ...   \n",
       "16859  1146 696 696 696 1200 1200 1200 1200 2702 4279...   \n",
       "16860  795 1910 795 1910 795 795 1910 795 795 795 411...   \n",
       "16861  661 661 643 643 4738 643 3740 643 4738 643 643...   \n",
       "16862  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "16863  2602 2602 2602 2602 2602 2602 3128 2602 2602 2...   \n",
       "\n",
       "                                              brand_path  \\\n",
       "0      127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1      5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2      1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4      549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "...                                                  ...   \n",
       "16859  8064.0 3600.0 3600.0 3600.0 2276.0 2276.0 2276...   \n",
       "16860  3608.0 950.0 3608.0 950.0 3608.0 3608.0 950.0 ...   \n",
       "16861  1484.0 1484.0 968.0 968.0 6220.0 968.0 4072.0 ...   \n",
       "16862  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "16863  1900.0 1900.0 1900.0 1900.0 1900.0 1900.0 8392...   \n",
       "\n",
       "                                         time_stamp_path  \\\n",
       "0      518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1      517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2      517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3      924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4      924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "...                                                  ...   \n",
       "16859  512 516 516 516 606 606 606 606 606 606 606 60...   \n",
       "16860  628 628 628 628 628 628 628 628 628 628 710 71...   \n",
       "16861  512 512 513 513 524 524 524 524 524 524 526 60...   \n",
       "16862  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "16863  521 521 521 521 521 521 521 521 521 521 521 52...   \n",
       "\n",
       "                                        action_type_path  \n",
       "0      2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "1      2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...  \n",
       "2      2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "3      0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "4      0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "...                                                  ...  \n",
       "16859  2 2 2 2 2 0 0 0 0 0 0 0 2 0 0 2 0 0 2 0 0 0 0 ...  \n",
       "16860  0 0 0 2 0 0 0 0 2 2 0 0 0 0 0 0 0 2 0 0 0 0 0 ...  \n",
       "16861  2 2 2 2 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 ...  \n",
       "16862  0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...  \n",
       "16863  0 0 0 0 2 0 0 0 0 0 2 0 2 2 0 0 0 0 2 2 0 0 0 ...  \n",
       "\n",
       "[16864 rows x 11 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a5b34055",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:54:34.730210Z",
     "start_time": "2022-02-14T09:54:34.581130Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    提取基本统计特征\n",
    "\"\"\"\n",
    "all_data_test = all_data_path.head(2000)\n",
    "#all_data_test = all_data_path\n",
    "# 统计用户 点击、浏览、加购、购买行为\n",
    "# 总次数\n",
    "all_data_test = user_cnt(all_data_test,  'seller_path', 'user_cnt')\n",
    "# 不同店铺个数\n",
    "all_data_test = user_nunique(all_data_test,  'seller_path', 'seller_nunique')\n",
    "# 不同品类个数\n",
    "all_data_test = user_nunique(all_data_test,  'cat_path', 'cat_nunique')\n",
    "# 不同品牌个数\n",
    "all_data_test = user_nunique(all_data_test,  'brand_path', 'brand_nunique')\n",
    "# 不同商品个数\n",
    "all_data_test = user_nunique(all_data_test,  'item_path', 'item_nunique')\n",
    "# 活跃天数\n",
    "all_data_test = user_nunique(all_data_test,  'time_stamp_path', 'time_stamp_nunique')\n",
    "# 不用行为种数\n",
    "all_data_test = user_nunique(all_data_test,  'action_type_path', 'action_type_nunique')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "72d3d5d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:54:44.646704Z",
     "start_time": "2022-02-14T09:54:44.625149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>action_type_path</th>\n",
       "      <th>user_cnt</th>\n",
       "      <th>seller_nunique</th>\n",
       "      <th>cat_nunique</th>\n",
       "      <th>brand_nunique</th>\n",
       "      <th>item_nunique</th>\n",
       "      <th>time_stamp_nunique</th>\n",
       "      <th>action_type_nunique</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>310</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>88</td>\n",
       "      <td>217</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "      <td>70</td>\n",
       "      <td>159</td>\n",
       "      <td>233</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>278</td>\n",
       "      <td>57</td>\n",
       "      <td>59</td>\n",
       "      <td>62</td>\n",
       "      <td>148</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...</td>\n",
       "      <td>237</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  label  age_range  gender  \\\n",
       "0   105600         1487    0.0        6.0     1.0   \n",
       "1   110976          159    0.0        5.0     0.0   \n",
       "2   374400          302    0.0        5.0     1.0   \n",
       "3   189312         1760    0.0        4.0     0.0   \n",
       "4   189312         2511    0.0        4.0     0.0   \n",
       "\n",
       "                                           item_path  \\\n",
       "0  986160 681407 681407 910680 681407 592698 3693...   \n",
       "1  396970 961553 627712 926681 1012423 825576 149...   \n",
       "2  256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3  290583 166235 556025 217894 166235 556025 5589...   \n",
       "4  290583 166235 556025 217894 166235 556025 5589...   \n",
       "\n",
       "                                            cat_path  \\\n",
       "0  35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1  1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2  1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4  601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "\n",
       "                                         seller_path  \\\n",
       "0  4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1  1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2  805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4  3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "\n",
       "                                          brand_path  \\\n",
       "0  127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1  5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2  1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4  549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "\n",
       "                                     time_stamp_path  \\\n",
       "0  518 518 518 520 520 524 524 524 525 525 525 52...   \n",
       "1  517 520 522 522 527 530 530 530 601 601 602 60...   \n",
       "2  517 604 604 604 607 609 609 609 609 615 621 62...   \n",
       "3  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "4  924 924 924 924 924 924 924 924 924 924 924 92...   \n",
       "\n",
       "                                    action_type_path  user_cnt  \\\n",
       "0  2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       310   \n",
       "1  2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 0 ...       274   \n",
       "2  2 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       278   \n",
       "3  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "4  0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 ...       237   \n",
       "\n",
       "   seller_nunique  cat_nunique  brand_nunique  item_nunique  \\\n",
       "0              96           37             88           217   \n",
       "1             181           70            159           233   \n",
       "2              57           59             62           148   \n",
       "3              49           35             45           170   \n",
       "4              49           35             45           170   \n",
       "\n",
       "   time_stamp_nunique  action_type_nunique  \n",
       "0                  29                    2  \n",
       "1                  52                    3  \n",
       "2                  35                    3  \n",
       "3                   9                    2  \n",
       "4                   9                    2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b0f1f79a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:55:15.921095Z",
     "start_time": "2022-02-14T09:55:15.691577Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最晚时间\n",
    "all_data_test = user_max(all_data_test,  'action_type_path', 'time_stamp_max')\n",
    "# 最早时间\n",
    "all_data_test = user_min(all_data_test,  'action_type_path', 'time_stamp_min')\n",
    "# 活跃天数方差\n",
    "all_data_test = user_std(all_data_test,  'action_type_path', 'time_stamp_std')\n",
    "# 最早和最晚相差天数\n",
    "all_data_test['time_stamp_range'] = all_data_test['time_stamp_max'] - all_data_test['time_stamp_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18711270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:55:26.423487Z",
     "start_time": "2022-02-14T09:55:26.287432Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺\n",
    "all_data_test = user_most_n(all_data_test, 'seller_path', 'seller_most_1', n=1)\n",
    "# 最喜欢的类目\n",
    "all_data_test = user_most_n(all_data_test, 'cat_path', 'cat_most_1', n=1)\n",
    "# 最喜欢的品牌\n",
    "all_data_test = user_most_n(all_data_test, 'brand_path', 'brand_most_1', n=1)\n",
    "# 最常见的行为动作\n",
    "all_data_test = user_most_n(all_data_test, 'action_type_path', 'action_type_1', n=1)\n",
    "# ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b97de6bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T09:56:14.655559Z",
     "start_time": "2022-02-14T09:56:14.520462Z"
    }
   },
   "outputs": [],
   "source": [
    "# 用户最喜欢的店铺 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'seller_path', 'seller_most_1_cnt', n=1)\n",
    "# 最喜欢的类目 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'cat_path', 'cat_most_1_cnt', n=1)\n",
    "# 最喜欢的品牌 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'brand_path', 'brand_most_1_cnt', n=1)\n",
    "# 最常见的行为动作 行为次数\n",
    "all_data_test = user_most_n_cnt(all_data_test, 'action_type_path', 'action_type_1_cnt', n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b1c2c6f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:02:15.842506Z",
     "start_time": "2022-02-14T10:02:15.829499Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击、加购、购买、收藏 分开统计\n",
    "\"\"\"\n",
    "统计基本特征函数  \n",
    "-- 知识点二\n",
    "-- 根据不同行为的业务函数\n",
    "-- 提取不同特征\n",
    "\"\"\"\n",
    "def col_cnt_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(data_out)  \n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def col_nuique_(df_data, columns_list, action_type):\n",
    "    try:\n",
    "        data_dict = {}\n",
    "\n",
    "        col_list = copy.deepcopy(columns_list)\n",
    "        if action_type != None:\n",
    "            col_list += ['action_type_path']\n",
    "\n",
    "        for col in col_list:\n",
    "            data_dict[col] = df_data[col].split(' ')\n",
    "\n",
    "        path_len = len(data_dict[col])\n",
    "\n",
    "        data_out = []\n",
    "        for i_ in range(path_len):\n",
    "            data_txt = ''\n",
    "            for col_ in columns_list:\n",
    "                if data_dict['action_type_path'][i_] == action_type:\n",
    "                    data_txt += '_' + data_dict[col_][i_]\n",
    "            data_out.append(data_txt)\n",
    "\n",
    "        return len(set(data_out))\n",
    "    except:\n",
    "        return -1\n",
    "    \n",
    "\n",
    "def user_col_cnt(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_cnt_(x, columns_list, action_type), axis=1)\n",
    "    return df_data\n",
    "\n",
    "def user_col_nunique(df_data, columns_list, action_type, name):\n",
    "    df_data[name] = df_data.apply(lambda x: col_nuique_(x, columns_list, action_type), axis=1)\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "31b7604d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:02:17.356707Z",
     "start_time": "2022-02-14T10:02:16.865597Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '0', 'user_cnt_0')\n",
    "# 加购次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '1', 'user_cnt_1')\n",
    "# 购买次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '2', 'user_cnt_2')\n",
    "# 收藏次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path'], '3', 'user_cnt_3')\n",
    "\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af77ecc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:02:17.794590Z",
     "start_time": "2022-02-14T10:02:17.764584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>label</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_path</th>\n",
       "      <th>cat_path</th>\n",
       "      <th>seller_path</th>\n",
       "      <th>brand_path</th>\n",
       "      <th>time_stamp_path</th>\n",
       "      <th>...</th>\n",
       "      <th>action_type_1</th>\n",
       "      <th>seller_most_1_cnt</th>\n",
       "      <th>cat_most_1_cnt</th>\n",
       "      <th>brand_most_1_cnt</th>\n",
       "      <th>action_type_1_cnt</th>\n",
       "      <th>user_cnt_0</th>\n",
       "      <th>user_cnt_1</th>\n",
       "      <th>user_cnt_2</th>\n",
       "      <th>user_cnt_3</th>\n",
       "      <th>seller_nunique_0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105600</td>\n",
       "      <td>1487</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>986160 681407 681407 910680 681407 592698 3693...</td>\n",
       "      <td>35 1554 1554 119 1554 662 1095 662 35 833 833 ...</td>\n",
       "      <td>4811 4811 4811 1897 4811 3315 2925 1340 1875 4...</td>\n",
       "      <td>127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...</td>\n",
       "      <td>518 518 518 520 520 524 524 524 525 525 525 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>299</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>310</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>110976</td>\n",
       "      <td>159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>396970 961553 627712 926681 1012423 825576 149...</td>\n",
       "      <td>1023 420 407 1505 962 602 184 1606 351 1505 11...</td>\n",
       "      <td>1435 1648 223 3178 2418 1614 3004 2511 2285 78...</td>\n",
       "      <td>5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...</td>\n",
       "      <td>517 520 522 522 527 530 530 530 601 601 602 60...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>259</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>374400</td>\n",
       "      <td>302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>256546 202393 927572 2587 10956 549283 270303 ...</td>\n",
       "      <td>1188 646 1175 1188 1414 681 1175 681 681 115 1...</td>\n",
       "      <td>805 390 4252 3979 1228 2029 2029 2029 4252 923...</td>\n",
       "      <td>1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...</td>\n",
       "      <td>517 604 604 604 607 609 609 609 609 615 621 62...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>29</td>\n",
       "      <td>48</td>\n",
       "      <td>241</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>189312</td>\n",
       "      <td>1760</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>189312</td>\n",
       "      <td>2511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>290583 166235 556025 217894 166235 556025 5589...</td>\n",
       "      <td>601 601 601 601 601 601 601 601 601 601 601 60...</td>\n",
       "      <td>3139 3139 3524 3139 3139 3524 3139 3139 3139 3...</td>\n",
       "      <td>549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....</td>\n",
       "      <td>924 924 924 924 924 924 924 924 924 924 924 92...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>45</td>\n",
       "      <td>228</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>237</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>305721</td>\n",
       "      <td>3734</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>195526 195526 195526 195526 1002121 36379 8135...</td>\n",
       "      <td>384 384 384 384 1075 1075 1075 1349 1349 1349 ...</td>\n",
       "      <td>4696 4696 4696 4696 494 3863 4741 1251 2269 12...</td>\n",
       "      <td>6548.0 6548.0 6548.0 6548.0 7288.0 2960.0 7088...</td>\n",
       "      <td>610 610 610 630 707 707 707 1109 1109 1109 110...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>109881</td>\n",
       "      <td>2639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>288225 812720 500918 500918 698153 507072 5009...</td>\n",
       "      <td>35 681 295 295 1518 1518 295 1468 1518 1468 81...</td>\n",
       "      <td>696 899 3398 3398 3398 3398 3398 3398 3398 339...</td>\n",
       "      <td>3600.0 6568.0 30.0 30.0 30.0 30.0 30.0 30.0 30...</td>\n",
       "      <td>524 528 530 530 530 530 530 530 530 530 705 70...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "      <td>66</td>\n",
       "      <td>264</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>284</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>185145</td>\n",
       "      <td>4950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>952981 502801 917988 917988 143799 667127 4004...</td>\n",
       "      <td>898 795 354 354 833 1150 843 898 898 898 898 8...</td>\n",
       "      <td>3319 390 1338 1338 3149 3319 1338 3319 3319 33...</td>\n",
       "      <td>5508.0 6876.0 4740.0 4740.0 5388.0 1283.0 4740...</td>\n",
       "      <td>515 516 517 517 604 606 606 619 619 619 619 61...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>131385</td>\n",
       "      <td>1582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>771829 332521 332521 60522 60522 672487 711909...</td>\n",
       "      <td>1213 1075 1075 1213 1213 1656 1075 1213 1213 1...</td>\n",
       "      <td>2588 3168 3168 2160 2160 2439 3168 2160 2588 4...</td>\n",
       "      <td>2760.0 4504.0 4504.0 1195.0 1195.0 1307.0 4504...</td>\n",
       "      <td>531 531 531 531 531 531 531 531 531 531 531 53...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>84</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>7737</td>\n",
       "      <td>2066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>305283 990086 985582 741215 741215 741215 6061...</td>\n",
       "      <td>1271 1271 812 464 464 464 1577 464 464 464 464...</td>\n",
       "      <td>1500 540 1849 176 176 176 3028 176 176 176 176...</td>\n",
       "      <td>7284.0 5384.0 5576.0 6664.0 6664.0 6664.0 4608...</td>\n",
       "      <td>511 511 514 520 520 520 520 520 520 520 520 52...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>189</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>210</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  merchant_id  label  age_range  gender  \\\n",
       "0      105600         1487    0.0        6.0     1.0   \n",
       "1      110976          159    0.0        5.0     0.0   \n",
       "2      374400          302    0.0        5.0     1.0   \n",
       "3      189312         1760    0.0        4.0     0.0   \n",
       "4      189312         2511    0.0        4.0     0.0   \n",
       "...       ...          ...    ...        ...     ...   \n",
       "1995   305721         3734    0.0        3.0     1.0   \n",
       "1996   109881         2639    0.0        4.0     0.0   \n",
       "1997   185145         4950    0.0        4.0     1.0   \n",
       "1998   131385         1582    0.0        3.0     1.0   \n",
       "1999     7737         2066    0.0        4.0     0.0   \n",
       "\n",
       "                                              item_path  \\\n",
       "0     986160 681407 681407 910680 681407 592698 3693...   \n",
       "1     396970 961553 627712 926681 1012423 825576 149...   \n",
       "2     256546 202393 927572 2587 10956 549283 270303 ...   \n",
       "3     290583 166235 556025 217894 166235 556025 5589...   \n",
       "4     290583 166235 556025 217894 166235 556025 5589...   \n",
       "...                                                 ...   \n",
       "1995  195526 195526 195526 195526 1002121 36379 8135...   \n",
       "1996  288225 812720 500918 500918 698153 507072 5009...   \n",
       "1997  952981 502801 917988 917988 143799 667127 4004...   \n",
       "1998  771829 332521 332521 60522 60522 672487 711909...   \n",
       "1999  305283 990086 985582 741215 741215 741215 6061...   \n",
       "\n",
       "                                               cat_path  \\\n",
       "0     35 1554 1554 119 1554 662 1095 662 35 833 833 ...   \n",
       "1     1023 420 407 1505 962 602 184 1606 351 1505 11...   \n",
       "2     1188 646 1175 1188 1414 681 1175 681 681 115 1...   \n",
       "3     601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "4     601 601 601 601 601 601 601 601 601 601 601 60...   \n",
       "...                                                 ...   \n",
       "1995  384 384 384 384 1075 1075 1075 1349 1349 1349 ...   \n",
       "1996  35 681 295 295 1518 1518 295 1468 1518 1468 81...   \n",
       "1997  898 795 354 354 833 1150 843 898 898 898 898 8...   \n",
       "1998  1213 1075 1075 1213 1213 1656 1075 1213 1213 1...   \n",
       "1999  1271 1271 812 464 464 464 1577 464 464 464 464...   \n",
       "\n",
       "                                            seller_path  \\\n",
       "0     4811 4811 4811 1897 4811 3315 2925 1340 1875 4...   \n",
       "1     1435 1648 223 3178 2418 1614 3004 2511 2285 78...   \n",
       "2     805 390 4252 3979 1228 2029 2029 2029 4252 923...   \n",
       "3     3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "4     3139 3139 3524 3139 3139 3524 3139 3139 3139 3...   \n",
       "...                                                 ...   \n",
       "1995  4696 4696 4696 4696 494 3863 4741 1251 2269 12...   \n",
       "1996  696 899 3398 3398 3398 3398 3398 3398 3398 339...   \n",
       "1997  3319 390 1338 1338 3149 3319 1338 3319 3319 33...   \n",
       "1998  2588 3168 3168 2160 2160 2439 3168 2160 2588 4...   \n",
       "1999  1500 540 1849 176 176 176 3028 176 176 176 176...   \n",
       "\n",
       "                                             brand_path  \\\n",
       "0     127.0 127.0 127.0 4704.0 127.0 1605.0 6000.0 1...   \n",
       "1     5504.0 7780.0 1751.0 7540.0 6652.0 8116.0 5328...   \n",
       "2     1842.0 5920.0 133.0 6304.0 7584.0 133.0 133.0 ...   \n",
       "3     549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "4     549.0 549.0 549.0 549.0 549.0 549.0 549.0 549....   \n",
       "...                                                 ...   \n",
       "1995  6548.0 6548.0 6548.0 6548.0 7288.0 2960.0 7088...   \n",
       "1996  3600.0 6568.0 30.0 30.0 30.0 30.0 30.0 30.0 30...   \n",
       "1997  5508.0 6876.0 4740.0 4740.0 5388.0 1283.0 4740...   \n",
       "1998  2760.0 4504.0 4504.0 1195.0 1195.0 1307.0 4504...   \n",
       "1999  7284.0 5384.0 5576.0 6664.0 6664.0 6664.0 4608...   \n",
       "\n",
       "                                        time_stamp_path  ... action_type_1  \\\n",
       "0     518 518 518 520 520 524 524 524 525 525 525 52...  ...             0   \n",
       "1     517 520 522 522 527 530 530 530 601 601 602 60...  ...             0   \n",
       "2     517 604 604 604 607 609 609 609 609 615 621 62...  ...             0   \n",
       "3     924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "4     924 924 924 924 924 924 924 924 924 924 924 92...  ...             0   \n",
       "...                                                 ...  ...           ...   \n",
       "1995  610 610 610 630 707 707 707 1109 1109 1109 110...  ...             0   \n",
       "1996  524 528 530 530 530 530 530 530 530 530 705 70...  ...             0   \n",
       "1997  515 516 517 517 604 606 606 619 619 619 619 61...  ...             0   \n",
       "1998  531 531 531 531 531 531 531 531 531 531 531 53...  ...             0   \n",
       "1999  511 511 514 520 520 520 520 520 520 520 520 52...  ...             0   \n",
       "\n",
       "      seller_most_1_cnt  cat_most_1_cnt  brand_most_1_cnt  action_type_1_cnt  \\\n",
       "0                    35              43                35                299   \n",
       "1                     9              56                11                259   \n",
       "2                    93              29                48                241   \n",
       "3                    45              68                45                228   \n",
       "4                    45              68                45                228   \n",
       "...                 ...             ...               ...                ...   \n",
       "1995                 11              11                11                 32   \n",
       "1996                 67              32                66                264   \n",
       "1997                 15              12                12                 74   \n",
       "1998                 16              21                17                 84   \n",
       "1999                 55              22                28                189   \n",
       "\n",
       "      user_cnt_0  user_cnt_1  user_cnt_2  user_cnt_3  seller_nunique_0  \n",
       "0            310         310         310         310                97  \n",
       "1            274         274         274         274               181  \n",
       "2            278         278         278         278                56  \n",
       "3            237         237         237         237                50  \n",
       "4            237         237         237         237                50  \n",
       "...          ...         ...         ...         ...               ...  \n",
       "1995          35          35          35          35                14  \n",
       "1996         284         284         284         284                52  \n",
       "1997          84          84          84          84                30  \n",
       "1998          92          92          92          92                39  \n",
       "1999         210         210         210         210                46  \n",
       "\n",
       "[2000 rows x 35 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2a7301d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:02:45.750570Z",
     "start_time": "2022-02-14T10:02:45.366569Z"
    }
   },
   "outputs": [],
   "source": [
    "# 点击次数\n",
    "all_data_test = user_col_cnt(all_data_test,  ['seller_path', 'item_path'], '0', 'user_cnt_0')\n",
    "\n",
    "# 不同店铺个数\n",
    "all_data_test = user_col_nunique(all_data_test,  ['seller_path', 'item_path'], '0', 'seller_nunique_0')\n",
    "# ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d036e75c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:02:54.692582Z",
     "start_time": "2022-02-14T10:02:54.675574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'merchant_id', 'label', 'age_range', 'gender', 'item_path',\n",
       "       'cat_path', 'seller_path', 'brand_path', 'time_stamp_path',\n",
       "       'action_type_path', 'user_cnt', 'seller_nunique', 'cat_nunique',\n",
       "       'brand_nunique', 'item_nunique', 'time_stamp_nunique',\n",
       "       'action_type_nunique', 'time_stamp_max', 'time_stamp_min',\n",
       "       'time_stamp_std', 'time_stamp_range', 'seller_most_1', 'cat_most_1',\n",
       "       'brand_most_1', 'action_type_1', 'seller_most_1_cnt', 'cat_most_1_cnt',\n",
       "       'brand_most_1_cnt', 'action_type_1_cnt', 'user_cnt_0', 'user_cnt_1',\n",
       "       'user_cnt_2', 'user_cnt_3', 'seller_nunique_0'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "66bf0f20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:03:00.208314Z",
     "start_time": "2022-02-14T10:03:00.197311Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id',\n",
       " 'merchant_id',\n",
       " 'label',\n",
       " 'age_range',\n",
       " 'gender',\n",
       " 'item_path',\n",
       " 'cat_path',\n",
       " 'seller_path',\n",
       " 'brand_path',\n",
       " 'time_stamp_path',\n",
       " 'action_type_path',\n",
       " 'user_cnt',\n",
       " 'seller_nunique',\n",
       " 'cat_nunique',\n",
       " 'brand_nunique',\n",
       " 'item_nunique',\n",
       " 'time_stamp_nunique',\n",
       " 'action_type_nunique',\n",
       " 'time_stamp_max',\n",
       " 'time_stamp_min',\n",
       " 'time_stamp_std',\n",
       " 'time_stamp_range',\n",
       " 'seller_most_1',\n",
       " 'cat_most_1',\n",
       " 'brand_most_1',\n",
       " 'action_type_1',\n",
       " 'seller_most_1_cnt',\n",
       " 'cat_most_1_cnt',\n",
       " 'brand_most_1_cnt',\n",
       " 'action_type_1_cnt',\n",
       " 'user_cnt_0',\n",
       " 'user_cnt_1',\n",
       " 'user_cnt_2',\n",
       " 'user_cnt_3',\n",
       " 'seller_nunique_0']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(all_data_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e6ca4e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:03:30.614070Z",
     "start_time": "2022-02-14T10:03:30.252990Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点四\n",
    "-- 利用countvector，tfidf提取特征\n",
    "\"\"\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from scipy import sparse\n",
    "# cntVec = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "tfidfVec = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 1), max_features=100)\n",
    "\n",
    "\n",
    "# columns_list = ['seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']\n",
    "columns_list = ['seller_path']\n",
    "for i, col in enumerate(columns_list):\n",
    "    all_data_test[col] = all_data_test[col].astype(str)\n",
    "    tfidfVec.fit(all_data_test[col])\n",
    "    data_ = tfidfVec.transform(all_data_test[col])\n",
    "    if i == 0:\n",
    "        data_cat = data_\n",
    "    else:\n",
    "        data_cat = sparse.hstack((data_cat, data_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "026a968c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:03:39.957273Z",
     "start_time": "2022-02-14T10:03:39.953912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x100 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12147 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "af19fb63",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:03:56.702600Z",
     "start_time": "2022-02-14T10:03:56.676512Z"
    }
   },
   "outputs": [],
   "source": [
    "df_tfidf = pd.DataFrame(data_cat.toarray())\n",
    "df_tfidf.columns = ['tfidf_' + str(i) for i in df_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a3d2210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:03:59.633885Z",
     "start_time": "2022-02-14T10:03:59.614491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_0</th>\n",
       "      <th>tfidf_1</th>\n",
       "      <th>tfidf_2</th>\n",
       "      <th>tfidf_3</th>\n",
       "      <th>tfidf_4</th>\n",
       "      <th>tfidf_5</th>\n",
       "      <th>tfidf_6</th>\n",
       "      <th>tfidf_7</th>\n",
       "      <th>tfidf_8</th>\n",
       "      <th>tfidf_9</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_90</th>\n",
       "      <th>tfidf_91</th>\n",
       "      <th>tfidf_92</th>\n",
       "      <th>tfidf_93</th>\n",
       "      <th>tfidf_94</th>\n",
       "      <th>tfidf_95</th>\n",
       "      <th>tfidf_96</th>\n",
       "      <th>tfidf_97</th>\n",
       "      <th>tfidf_98</th>\n",
       "      <th>tfidf_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.085794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.115594</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.151756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.438598</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163503</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181005</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.203564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.746582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190792</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.984127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.065932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tfidf_0   tfidf_1  tfidf_2   tfidf_3  tfidf_4   tfidf_5  tfidf_6  \\\n",
       "0         0.0  0.085794      0.0  0.000000      0.0  0.337596      0.0   \n",
       "1         0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "2         0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "3         0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "4         0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "...       ...       ...      ...       ...      ...       ...      ...   \n",
       "1995      0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "1996      0.0  0.746582      0.0  0.090532      0.0  0.000000      0.0   \n",
       "1997      0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "1998      0.0  0.000000      0.0  0.000000      0.0  0.000000      0.0   \n",
       "1999      0.0  0.065932      0.0  0.000000      0.0  0.000000      0.0   \n",
       "\n",
       "       tfidf_7   tfidf_8   tfidf_9  ...  tfidf_90  tfidf_91  tfidf_92  \\\n",
       "0     0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "1     0.000000  0.000000  0.000000  ...  0.000000  0.151756       0.0   \n",
       "2     0.000000  0.000000  0.181005  ...  0.000000  0.000000       0.0   \n",
       "3     0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1995  0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "1996  0.000000  0.190792  0.000000  ...  0.000000  0.000000       0.0   \n",
       "1997  0.984127  0.000000  0.000000  ...  0.087009  0.000000       0.0   \n",
       "1998  0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "1999  0.000000  0.000000  0.000000  ...  0.000000  0.000000       0.0   \n",
       "\n",
       "      tfidf_93  tfidf_94  tfidf_95  tfidf_96  tfidf_97  tfidf_98  tfidf_99  \n",
       "0     0.115594  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "1     0.000000  0.438598       0.0  0.163503       0.0  0.000000       0.0  \n",
       "2     0.000000  0.000000       0.0  0.000000       0.0  0.203564       0.0  \n",
       "3     0.000000  0.053659       0.0  0.015003       0.0  0.000000       0.0  \n",
       "4     0.000000  0.053659       0.0  0.015003       0.0  0.000000       0.0  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1995  0.000000  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "1996  0.000000  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "1997  0.000000  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "1998  0.000000  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "1999  0.000000  0.000000       0.0  0.000000       0.0  0.000000       0.0  \n",
       "\n",
       "[2000 rows x 100 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "208e0bd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:08:05.717385Z",
     "start_time": "2022-02-14T10:08:05.699470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4811 4811 4811 1897 4811 3315 2925 1340 1875 4...\n",
       "1       1435 1648 223 3178 2418 1614 3004 2511 2285 78...\n",
       "2       805 390 4252 3979 1228 2029 2029 2029 4252 923...\n",
       "3       3139 3139 3524 3139 3139 3524 3139 3139 3139 3...\n",
       "4       3139 3139 3524 3139 3139 3524 3139 3139 3139 3...\n",
       "                              ...                        \n",
       "1995    4696 4696 4696 4696 494 3863 4741 1251 2269 12...\n",
       "1996    696 899 3398 3398 3398 3398 3398 3398 3398 339...\n",
       "1997    3319 390 1338 1338 3149 3319 1338 3319 3319 33...\n",
       "1998    2588 3168 3168 2160 2160 2439 3168 2160 2588 4...\n",
       "1999    1500 540 1849 176 176 176 3028 176 176 176 176...\n",
       "Name: seller_path, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_test['seller_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e63f195d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:09:02.505322Z",
     "start_time": "2022-02-14T10:09:01.791114Z"
    }
   },
   "outputs": [],
   "source": [
    "#用W2V来实现label的embedding\n",
    "import gensim\n",
    "\n",
    "# Train Word2Vec model\n",
    "\n",
    "model = gensim.models.Word2Vec(all_data_test['seller_path'].apply(lambda x: x.split(' ')), vector_size=100, window=5, min_count=5, workers=4)\n",
    "# model.save(\"product2vec.model\")\n",
    "# model = gensim.models.Word2Vec.load(\"product2vec.model\")\n",
    "\n",
    "def mean_w2v_(x, model, size=100):\n",
    "    try:\n",
    "        i = 0\n",
    "        for word in x.split(' '):\n",
    "            if word in model.wv.vocab:\n",
    "                i += 1\n",
    "                if i == 1:\n",
    "                    vec = np.zeros(size)\n",
    "                vec += model.wv[word]\n",
    "        return vec / i \n",
    "    except:\n",
    "        return  np.zeros(size)\n",
    "\n",
    "\n",
    "def get_mean_w2v(df_data, columns, model, size):\n",
    "    data_array = []\n",
    "    for index, row in df_data.iterrows():\n",
    "        w2v = mean_w2v_(row[columns], model, size)\n",
    "        data_array.append(w2v)\n",
    "    return pd.DataFrame(data_array)\n",
    "\n",
    "df_embeeding = get_mean_w2v(all_data_test, 'seller_path', model, 100)\n",
    "df_embeeding.columns = ['embeeding_' + str(i) for i in df_embeeding.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "109124d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:09:06.277121Z",
     "start_time": "2022-02-14T10:09:06.253109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>embeeding_0</th>\n",
       "      <th>embeeding_1</th>\n",
       "      <th>embeeding_2</th>\n",
       "      <th>embeeding_3</th>\n",
       "      <th>embeeding_4</th>\n",
       "      <th>embeeding_5</th>\n",
       "      <th>embeeding_6</th>\n",
       "      <th>embeeding_7</th>\n",
       "      <th>embeeding_8</th>\n",
       "      <th>embeeding_9</th>\n",
       "      <th>...</th>\n",
       "      <th>embeeding_90</th>\n",
       "      <th>embeeding_91</th>\n",
       "      <th>embeeding_92</th>\n",
       "      <th>embeeding_93</th>\n",
       "      <th>embeeding_94</th>\n",
       "      <th>embeeding_95</th>\n",
       "      <th>embeeding_96</th>\n",
       "      <th>embeeding_97</th>\n",
       "      <th>embeeding_98</th>\n",
       "      <th>embeeding_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      embeeding_0  embeeding_1  embeeding_2  embeeding_3  embeeding_4  \\\n",
       "0             0.0          0.0          0.0          0.0          0.0   \n",
       "1             0.0          0.0          0.0          0.0          0.0   \n",
       "2             0.0          0.0          0.0          0.0          0.0   \n",
       "3             0.0          0.0          0.0          0.0          0.0   \n",
       "4             0.0          0.0          0.0          0.0          0.0   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1995          0.0          0.0          0.0          0.0          0.0   \n",
       "1996          0.0          0.0          0.0          0.0          0.0   \n",
       "1997          0.0          0.0          0.0          0.0          0.0   \n",
       "1998          0.0          0.0          0.0          0.0          0.0   \n",
       "1999          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "      embeeding_5  embeeding_6  embeeding_7  embeeding_8  embeeding_9  ...  \\\n",
       "0             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "2             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "3             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "4             0.0          0.0          0.0          0.0          0.0  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1995          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1996          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1997          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1998          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "1999          0.0          0.0          0.0          0.0          0.0  ...   \n",
       "\n",
       "      embeeding_90  embeeding_91  embeeding_92  embeeding_93  embeeding_94  \\\n",
       "0              0.0           0.0           0.0           0.0           0.0   \n",
       "1              0.0           0.0           0.0           0.0           0.0   \n",
       "2              0.0           0.0           0.0           0.0           0.0   \n",
       "3              0.0           0.0           0.0           0.0           0.0   \n",
       "4              0.0           0.0           0.0           0.0           0.0   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "1995           0.0           0.0           0.0           0.0           0.0   \n",
       "1996           0.0           0.0           0.0           0.0           0.0   \n",
       "1997           0.0           0.0           0.0           0.0           0.0   \n",
       "1998           0.0           0.0           0.0           0.0           0.0   \n",
       "1999           0.0           0.0           0.0           0.0           0.0   \n",
       "\n",
       "      embeeding_95  embeeding_96  embeeding_97  embeeding_98  embeeding_99  \n",
       "0              0.0           0.0           0.0           0.0           0.0  \n",
       "1              0.0           0.0           0.0           0.0           0.0  \n",
       "2              0.0           0.0           0.0           0.0           0.0  \n",
       "3              0.0           0.0           0.0           0.0           0.0  \n",
       "4              0.0           0.0           0.0           0.0           0.0  \n",
       "...            ...           ...           ...           ...           ...  \n",
       "1995           0.0           0.0           0.0           0.0           0.0  \n",
       "1996           0.0           0.0           0.0           0.0           0.0  \n",
       "1997           0.0           0.0           0.0           0.0           0.0  \n",
       "1998           0.0           0.0           0.0           0.0           0.0  \n",
       "1999           0.0           0.0           0.0           0.0           0.0  \n",
       "\n",
       "[2000 rows x 100 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_embeeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1bd65d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:01.132959Z",
     "start_time": "2022-02-14T10:10:01.115956Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_tfidf],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6a2ad17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:07.448050Z",
     "start_time": "2022-02-14T10:10:07.430039Z"
    }
   },
   "outputs": [],
   "source": [
    "all_data_test = pd.concat([all_data_test, df_embeeding],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5a63ea60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:19.104293Z",
     "start_time": "2022-02-14T10:10:18.288834Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 知识点六\n",
    "-- stacking特征\n",
    "\"\"\"\n",
    "# from sklearn.cross_validation import KFold\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "435b2524",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:31.283049Z",
     "start_time": "2022-02-14T10:10:31.266043Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 回归\n",
    "-- stacking 回归特征\n",
    "\"\"\"\n",
    "def stacking_reg(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict(te_x).reshape(-1,1)\n",
    "            train[test_index]=pre\n",
    "            test_pre[i,:]=clf.predict(test_x).reshape(-1,1)\n",
    "            cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x, label=te_y, missing=-1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'eval_metric': 'rmse',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'regression_l2',\n",
    "                      'metric': 'mse',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                train[test_index]=pre\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration).reshape(-1,1)\n",
    "                cv_scores.append(mean_squared_error(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestRegressor(n_estimators=600, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_reg(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf_reg\"\n",
    "\n",
    "def ada_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostRegressor(n_estimators=30, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_reg(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada_reg\"\n",
    "\n",
    "def gb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingRegressor(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_reg(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb_reg\"\n",
    "\n",
    "def et_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesRegressor(n_estimators=600, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_reg(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et_reg\"\n",
    "\n",
    "def lr_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lr_reg=LinearRegression(n_jobs=-1)\n",
    "    lr_train, lr_test = stacking_reg(lr_reg, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr_reg\"\n",
    "\n",
    "def xgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_reg(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb_reg\"\n",
    "\n",
    "def lgb_reg(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    lgb_train, lgb_test = stacking_reg(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return lgb_train, lgb_test,\"lgb_reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f9362ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:42.135998Z",
     "start_time": "2022-02-14T10:10:42.116994Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- 分类\n",
    "-- stacking 分类特征\n",
    "\"\"\"\n",
    "def stacking_clf(clf,train_x,train_y,test_x,clf_name,kf,label_split=None):\n",
    "    train=np.zeros((train_x.shape[0],1))\n",
    "    test=np.zeros((test_x.shape[0],1))\n",
    "    test_pre=np.empty((folds,test_x.shape[0],1))\n",
    "    cv_scores=[]\n",
    "    for i,(train_index,test_index) in enumerate(kf.split(train_x,label_split)):       \n",
    "        tr_x=train_x[train_index]\n",
    "        tr_y=train_y[train_index]\n",
    "        te_x=train_x[test_index]\n",
    "        te_y = train_y[test_index]\n",
    "\n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre=clf.predict_proba(te_x)\n",
    "            \n",
    "            train[test_index]=pre[:,0].reshape(-1,1)\n",
    "            test_pre[i,:]=clf.predict_proba(test_x)[:,0].reshape(-1,1)\n",
    "            \n",
    "            cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label=tr_y, missing=-1)\n",
    "            test_matrix = clf.DMatrix(te_x, label=te_y, missing=-1)\n",
    "            z = clf.DMatrix(test_x)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2\n",
    "                      }\n",
    "\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round=num_round,evals=watchlist,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(test_matrix,ntree_limit=model.best_ntree_limit)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(z, ntree_limit=model.best_ntree_limit)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label=tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label=te_y)\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      #'boosting_type': 'dart',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      \"num_class\": 2,\n",
    "                      'silent': True,\n",
    "                      }\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix,num_round,valid_sets=test_matrix,\n",
    "                                  early_stopping_rounds=early_stopping_rounds\n",
    "                                  )\n",
    "                pre= model.predict(te_x,num_iteration=model.best_iteration)\n",
    "                train[test_index]=pre[:,0].reshape(-1,1)\n",
    "                test_pre[i, :]= model.predict(test_x, num_iteration=model.best_iteration)[:,0].reshape(-1,1)\n",
    "                cv_scores.append(log_loss(te_y, pre[:,0].reshape(-1,1)))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    return train.reshape(-1,1),test.reshape(-1,1)\n",
    "\n",
    "def rf_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking_clf(randomforest, x_train, y_train, x_valid, \"rf\", kf, label_split=label_split)\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking_clf(adaboost, x_train, y_train, x_valid, \"ada\", kf, label_split=label_split)\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking_clf(gbdt, x_train, y_train, x_valid, \"gb\", kf, label_split=label_split)\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking_clf(extratree, x_train, y_train, x_valid, \"et\", kf, label_split=label_split)\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def xgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(xgboost, x_train, y_train, x_valid, \"xgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    xgb_train, xgb_test = stacking_clf(lightgbm, x_train, y_train, x_valid, \"lgb\", kf, label_split=label_split)\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking_clf(gnb, x_train, y_train, x_valid, \"gnb\", kf, label_split=label_split)\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking_clf(logisticregression, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def knn_clf(x_train, y_train, x_valid, kf, label_split=None):\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking_clf(kneighbors, x_train, y_train, x_valid, \"lr\", kf, label_split=label_split)\n",
    "    return knn_train, knn_test, \"knn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "75f42399",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:10:59.929985Z",
     "start_time": "2022-02-14T10:10:59.897969Z"
    }
   },
   "outputs": [],
   "source": [
    "#处理数据\n",
    "features_columns = [c for c in all_data_test.columns if c not in ['label', 'prob', 'seller_path', 'cat_path', 'brand_path', 'action_type_path', 'item_path', 'time_stamp_path']]\n",
    "x_train = all_data_test[~all_data_test['label'].isna()][features_columns].values\n",
    "y_train = all_data_test[~all_data_test['label'].isna()]['label'].values\n",
    "x_valid = all_data_test[all_data_test['label'].isna()][features_columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d7853648",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:11:05.454694Z",
     "start_time": "2022-02-14T10:11:05.438687Z"
    }
   },
   "outputs": [],
   "source": [
    "#处理inf和nan\n",
    "def get_matrix(data):\n",
    "    where_are_nan = np.isnan(data)\n",
    "    where_are_inf = np.isinf(data)\n",
    "    data[where_are_nan] = 0\n",
    "    data[where_are_inf] = 0\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5ef3fedf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:11:18.597482Z",
     "start_time": "2022-02-14T10:11:18.578364Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = np.float_(get_matrix(np.float_(x_train)))\n",
    "y_train = np.int_(y_train)\n",
    "x_valid = x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a760d848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:11:32.273947Z",
     "start_time": "2022-02-14T10:11:32.255937Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "folds = 5\n",
    "seed = 1\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0e00821c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:11:38.855954Z",
     "start_time": "2022-02-14T10:11:38.849951Z"
    }
   },
   "outputs": [],
   "source": [
    "# clf_list = [lgb_clf, xgb_clf, lgb_reg, xgb_reg]\n",
    "# clf_list_col = ['lgb_clf', 'xgb_clf', 'lgb_reg', 'xgb_reg']\n",
    "\n",
    "clf_list = [lgb_clf, xgb_clf]\n",
    "clf_list_col = ['lgb_clf', 'xgb_clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a2baf3c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:11:52.087909Z",
     "start_time": "2022-02-14T10:11:46.680276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001943 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7149\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.2941\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.293533\n",
      "[3]\tvalid_0's multi_logloss: 0.293309\n",
      "[4]\tvalid_0's multi_logloss: 0.292985\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.29292\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.293239\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.293335\n",
      "[8]\tvalid_0's multi_logloss: 0.292955\n",
      "[9]\tvalid_0's multi_logloss: 0.292493\n",
      "[10]\tvalid_0's multi_logloss: 0.292278\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.292112\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.292254\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[13]\tvalid_0's multi_logloss: 0.292591\n",
      "[14]\tvalid_0's multi_logloss: 0.292849\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[15]\tvalid_0's multi_logloss: 0.292989\n",
      "[16]\tvalid_0's multi_logloss: 0.293072\n",
      "[17]\tvalid_0's multi_logloss: 0.293061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[18]\tvalid_0's multi_logloss: 0.293184\n",
      "[19]\tvalid_0's multi_logloss: 0.293461\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.293806\n",
      "[21]\tvalid_0's multi_logloss: 0.293557\n",
      "[22]\tvalid_0's multi_logloss: 0.293778\n",
      "[23]\tvalid_0's multi_logloss: 0.293994\n",
      "[24]\tvalid_0's multi_logloss: 0.293759\n",
      "[25]\tvalid_0's multi_logloss: 0.293306\n",
      "[26]\tvalid_0's multi_logloss: 0.293661\n",
      "[27]\tvalid_0's multi_logloss: 0.293686\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[28]\tvalid_0's multi_logloss: 0.293729\n",
      "[29]\tvalid_0's multi_logloss: 0.293894\n",
      "[30]\tvalid_0's multi_logloss: 0.293536\n",
      "[31]\tvalid_0's multi_logloss: 0.29363\n",
      "[32]\tvalid_0's multi_logloss: 0.293792\n",
      "[33]\tvalid_0's multi_logloss: 0.293716\n",
      "[34]\tvalid_0's multi_logloss: 0.293702\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.293203\n",
      "[36]\tvalid_0's multi_logloss: 0.293108\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[37]\tvalid_0's multi_logloss: 0.293535\n",
      "[38]\tvalid_0's multi_logloss: 0.293693\n",
      "[39]\tvalid_0's multi_logloss: 0.29383\n",
      "[40]\tvalid_0's multi_logloss: 0.294166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[41]\tvalid_0's multi_logloss: 0.293645\n",
      "[42]\tvalid_0's multi_logloss: 0.29406\n",
      "[43]\tvalid_0's multi_logloss: 0.294467\n",
      "[44]\tvalid_0's multi_logloss: 0.29427\n",
      "[45]\tvalid_0's multi_logloss: 0.294452\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[46]\tvalid_0's multi_logloss: 0.294696\n",
      "[47]\tvalid_0's multi_logloss: 0.294757\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.294931\n",
      "[49]\tvalid_0's multi_logloss: 0.294998\n",
      "[50]\tvalid_0's multi_logloss: 0.295181\n",
      "[51]\tvalid_0's multi_logloss: 0.295276\n",
      "[52]\tvalid_0's multi_logloss: 0.295335\n",
      "[53]\tvalid_0's multi_logloss: 0.295182\n",
      "[54]\tvalid_0's multi_logloss: 0.295294\n",
      "[55]\tvalid_0's multi_logloss: 0.295312\n",
      "[56]\tvalid_0's multi_logloss: 0.295138\n",
      "[57]\tvalid_0's multi_logloss: 0.29517\n",
      "[58]\tvalid_0's multi_logloss: 0.294969\n",
      "[59]\tvalid_0's multi_logloss: 0.294871\n",
      "[60]\tvalid_0's multi_logloss: 0.294935\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[61]\tvalid_0's multi_logloss: 0.294986\n",
      "[62]\tvalid_0's multi_logloss: 0.295196\n",
      "[63]\tvalid_0's multi_logloss: 0.294952\n",
      "[64]\tvalid_0's multi_logloss: 0.295115\n",
      "[65]\tvalid_0's multi_logloss: 0.295085\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[66]\tvalid_0's multi_logloss: 0.295007\n",
      "[67]\tvalid_0's multi_logloss: 0.294977\n",
      "[68]\tvalid_0's multi_logloss: 0.295138\n",
      "[69]\tvalid_0's multi_logloss: 0.295182\n",
      "[70]\tvalid_0's multi_logloss: 0.295268\n",
      "[71]\tvalid_0's multi_logloss: 0.295308\n",
      "[72]\tvalid_0's multi_logloss: 0.295437\n",
      "[73]\tvalid_0's multi_logloss: 0.295747\n",
      "[74]\tvalid_0's multi_logloss: 0.295811\n",
      "[75]\tvalid_0's multi_logloss: 0.295919\n",
      "[76]\tvalid_0's multi_logloss: 0.295956\n",
      "[77]\tvalid_0's multi_logloss: 0.296007\n",
      "[78]\tvalid_0's multi_logloss: 0.296202\n",
      "[79]\tvalid_0's multi_logloss: 0.296598\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[80]\tvalid_0's multi_logloss: 0.296605\n",
      "[81]\tvalid_0's multi_logloss: 0.29688\n",
      "[82]\tvalid_0's multi_logloss: 0.296997\n",
      "[83]\tvalid_0's multi_logloss: 0.297184\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[84]\tvalid_0's multi_logloss: 0.297309\n",
      "[85]\tvalid_0's multi_logloss: 0.297541\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.297489\n",
      "[87]\tvalid_0's multi_logloss: 0.297564\n",
      "[88]\tvalid_0's multi_logloss: 0.29738\n",
      "[89]\tvalid_0's multi_logloss: 0.297789\n",
      "[90]\tvalid_0's multi_logloss: 0.297951\n",
      "[91]\tvalid_0's multi_logloss: 0.298052\n",
      "[92]\tvalid_0's multi_logloss: 0.298301\n",
      "[93]\tvalid_0's multi_logloss: 0.298319\n",
      "[94]\tvalid_0's multi_logloss: 0.298262\n",
      "[95]\tvalid_0's multi_logloss: 0.298518\n",
      "[96]\tvalid_0's multi_logloss: 0.298684\n",
      "[97]\tvalid_0's multi_logloss: 0.298408\n",
      "[98]\tvalid_0's multi_logloss: 0.298376\n",
      "[99]\tvalid_0's multi_logloss: 0.298395\n",
      "[100]\tvalid_0's multi_logloss: 0.298308\n",
      "[101]\tvalid_0's multi_logloss: 0.298738\n",
      "[102]\tvalid_0's multi_logloss: 0.299021\n",
      "[103]\tvalid_0's multi_logloss: 0.299113\n",
      "[104]\tvalid_0's multi_logloss: 0.299467\n",
      "[105]\tvalid_0's multi_logloss: 0.299607\n",
      "[106]\tvalid_0's multi_logloss: 0.299921\n",
      "[107]\tvalid_0's multi_logloss: 0.300194\n",
      "[108]\tvalid_0's multi_logloss: 0.300441\n",
      "[109]\tvalid_0's multi_logloss: 0.300814\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[110]\tvalid_0's multi_logloss: 0.300951\n",
      "[111]\tvalid_0's multi_logloss: 0.301012\n",
      "Early stopping, best iteration is:\n",
      "[11]\tvalid_0's multi_logloss: 0.292112\n",
      "lgb now score is: [2.56250120147197]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001691 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7188\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "[1]\tvalid_0's multi_logloss: 0.221114\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.220971\n",
      "[3]\tvalid_0's multi_logloss: 0.220748\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.220454\n",
      "[5]\tvalid_0's multi_logloss: 0.220325\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.219901\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.220001\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.219922\n",
      "[9]\tvalid_0's multi_logloss: 0.219529\n",
      "[10]\tvalid_0's multi_logloss: 0.219366\n",
      "[11]\tvalid_0's multi_logloss: 0.219126\n",
      "[12]\tvalid_0's multi_logloss: 0.219008\n",
      "[13]\tvalid_0's multi_logloss: 0.21893\n",
      "[14]\tvalid_0's multi_logloss: 0.218818\n",
      "[15]\tvalid_0's multi_logloss: 0.218586\n",
      "[16]\tvalid_0's multi_logloss: 0.21857\n",
      "[17]\tvalid_0's multi_logloss: 0.218491\n",
      "[18]\tvalid_0's multi_logloss: 0.218568\n",
      "[19]\tvalid_0's multi_logloss: 0.218632\n",
      "[20]\tvalid_0's multi_logloss: 0.218645\n",
      "[21]\tvalid_0's multi_logloss: 0.218731\n",
      "[22]\tvalid_0's multi_logloss: 0.218641\n",
      "[23]\tvalid_0's multi_logloss: 0.218713\n",
      "[24]\tvalid_0's multi_logloss: 0.218877\n",
      "[25]\tvalid_0's multi_logloss: 0.218809\n",
      "[26]\tvalid_0's multi_logloss: 0.218977\n",
      "[27]\tvalid_0's multi_logloss: 0.219061\n",
      "[28]\tvalid_0's multi_logloss: 0.219015\n",
      "[29]\tvalid_0's multi_logloss: 0.218798\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[30]\tvalid_0's multi_logloss: 0.218948\n",
      "[31]\tvalid_0's multi_logloss: 0.219287\n",
      "[32]\tvalid_0's multi_logloss: 0.219185\n",
      "[33]\tvalid_0's multi_logloss: 0.219248\n",
      "[34]\tvalid_0's multi_logloss: 0.219345\n",
      "[35]\tvalid_0's multi_logloss: 0.21922\n",
      "[36]\tvalid_0's multi_logloss: 0.219515\n",
      "[37]\tvalid_0's multi_logloss: 0.219395\n",
      "[38]\tvalid_0's multi_logloss: 0.219528\n",
      "[39]\tvalid_0's multi_logloss: 0.219549\n",
      "[40]\tvalid_0's multi_logloss: 0.219546\n",
      "[41]\tvalid_0's multi_logloss: 0.219388\n",
      "[42]\tvalid_0's multi_logloss: 0.219489\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[43]\tvalid_0's multi_logloss: 0.219454\n",
      "[44]\tvalid_0's multi_logloss: 0.219326\n",
      "[45]\tvalid_0's multi_logloss: 0.219374\n",
      "[46]\tvalid_0's multi_logloss: 0.219384\n",
      "[47]\tvalid_0's multi_logloss: 0.219367\n",
      "[48]\tvalid_0's multi_logloss: 0.219261\n",
      "[49]\tvalid_0's multi_logloss: 0.219253\n",
      "[50]\tvalid_0's multi_logloss: 0.219417\n",
      "[51]\tvalid_0's multi_logloss: 0.219656\n",
      "[52]\tvalid_0's multi_logloss: 0.219628\n",
      "[53]\tvalid_0's multi_logloss: 0.219878\n",
      "[54]\tvalid_0's multi_logloss: 0.220033\n",
      "[55]\tvalid_0's multi_logloss: 0.220147\n",
      "[56]\tvalid_0's multi_logloss: 0.220229\n",
      "[57]\tvalid_0's multi_logloss: 0.220585\n",
      "[58]\tvalid_0's multi_logloss: 0.220803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[59]\tvalid_0's multi_logloss: 0.221011\n",
      "[60]\tvalid_0's multi_logloss: 0.221013\n",
      "[61]\tvalid_0's multi_logloss: 0.221312\n",
      "[62]\tvalid_0's multi_logloss: 0.221398\n",
      "[63]\tvalid_0's multi_logloss: 0.221429\n",
      "[64]\tvalid_0's multi_logloss: 0.221434\n",
      "[65]\tvalid_0's multi_logloss: 0.221454\n",
      "[66]\tvalid_0's multi_logloss: 0.221817\n",
      "[67]\tvalid_0's multi_logloss: 0.222196\n",
      "[68]\tvalid_0's multi_logloss: 0.222327\n",
      "[69]\tvalid_0's multi_logloss: 0.222528\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[70]\tvalid_0's multi_logloss: 0.222726\n",
      "[71]\tvalid_0's multi_logloss: 0.222799\n",
      "[72]\tvalid_0's multi_logloss: 0.223155\n",
      "[73]\tvalid_0's multi_logloss: 0.223365\n",
      "[74]\tvalid_0's multi_logloss: 0.223627\n",
      "[75]\tvalid_0's multi_logloss: 0.224035\n",
      "[76]\tvalid_0's multi_logloss: 0.224164\n",
      "[77]\tvalid_0's multi_logloss: 0.224268\n",
      "[78]\tvalid_0's multi_logloss: 0.22433\n",
      "[79]\tvalid_0's multi_logloss: 0.224295\n",
      "[80]\tvalid_0's multi_logloss: 0.224515\n",
      "[81]\tvalid_0's multi_logloss: 0.224461\n",
      "[82]\tvalid_0's multi_logloss: 0.224541\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[83]\tvalid_0's multi_logloss: 0.224727\n",
      "[84]\tvalid_0's multi_logloss: 0.22476\n",
      "[85]\tvalid_0's multi_logloss: 0.224818\n",
      "[86]\tvalid_0's multi_logloss: 0.224842\n",
      "[87]\tvalid_0's multi_logloss: 0.22503\n",
      "[88]\tvalid_0's multi_logloss: 0.225008\n",
      "[89]\tvalid_0's multi_logloss: 0.224966\n",
      "[90]\tvalid_0's multi_logloss: 0.225037\n",
      "[91]\tvalid_0's multi_logloss: 0.225266\n",
      "[92]\tvalid_0's multi_logloss: 0.225185\n",
      "[93]\tvalid_0's multi_logloss: 0.225396\n",
      "[94]\tvalid_0's multi_logloss: 0.225646\n",
      "[95]\tvalid_0's multi_logloss: 0.225985\n",
      "[96]\tvalid_0's multi_logloss: 0.226135\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's multi_logloss: 0.226125\n",
      "[98]\tvalid_0's multi_logloss: 0.225685\n",
      "[99]\tvalid_0's multi_logloss: 0.225705\n",
      "[100]\tvalid_0's multi_logloss: 0.225878\n",
      "[101]\tvalid_0's multi_logloss: 0.225788\n",
      "[102]\tvalid_0's multi_logloss: 0.225919\n",
      "[103]\tvalid_0's multi_logloss: 0.226149\n",
      "[104]\tvalid_0's multi_logloss: 0.226244\n",
      "[105]\tvalid_0's multi_logloss: 0.226142\n",
      "[106]\tvalid_0's multi_logloss: 0.225901\n",
      "[107]\tvalid_0's multi_logloss: 0.226171\n",
      "[108]\tvalid_0's multi_logloss: 0.226154\n",
      "[109]\tvalid_0's multi_logloss: 0.226163\n",
      "[110]\tvalid_0's multi_logloss: 0.226392\n",
      "[111]\tvalid_0's multi_logloss: 0.226434\n",
      "[112]\tvalid_0's multi_logloss: 0.226579\n",
      "[113]\tvalid_0's multi_logloss: 0.226722\n",
      "[114]\tvalid_0's multi_logloss: 0.226763\n",
      "[115]\tvalid_0's multi_logloss: 0.226679\n",
      "[116]\tvalid_0's multi_logloss: 0.226718\n",
      "[117]\tvalid_0's multi_logloss: 0.226888\n",
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's multi_logloss: 0.218491\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001713 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7166\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 127\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073243\n",
      "[LightGBM] [Info] Start training from score -2.650371\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.221528\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[2]\tvalid_0's multi_logloss: 0.221857\n",
      "[3]\tvalid_0's multi_logloss: 0.22195\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[4]\tvalid_0's multi_logloss: 0.222166\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.22235\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.222322\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.222689\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.222731\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.22327\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's multi_logloss: 0.223744\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.224048\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.22436\n",
      "[13]\tvalid_0's multi_logloss: 0.224729\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[14]\tvalid_0's multi_logloss: 0.225048\n",
      "[15]\tvalid_0's multi_logloss: 0.225317\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.225558\n",
      "[17]\tvalid_0's multi_logloss: 0.225701\n",
      "[18]\tvalid_0's multi_logloss: 0.226052\n",
      "[19]\tvalid_0's multi_logloss: 0.226517\n",
      "[20]\tvalid_0's multi_logloss: 0.226809\n",
      "[21]\tvalid_0's multi_logloss: 0.226944\n",
      "[22]\tvalid_0's multi_logloss: 0.227411\n",
      "[23]\tvalid_0's multi_logloss: 0.227451\n",
      "[24]\tvalid_0's multi_logloss: 0.22778\n",
      "[25]\tvalid_0's multi_logloss: 0.227824\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[26]\tvalid_0's multi_logloss: 0.228031\n",
      "[27]\tvalid_0's multi_logloss: 0.228208\n",
      "[28]\tvalid_0's multi_logloss: 0.228396\n",
      "[29]\tvalid_0's multi_logloss: 0.228588\n",
      "[30]\tvalid_0's multi_logloss: 0.229055\n",
      "[31]\tvalid_0's multi_logloss: 0.229327\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[32]\tvalid_0's multi_logloss: 0.229414\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[33]\tvalid_0's multi_logloss: 0.229644\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[34]\tvalid_0's multi_logloss: 0.229947\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[35]\tvalid_0's multi_logloss: 0.230123\n",
      "[36]\tvalid_0's multi_logloss: 0.230157\n",
      "[37]\tvalid_0's multi_logloss: 0.230513\n",
      "[38]\tvalid_0's multi_logloss: 0.230879\n",
      "[39]\tvalid_0's multi_logloss: 0.230961\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[40]\tvalid_0's multi_logloss: 0.231261\n",
      "[41]\tvalid_0's multi_logloss: 0.231408\n",
      "[42]\tvalid_0's multi_logloss: 0.231569\n",
      "[43]\tvalid_0's multi_logloss: 0.232012\n",
      "[44]\tvalid_0's multi_logloss: 0.23252\n",
      "[45]\tvalid_0's multi_logloss: 0.232716\n",
      "[46]\tvalid_0's multi_logloss: 0.232886\n",
      "[47]\tvalid_0's multi_logloss: 0.233371\n",
      "[48]\tvalid_0's multi_logloss: 0.23358\n",
      "[49]\tvalid_0's multi_logloss: 0.233917\n",
      "[50]\tvalid_0's multi_logloss: 0.234192\n",
      "[51]\tvalid_0's multi_logloss: 0.234626\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[52]\tvalid_0's multi_logloss: 0.234935\n",
      "[53]\tvalid_0's multi_logloss: 0.235203\n",
      "[54]\tvalid_0's multi_logloss: 0.235502\n",
      "[55]\tvalid_0's multi_logloss: 0.235845\n",
      "[56]\tvalid_0's multi_logloss: 0.235998\n",
      "[57]\tvalid_0's multi_logloss: 0.23621\n",
      "[58]\tvalid_0's multi_logloss: 0.236269\n",
      "[59]\tvalid_0's multi_logloss: 0.236557\n",
      "[60]\tvalid_0's multi_logloss: 0.236804\n",
      "[61]\tvalid_0's multi_logloss: 0.237073\n",
      "[62]\tvalid_0's multi_logloss: 0.237402\n",
      "[63]\tvalid_0's multi_logloss: 0.237434\n",
      "[64]\tvalid_0's multi_logloss: 0.237773\n",
      "[65]\tvalid_0's multi_logloss: 0.238168\n",
      "[66]\tvalid_0's multi_logloss: 0.23838\n",
      "[67]\tvalid_0's multi_logloss: 0.238457\n",
      "[68]\tvalid_0's multi_logloss: 0.238722\n",
      "[69]\tvalid_0's multi_logloss: 0.238917\n",
      "[70]\tvalid_0's multi_logloss: 0.239219\n",
      "[71]\tvalid_0's multi_logloss: 0.23931\n",
      "[72]\tvalid_0's multi_logloss: 0.239478\n",
      "[73]\tvalid_0's multi_logloss: 0.23961\n",
      "[74]\tvalid_0's multi_logloss: 0.239768\n",
      "[75]\tvalid_0's multi_logloss: 0.239838\n",
      "[76]\tvalid_0's multi_logloss: 0.240033\n",
      "[77]\tvalid_0's multi_logloss: 0.240123\n",
      "[78]\tvalid_0's multi_logloss: 0.240078\n",
      "[79]\tvalid_0's multi_logloss: 0.240269\n",
      "[80]\tvalid_0's multi_logloss: 0.240477\n",
      "[81]\tvalid_0's multi_logloss: 0.240645\n",
      "[82]\tvalid_0's multi_logloss: 0.240881\n",
      "[83]\tvalid_0's multi_logloss: 0.240949\n",
      "[84]\tvalid_0's multi_logloss: 0.241447\n",
      "[85]\tvalid_0's multi_logloss: 0.241517\n",
      "[86]\tvalid_0's multi_logloss: 0.241701\n",
      "[87]\tvalid_0's multi_logloss: 0.241659\n",
      "[88]\tvalid_0's multi_logloss: 0.24167\n",
      "[89]\tvalid_0's multi_logloss: 0.242132\n",
      "[90]\tvalid_0's multi_logloss: 0.242125\n",
      "[91]\tvalid_0's multi_logloss: 0.242312\n",
      "[92]\tvalid_0's multi_logloss: 0.242255\n",
      "[93]\tvalid_0's multi_logloss: 0.242469\n",
      "[94]\tvalid_0's multi_logloss: 0.24249\n",
      "[95]\tvalid_0's multi_logloss: 0.242324\n",
      "[96]\tvalid_0's multi_logloss: 0.242329\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[97]\tvalid_0's multi_logloss: 0.242672\n",
      "[98]\tvalid_0's multi_logloss: 0.242931\n",
      "[99]\tvalid_0's multi_logloss: 0.243312\n",
      "[100]\tvalid_0's multi_logloss: 0.243358\n",
      "[101]\tvalid_0's multi_logloss: 0.243655\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's multi_logloss: 0.221528\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001453 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7089\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 126\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.065873\n",
      "[LightGBM] [Info] Start training from score -2.752786\n",
      "[1]\tvalid_0's multi_logloss: 0.293656\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.293021\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.292481\n",
      "[4]\tvalid_0's multi_logloss: 0.292034\n",
      "[5]\tvalid_0's multi_logloss: 0.291655\n",
      "[6]\tvalid_0's multi_logloss: 0.291417\n",
      "[7]\tvalid_0's multi_logloss: 0.291373\n",
      "[8]\tvalid_0's multi_logloss: 0.290956\n",
      "[9]\tvalid_0's multi_logloss: 0.290631\n",
      "[10]\tvalid_0's multi_logloss: 0.290425\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[11]\tvalid_0's multi_logloss: 0.29011\n",
      "[12]\tvalid_0's multi_logloss: 0.2902\n",
      "[13]\tvalid_0's multi_logloss: 0.290081\n",
      "[14]\tvalid_0's multi_logloss: 0.290042\n",
      "[15]\tvalid_0's multi_logloss: 0.289992\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.290012\n",
      "[17]\tvalid_0's multi_logloss: 0.290061\n",
      "[18]\tvalid_0's multi_logloss: 0.289938\n",
      "[19]\tvalid_0's multi_logloss: 0.290005\n",
      "[20]\tvalid_0's multi_logloss: 0.290059\n",
      "[21]\tvalid_0's multi_logloss: 0.289948\n",
      "[22]\tvalid_0's multi_logloss: 0.290233\n",
      "[23]\tvalid_0's multi_logloss: 0.28981\n",
      "[24]\tvalid_0's multi_logloss: 0.290023\n",
      "[25]\tvalid_0's multi_logloss: 0.290194\n",
      "[26]\tvalid_0's multi_logloss: 0.289988\n",
      "[27]\tvalid_0's multi_logloss: 0.290035\n",
      "[28]\tvalid_0's multi_logloss: 0.290102\n",
      "[29]\tvalid_0's multi_logloss: 0.290136\n",
      "[30]\tvalid_0's multi_logloss: 0.2903\n",
      "[31]\tvalid_0's multi_logloss: 0.290591\n",
      "[32]\tvalid_0's multi_logloss: 0.290626\n",
      "[33]\tvalid_0's multi_logloss: 0.290603\n",
      "[34]\tvalid_0's multi_logloss: 0.290639\n",
      "[35]\tvalid_0's multi_logloss: 0.290535\n",
      "[36]\tvalid_0's multi_logloss: 0.290658\n",
      "[37]\tvalid_0's multi_logloss: 0.290501\n",
      "[38]\tvalid_0's multi_logloss: 0.290416\n",
      "[39]\tvalid_0's multi_logloss: 0.290622\n",
      "[40]\tvalid_0's multi_logloss: 0.290641\n",
      "[41]\tvalid_0's multi_logloss: 0.290743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\tvalid_0's multi_logloss: 0.290705\n",
      "[43]\tvalid_0's multi_logloss: 0.290547\n",
      "[44]\tvalid_0's multi_logloss: 0.290546\n",
      "[45]\tvalid_0's multi_logloss: 0.290668\n",
      "[46]\tvalid_0's multi_logloss: 0.290974\n",
      "[47]\tvalid_0's multi_logloss: 0.290883\n",
      "[48]\tvalid_0's multi_logloss: 0.291027\n",
      "[49]\tvalid_0's multi_logloss: 0.29101\n",
      "[50]\tvalid_0's multi_logloss: 0.291058\n",
      "[51]\tvalid_0's multi_logloss: 0.290936\n",
      "[52]\tvalid_0's multi_logloss: 0.291074\n",
      "[53]\tvalid_0's multi_logloss: 0.291355\n",
      "[54]\tvalid_0's multi_logloss: 0.2914\n",
      "[55]\tvalid_0's multi_logloss: 0.291556\n",
      "[56]\tvalid_0's multi_logloss: 0.291499\n",
      "[57]\tvalid_0's multi_logloss: 0.29149\n",
      "[58]\tvalid_0's multi_logloss: 0.291817\n",
      "[59]\tvalid_0's multi_logloss: 0.292092\n",
      "[60]\tvalid_0's multi_logloss: 0.291902\n",
      "[61]\tvalid_0's multi_logloss: 0.291846\n",
      "[62]\tvalid_0's multi_logloss: 0.292088\n",
      "[63]\tvalid_0's multi_logloss: 0.291935\n",
      "[64]\tvalid_0's multi_logloss: 0.291658\n",
      "[65]\tvalid_0's multi_logloss: 0.291977\n",
      "[66]\tvalid_0's multi_logloss: 0.29208\n",
      "[67]\tvalid_0's multi_logloss: 0.292006\n",
      "[68]\tvalid_0's multi_logloss: 0.292049\n",
      "[69]\tvalid_0's multi_logloss: 0.292219\n",
      "[70]\tvalid_0's multi_logloss: 0.29216\n",
      "[71]\tvalid_0's multi_logloss: 0.292237\n",
      "[72]\tvalid_0's multi_logloss: 0.292462\n",
      "[73]\tvalid_0's multi_logloss: 0.292606\n",
      "[74]\tvalid_0's multi_logloss: 0.29259\n",
      "[75]\tvalid_0's multi_logloss: 0.292724\n",
      "[76]\tvalid_0's multi_logloss: 0.292817\n",
      "[77]\tvalid_0's multi_logloss: 0.292865\n",
      "[78]\tvalid_0's multi_logloss: 0.292625\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[79]\tvalid_0's multi_logloss: 0.292672\n",
      "[80]\tvalid_0's multi_logloss: 0.292687\n",
      "[81]\tvalid_0's multi_logloss: 0.292669\n",
      "[82]\tvalid_0's multi_logloss: 0.292789\n",
      "[83]\tvalid_0's multi_logloss: 0.293039\n",
      "[84]\tvalid_0's multi_logloss: 0.29313\n",
      "[85]\tvalid_0's multi_logloss: 0.293202\n",
      "[86]\tvalid_0's multi_logloss: 0.293399\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[87]\tvalid_0's multi_logloss: 0.293253\n",
      "[88]\tvalid_0's multi_logloss: 0.293219\n",
      "[89]\tvalid_0's multi_logloss: 0.293344\n",
      "[90]\tvalid_0's multi_logloss: 0.293555\n",
      "[91]\tvalid_0's multi_logloss: 0.293482\n",
      "[92]\tvalid_0's multi_logloss: 0.293483\n",
      "[93]\tvalid_0's multi_logloss: 0.29356\n",
      "[94]\tvalid_0's multi_logloss: 0.29376\n",
      "[95]\tvalid_0's multi_logloss: 0.293899\n",
      "[96]\tvalid_0's multi_logloss: 0.293834\n",
      "[97]\tvalid_0's multi_logloss: 0.293854\n",
      "[98]\tvalid_0's multi_logloss: 0.2941\n",
      "[99]\tvalid_0's multi_logloss: 0.294109\n",
      "[100]\tvalid_0's multi_logloss: 0.293975\n",
      "[101]\tvalid_0's multi_logloss: 0.293841\n",
      "[102]\tvalid_0's multi_logloss: 0.29372\n",
      "[103]\tvalid_0's multi_logloss: 0.293947\n",
      "[104]\tvalid_0's multi_logloss: 0.293948\n",
      "[105]\tvalid_0's multi_logloss: 0.294149\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[106]\tvalid_0's multi_logloss: 0.29437\n",
      "[107]\tvalid_0's multi_logloss: 0.294577\n",
      "[108]\tvalid_0's multi_logloss: 0.294593\n",
      "[109]\tvalid_0's multi_logloss: 0.29462\n",
      "[110]\tvalid_0's multi_logloss: 0.295016\n",
      "[111]\tvalid_0's multi_logloss: 0.295244\n",
      "[112]\tvalid_0's multi_logloss: 0.295403\n",
      "[113]\tvalid_0's multi_logloss: 0.295517\n",
      "[114]\tvalid_0's multi_logloss: 0.295885\n",
      "[115]\tvalid_0's multi_logloss: 0.29581\n",
      "[116]\tvalid_0's multi_logloss: 0.295988\n",
      "[117]\tvalid_0's multi_logloss: 0.296078\n",
      "[118]\tvalid_0's multi_logloss: 0.296346\n",
      "[119]\tvalid_0's multi_logloss: 0.296195\n",
      "[120]\tvalid_0's multi_logloss: 0.296395\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[121]\tvalid_0's multi_logloss: 0.296646\n",
      "[122]\tvalid_0's multi_logloss: 0.296592\n",
      "[123]\tvalid_0's multi_logloss: 0.296633\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's multi_logloss: 0.28981\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474]\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7210\n",
      "[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 126\n",
      "[LightGBM] [Warning] Unknown parameter: colsample_bylevel\n",
      "[LightGBM] [Warning] Unknown parameter: tree_method\n",
      "[LightGBM] [Warning] Unknown parameter: silent\n",
      "[LightGBM] [Info] Start training from score -0.073916\n",
      "[LightGBM] [Info] Start training from score -2.641560\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[1]\tvalid_0's multi_logloss: 0.215211\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.215105\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[3]\tvalid_0's multi_logloss: 0.214966\n",
      "[4]\tvalid_0's multi_logloss: 0.214635\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[5]\tvalid_0's multi_logloss: 0.214574\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[6]\tvalid_0's multi_logloss: 0.214443\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[7]\tvalid_0's multi_logloss: 0.2142\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[8]\tvalid_0's multi_logloss: 0.214103\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[9]\tvalid_0's multi_logloss: 0.213916\n",
      "[10]\tvalid_0's multi_logloss: 0.213851\n",
      "[11]\tvalid_0's multi_logloss: 0.213697\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[12]\tvalid_0's multi_logloss: 0.213646\n",
      "[13]\tvalid_0's multi_logloss: 0.21356\n",
      "[14]\tvalid_0's multi_logloss: 0.213609\n",
      "[15]\tvalid_0's multi_logloss: 0.213354\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[16]\tvalid_0's multi_logloss: 0.213583\n",
      "[17]\tvalid_0's multi_logloss: 0.213567\n",
      "[18]\tvalid_0's multi_logloss: 0.213753\n",
      "[19]\tvalid_0's multi_logloss: 0.213901\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[20]\tvalid_0's multi_logloss: 0.214076\n",
      "[21]\tvalid_0's multi_logloss: 0.214227\n",
      "[22]\tvalid_0's multi_logloss: 0.214338\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[23]\tvalid_0's multi_logloss: 0.214232\n",
      "[24]\tvalid_0's multi_logloss: 0.214347\n",
      "[25]\tvalid_0's multi_logloss: 0.214371\n",
      "[26]\tvalid_0's multi_logloss: 0.214104\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[27]\tvalid_0's multi_logloss: 0.214268\n",
      "[28]\tvalid_0's multi_logloss: 0.214136\n",
      "[29]\tvalid_0's multi_logloss: 0.214369\n",
      "[30]\tvalid_0's multi_logloss: 0.214434\n",
      "[31]\tvalid_0's multi_logloss: 0.214142\n",
      "[32]\tvalid_0's multi_logloss: 0.214415\n",
      "[33]\tvalid_0's multi_logloss: 0.214615\n",
      "[34]\tvalid_0's multi_logloss: 0.214818\n",
      "[35]\tvalid_0's multi_logloss: 0.215004\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[36]\tvalid_0's multi_logloss: 0.214995\n",
      "[37]\tvalid_0's multi_logloss: 0.214915\n",
      "[38]\tvalid_0's multi_logloss: 0.215043\n",
      "[39]\tvalid_0's multi_logloss: 0.215168\n",
      "[40]\tvalid_0's multi_logloss: 0.215596\n",
      "[41]\tvalid_0's multi_logloss: 0.215761\n",
      "[42]\tvalid_0's multi_logloss: 0.215772\n",
      "[43]\tvalid_0's multi_logloss: 0.215701\n",
      "[44]\tvalid_0's multi_logloss: 0.215671\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[45]\tvalid_0's multi_logloss: 0.216097\n",
      "[46]\tvalid_0's multi_logloss: 0.216403\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[47]\tvalid_0's multi_logloss: 0.216812\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[48]\tvalid_0's multi_logloss: 0.21698\n",
      "[49]\tvalid_0's multi_logloss: 0.217035\n",
      "[50]\tvalid_0's multi_logloss: 0.217251\n",
      "[51]\tvalid_0's multi_logloss: 0.217107\n",
      "[52]\tvalid_0's multi_logloss: 0.217062\n",
      "[53]\tvalid_0's multi_logloss: 0.217163\n",
      "[54]\tvalid_0's multi_logloss: 0.21753\n",
      "[55]\tvalid_0's multi_logloss: 0.217594\n",
      "[56]\tvalid_0's multi_logloss: 0.217704\n",
      "[57]\tvalid_0's multi_logloss: 0.217618\n",
      "[58]\tvalid_0's multi_logloss: 0.217778\n",
      "[59]\tvalid_0's multi_logloss: 0.218142\n",
      "[60]\tvalid_0's multi_logloss: 0.218383\n",
      "[61]\tvalid_0's multi_logloss: 0.218346\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[62]\tvalid_0's multi_logloss: 0.218499\n",
      "[63]\tvalid_0's multi_logloss: 0.218359\n",
      "[64]\tvalid_0's multi_logloss: 0.21835\n",
      "[65]\tvalid_0's multi_logloss: 0.218586\n",
      "[66]\tvalid_0's multi_logloss: 0.218658\n",
      "[67]\tvalid_0's multi_logloss: 0.218697\n",
      "[68]\tvalid_0's multi_logloss: 0.218751\n",
      "[69]\tvalid_0's multi_logloss: 0.218971\n",
      "[70]\tvalid_0's multi_logloss: 0.219035\n",
      "[71]\tvalid_0's multi_logloss: 0.219206\n",
      "[72]\tvalid_0's multi_logloss: 0.219163\n",
      "[73]\tvalid_0's multi_logloss: 0.219238\n",
      "[74]\tvalid_0's multi_logloss: 0.219357\n",
      "[75]\tvalid_0's multi_logloss: 0.219573\n",
      "[76]\tvalid_0's multi_logloss: 0.219711\n",
      "[77]\tvalid_0's multi_logloss: 0.21988\n",
      "[78]\tvalid_0's multi_logloss: 0.220107\n",
      "[79]\tvalid_0's multi_logloss: 0.220179\n",
      "[80]\tvalid_0's multi_logloss: 0.220392\n",
      "[81]\tvalid_0's multi_logloss: 0.220562\n",
      "[82]\tvalid_0's multi_logloss: 0.220507\n",
      "[83]\tvalid_0's multi_logloss: 0.220714\n",
      "[84]\tvalid_0's multi_logloss: 0.220812\n",
      "[85]\tvalid_0's multi_logloss: 0.220846\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[86]\tvalid_0's multi_logloss: 0.221072\n",
      "[87]\tvalid_0's multi_logloss: 0.221231\n",
      "[88]\tvalid_0's multi_logloss: 0.221227\n",
      "[89]\tvalid_0's multi_logloss: 0.221265\n",
      "[90]\tvalid_0's multi_logloss: 0.221485\n",
      "[91]\tvalid_0's multi_logloss: 0.221632\n",
      "[92]\tvalid_0's multi_logloss: 0.221681\n",
      "[93]\tvalid_0's multi_logloss: 0.222016\n",
      "[94]\tvalid_0's multi_logloss: 0.221897\n",
      "[95]\tvalid_0's multi_logloss: 0.221829\n",
      "[96]\tvalid_0's multi_logloss: 0.221865\n",
      "[97]\tvalid_0's multi_logloss: 0.222044\n",
      "[98]\tvalid_0's multi_logloss: 0.222038\n",
      "[99]\tvalid_0's multi_logloss: 0.222244\n",
      "[100]\tvalid_0's multi_logloss: 0.222457\n",
      "[101]\tvalid_0's multi_logloss: 0.222508\n",
      "[102]\tvalid_0's multi_logloss: 0.222594\n",
      "[103]\tvalid_0's multi_logloss: 0.222666\n",
      "[104]\tvalid_0's multi_logloss: 0.222908\n",
      "[105]\tvalid_0's multi_logloss: 0.223133\n",
      "[106]\tvalid_0's multi_logloss: 0.223405\n",
      "[107]\tvalid_0's multi_logloss: 0.22333\n",
      "[108]\tvalid_0's multi_logloss: 0.223333\n",
      "[109]\tvalid_0's multi_logloss: 0.223556\n",
      "[110]\tvalid_0's multi_logloss: 0.223722\n",
      "[111]\tvalid_0's multi_logloss: 0.223819\n",
      "[112]\tvalid_0's multi_logloss: 0.223939\n",
      "[113]\tvalid_0's multi_logloss: 0.224093\n",
      "[114]\tvalid_0's multi_logloss: 0.224246\n",
      "[115]\tvalid_0's multi_logloss: 0.224081\n",
      "Early stopping, best iteration is:\n",
      "[15]\tvalid_0's multi_logloss: 0.213354\n",
      "lgb now score is: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474, 2.56177386068988]\n",
      "lgb_score_list: [2.56250120147197, 2.565956170400075, 2.5049622881701676, 2.5875705148000474, 2.56177386068988]\n",
      "lgb_score_mean: 2.556552807106428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:0.67087\teval-mlogloss:0.67237\n",
      "[1]\ttrain-mlogloss:0.64996\teval-mlogloss:0.65274\n",
      "[2]\ttrain-mlogloss:0.62993\teval-mlogloss:0.63393\n",
      "[3]\ttrain-mlogloss:0.61127\teval-mlogloss:0.61656\n",
      "[4]\ttrain-mlogloss:0.59351\teval-mlogloss:0.59998\n",
      "[5]\ttrain-mlogloss:0.57657\teval-mlogloss:0.58419\n",
      "[6]\ttrain-mlogloss:0.56064\teval-mlogloss:0.56940\n",
      "[7]\ttrain-mlogloss:0.54550\teval-mlogloss:0.55517\n",
      "[8]\ttrain-mlogloss:0.53123\teval-mlogloss:0.54203\n",
      "[9]\ttrain-mlogloss:0.51758\teval-mlogloss:0.52964\n",
      "[10]\ttrain-mlogloss:0.50460\teval-mlogloss:0.51774\n",
      "[11]\ttrain-mlogloss:0.49242\teval-mlogloss:0.50664\n",
      "[12]\ttrain-mlogloss:0.48057\teval-mlogloss:0.49591\n",
      "[13]\ttrain-mlogloss:0.46934\teval-mlogloss:0.48582\n",
      "[14]\ttrain-mlogloss:0.45864\teval-mlogloss:0.47599\n",
      "[15]\ttrain-mlogloss:0.44839\teval-mlogloss:0.46671\n",
      "[16]\ttrain-mlogloss:0.43870\teval-mlogloss:0.45804\n",
      "[17]\ttrain-mlogloss:0.42953\teval-mlogloss:0.44992\n",
      "[18]\ttrain-mlogloss:0.42068\teval-mlogloss:0.44205\n",
      "[19]\ttrain-mlogloss:0.41222\teval-mlogloss:0.43449\n",
      "[20]\ttrain-mlogloss:0.40404\teval-mlogloss:0.42738\n",
      "[21]\ttrain-mlogloss:0.39585\teval-mlogloss:0.42008\n",
      "[22]\ttrain-mlogloss:0.38835\teval-mlogloss:0.41363\n",
      "[23]\ttrain-mlogloss:0.38111\teval-mlogloss:0.40719\n",
      "[24]\ttrain-mlogloss:0.37426\teval-mlogloss:0.40127\n",
      "[25]\ttrain-mlogloss:0.36764\teval-mlogloss:0.39545\n",
      "[26]\ttrain-mlogloss:0.36136\teval-mlogloss:0.39002\n",
      "[27]\ttrain-mlogloss:0.35543\teval-mlogloss:0.38510\n",
      "[28]\ttrain-mlogloss:0.34956\teval-mlogloss:0.38007\n",
      "[29]\ttrain-mlogloss:0.34398\teval-mlogloss:0.37540\n",
      "[30]\ttrain-mlogloss:0.33860\teval-mlogloss:0.37102\n",
      "[31]\ttrain-mlogloss:0.33369\teval-mlogloss:0.36674\n",
      "[32]\ttrain-mlogloss:0.32884\teval-mlogloss:0.36301\n",
      "[33]\ttrain-mlogloss:0.32426\teval-mlogloss:0.35931\n",
      "[34]\ttrain-mlogloss:0.31967\teval-mlogloss:0.35552\n",
      "[35]\ttrain-mlogloss:0.31517\teval-mlogloss:0.35180\n",
      "[36]\ttrain-mlogloss:0.31082\teval-mlogloss:0.34848\n",
      "[37]\ttrain-mlogloss:0.30683\teval-mlogloss:0.34557\n",
      "[38]\ttrain-mlogloss:0.30291\teval-mlogloss:0.34265\n",
      "[39]\ttrain-mlogloss:0.29930\teval-mlogloss:0.33982\n",
      "[40]\ttrain-mlogloss:0.29584\teval-mlogloss:0.33721\n",
      "[41]\ttrain-mlogloss:0.29235\teval-mlogloss:0.33442\n",
      "[42]\ttrain-mlogloss:0.28903\teval-mlogloss:0.33199\n",
      "[43]\ttrain-mlogloss:0.28586\teval-mlogloss:0.32965\n",
      "[44]\ttrain-mlogloss:0.28278\teval-mlogloss:0.32734\n",
      "[45]\ttrain-mlogloss:0.27969\teval-mlogloss:0.32531\n",
      "[46]\ttrain-mlogloss:0.27693\teval-mlogloss:0.32321\n",
      "[47]\ttrain-mlogloss:0.27422\teval-mlogloss:0.32138\n",
      "[48]\ttrain-mlogloss:0.27160\teval-mlogloss:0.31962\n",
      "[49]\ttrain-mlogloss:0.26900\teval-mlogloss:0.31772\n",
      "[50]\ttrain-mlogloss:0.26655\teval-mlogloss:0.31608\n",
      "[51]\ttrain-mlogloss:0.26403\teval-mlogloss:0.31448\n",
      "[52]\ttrain-mlogloss:0.26161\teval-mlogloss:0.31315\n",
      "[53]\ttrain-mlogloss:0.25944\teval-mlogloss:0.31166\n",
      "[54]\ttrain-mlogloss:0.25732\teval-mlogloss:0.31021\n",
      "[55]\ttrain-mlogloss:0.25527\teval-mlogloss:0.30895\n",
      "[56]\ttrain-mlogloss:0.25333\teval-mlogloss:0.30794\n",
      "[57]\ttrain-mlogloss:0.25131\teval-mlogloss:0.30682\n",
      "[58]\ttrain-mlogloss:0.24934\teval-mlogloss:0.30566\n",
      "[59]\ttrain-mlogloss:0.24751\teval-mlogloss:0.30472\n",
      "[60]\ttrain-mlogloss:0.24569\teval-mlogloss:0.30379\n",
      "[61]\ttrain-mlogloss:0.24413\teval-mlogloss:0.30289\n",
      "[62]\ttrain-mlogloss:0.24235\teval-mlogloss:0.30196\n",
      "[63]\ttrain-mlogloss:0.24056\teval-mlogloss:0.30117\n",
      "[64]\ttrain-mlogloss:0.23889\teval-mlogloss:0.30046\n",
      "[65]\ttrain-mlogloss:0.23735\teval-mlogloss:0.29977\n",
      "[66]\ttrain-mlogloss:0.23609\teval-mlogloss:0.29913\n",
      "[67]\ttrain-mlogloss:0.23458\teval-mlogloss:0.29850\n",
      "[68]\ttrain-mlogloss:0.23334\teval-mlogloss:0.29795\n",
      "[69]\ttrain-mlogloss:0.23201\teval-mlogloss:0.29756\n",
      "[70]\ttrain-mlogloss:0.23083\teval-mlogloss:0.29708\n",
      "[71]\ttrain-mlogloss:0.22953\teval-mlogloss:0.29648\n",
      "[72]\ttrain-mlogloss:0.22821\teval-mlogloss:0.29598\n",
      "[73]\ttrain-mlogloss:0.22703\teval-mlogloss:0.29545\n",
      "[74]\ttrain-mlogloss:0.22580\teval-mlogloss:0.29492\n",
      "[75]\ttrain-mlogloss:0.22466\teval-mlogloss:0.29459\n",
      "[76]\ttrain-mlogloss:0.22359\teval-mlogloss:0.29446\n",
      "[77]\ttrain-mlogloss:0.22245\teval-mlogloss:0.29434\n",
      "[78]\ttrain-mlogloss:0.22131\teval-mlogloss:0.29407\n",
      "[79]\ttrain-mlogloss:0.22030\teval-mlogloss:0.29377\n",
      "[80]\ttrain-mlogloss:0.21937\teval-mlogloss:0.29352\n",
      "[81]\ttrain-mlogloss:0.21854\teval-mlogloss:0.29326\n",
      "[82]\ttrain-mlogloss:0.21758\teval-mlogloss:0.29316\n",
      "[83]\ttrain-mlogloss:0.21663\teval-mlogloss:0.29283\n",
      "[84]\ttrain-mlogloss:0.21583\teval-mlogloss:0.29259\n",
      "[85]\ttrain-mlogloss:0.21490\teval-mlogloss:0.29219\n",
      "[86]\ttrain-mlogloss:0.21400\teval-mlogloss:0.29201\n",
      "[87]\ttrain-mlogloss:0.21322\teval-mlogloss:0.29178\n",
      "[88]\ttrain-mlogloss:0.21235\teval-mlogloss:0.29163\n",
      "[89]\ttrain-mlogloss:0.21170\teval-mlogloss:0.29160\n",
      "[90]\ttrain-mlogloss:0.21092\teval-mlogloss:0.29131\n",
      "[91]\ttrain-mlogloss:0.21018\teval-mlogloss:0.29123\n",
      "[92]\ttrain-mlogloss:0.20926\teval-mlogloss:0.29114\n",
      "[93]\ttrain-mlogloss:0.20861\teval-mlogloss:0.29134\n",
      "[94]\ttrain-mlogloss:0.20790\teval-mlogloss:0.29132\n",
      "[95]\ttrain-mlogloss:0.20726\teval-mlogloss:0.29148\n",
      "[96]\ttrain-mlogloss:0.20649\teval-mlogloss:0.29140\n",
      "[97]\ttrain-mlogloss:0.20588\teval-mlogloss:0.29145\n",
      "[98]\ttrain-mlogloss:0.20516\teval-mlogloss:0.29156\n",
      "[99]\ttrain-mlogloss:0.20454\teval-mlogloss:0.29141\n",
      "[100]\ttrain-mlogloss:0.20386\teval-mlogloss:0.29123\n",
      "[101]\ttrain-mlogloss:0.20325\teval-mlogloss:0.29126\n",
      "[102]\ttrain-mlogloss:0.20252\teval-mlogloss:0.29112\n",
      "[103]\ttrain-mlogloss:0.20191\teval-mlogloss:0.29128\n",
      "[104]\ttrain-mlogloss:0.20136\teval-mlogloss:0.29119\n",
      "[105]\ttrain-mlogloss:0.20076\teval-mlogloss:0.29138\n",
      "[106]\ttrain-mlogloss:0.20009\teval-mlogloss:0.29130\n",
      "[107]\ttrain-mlogloss:0.19950\teval-mlogloss:0.29127\n",
      "[108]\ttrain-mlogloss:0.19875\teval-mlogloss:0.29112\n",
      "[109]\ttrain-mlogloss:0.19837\teval-mlogloss:0.29110\n",
      "[110]\ttrain-mlogloss:0.19776\teval-mlogloss:0.29159\n",
      "[111]\ttrain-mlogloss:0.19739\teval-mlogloss:0.29172\n",
      "[112]\ttrain-mlogloss:0.19679\teval-mlogloss:0.29182\n",
      "[113]\ttrain-mlogloss:0.19632\teval-mlogloss:0.29200\n",
      "[114]\ttrain-mlogloss:0.19580\teval-mlogloss:0.29195\n",
      "[115]\ttrain-mlogloss:0.19520\teval-mlogloss:0.29223\n",
      "[116]\ttrain-mlogloss:0.19448\teval-mlogloss:0.29227\n",
      "[117]\ttrain-mlogloss:0.19389\teval-mlogloss:0.29243\n",
      "[118]\ttrain-mlogloss:0.19339\teval-mlogloss:0.29247\n",
      "[119]\ttrain-mlogloss:0.19286\teval-mlogloss:0.29269\n",
      "[120]\ttrain-mlogloss:0.19230\teval-mlogloss:0.29292\n",
      "[121]\ttrain-mlogloss:0.19177\teval-mlogloss:0.29288\n",
      "[122]\ttrain-mlogloss:0.19130\teval-mlogloss:0.29277\n",
      "[123]\ttrain-mlogloss:0.19066\teval-mlogloss:0.29295\n",
      "[124]\ttrain-mlogloss:0.19012\teval-mlogloss:0.29317\n",
      "[125]\ttrain-mlogloss:0.18981\teval-mlogloss:0.29346\n",
      "[126]\ttrain-mlogloss:0.18936\teval-mlogloss:0.29371\n",
      "[127]\ttrain-mlogloss:0.18885\teval-mlogloss:0.29386\n",
      "[128]\ttrain-mlogloss:0.18845\teval-mlogloss:0.29376\n",
      "[129]\ttrain-mlogloss:0.18800\teval-mlogloss:0.29412\n",
      "[130]\ttrain-mlogloss:0.18752\teval-mlogloss:0.29409\n",
      "[131]\ttrain-mlogloss:0.18704\teval-mlogloss:0.29396\n",
      "[132]\ttrain-mlogloss:0.18647\teval-mlogloss:0.29386\n",
      "[133]\ttrain-mlogloss:0.18607\teval-mlogloss:0.29383\n",
      "[134]\ttrain-mlogloss:0.18568\teval-mlogloss:0.29374\n",
      "[135]\ttrain-mlogloss:0.18529\teval-mlogloss:0.29403\n",
      "[136]\ttrain-mlogloss:0.18489\teval-mlogloss:0.29413\n",
      "[137]\ttrain-mlogloss:0.18449\teval-mlogloss:0.29424\n",
      "[138]\ttrain-mlogloss:0.18408\teval-mlogloss:0.29451\n",
      "[139]\ttrain-mlogloss:0.18382\teval-mlogloss:0.29459\n",
      "[140]\ttrain-mlogloss:0.18332\teval-mlogloss:0.29442\n",
      "[141]\ttrain-mlogloss:0.18278\teval-mlogloss:0.29433\n",
      "[142]\ttrain-mlogloss:0.18239\teval-mlogloss:0.29435\n",
      "[143]\ttrain-mlogloss:0.18185\teval-mlogloss:0.29403\n",
      "[144]\ttrain-mlogloss:0.18142\teval-mlogloss:0.29403\n",
      "[145]\ttrain-mlogloss:0.18085\teval-mlogloss:0.29398\n",
      "[146]\ttrain-mlogloss:0.18044\teval-mlogloss:0.29398\n",
      "[147]\ttrain-mlogloss:0.18015\teval-mlogloss:0.29409\n",
      "[148]\ttrain-mlogloss:0.17977\teval-mlogloss:0.29419\n",
      "[149]\ttrain-mlogloss:0.17922\teval-mlogloss:0.29420\n",
      "[150]\ttrain-mlogloss:0.17888\teval-mlogloss:0.29444\n",
      "[151]\ttrain-mlogloss:0.17849\teval-mlogloss:0.29432\n",
      "[152]\ttrain-mlogloss:0.17812\teval-mlogloss:0.29397\n",
      "[153]\ttrain-mlogloss:0.17774\teval-mlogloss:0.29392\n",
      "[154]\ttrain-mlogloss:0.17728\teval-mlogloss:0.29413\n",
      "[155]\ttrain-mlogloss:0.17695\teval-mlogloss:0.29427\n",
      "[156]\ttrain-mlogloss:0.17663\teval-mlogloss:0.29433\n",
      "[157]\ttrain-mlogloss:0.17622\teval-mlogloss:0.29416\n",
      "[158]\ttrain-mlogloss:0.17586\teval-mlogloss:0.29407\n",
      "[159]\ttrain-mlogloss:0.17550\teval-mlogloss:0.29410\n",
      "[160]\ttrain-mlogloss:0.17511\teval-mlogloss:0.29406\n",
      "[161]\ttrain-mlogloss:0.17472\teval-mlogloss:0.29402\n",
      "[162]\ttrain-mlogloss:0.17407\teval-mlogloss:0.29422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[163]\ttrain-mlogloss:0.17375\teval-mlogloss:0.29435\n",
      "[164]\ttrain-mlogloss:0.17348\teval-mlogloss:0.29436\n",
      "[165]\ttrain-mlogloss:0.17313\teval-mlogloss:0.29415\n",
      "[166]\ttrain-mlogloss:0.17274\teval-mlogloss:0.29402\n",
      "[167]\ttrain-mlogloss:0.17239\teval-mlogloss:0.29402\n",
      "[168]\ttrain-mlogloss:0.17202\teval-mlogloss:0.29412\n",
      "[169]\ttrain-mlogloss:0.17168\teval-mlogloss:0.29403\n",
      "[170]\ttrain-mlogloss:0.17131\teval-mlogloss:0.29405\n",
      "[171]\ttrain-mlogloss:0.17093\teval-mlogloss:0.29416\n",
      "[172]\ttrain-mlogloss:0.17041\teval-mlogloss:0.29431\n",
      "[173]\ttrain-mlogloss:0.17017\teval-mlogloss:0.29439\n",
      "[174]\ttrain-mlogloss:0.16986\teval-mlogloss:0.29463\n",
      "[175]\ttrain-mlogloss:0.16958\teval-mlogloss:0.29482\n",
      "[176]\ttrain-mlogloss:0.16915\teval-mlogloss:0.29493\n",
      "[177]\ttrain-mlogloss:0.16884\teval-mlogloss:0.29506\n",
      "[178]\ttrain-mlogloss:0.16847\teval-mlogloss:0.29496\n",
      "[179]\ttrain-mlogloss:0.16803\teval-mlogloss:0.29481\n",
      "[180]\ttrain-mlogloss:0.16764\teval-mlogloss:0.29469\n",
      "[181]\ttrain-mlogloss:0.16731\teval-mlogloss:0.29467\n",
      "[182]\ttrain-mlogloss:0.16712\teval-mlogloss:0.29474\n",
      "[183]\ttrain-mlogloss:0.16685\teval-mlogloss:0.29483\n",
      "[184]\ttrain-mlogloss:0.16636\teval-mlogloss:0.29528\n",
      "[185]\ttrain-mlogloss:0.16600\teval-mlogloss:0.29543\n",
      "[186]\ttrain-mlogloss:0.16567\teval-mlogloss:0.29544\n",
      "[187]\ttrain-mlogloss:0.16529\teval-mlogloss:0.29536\n",
      "[188]\ttrain-mlogloss:0.16503\teval-mlogloss:0.29544\n",
      "[189]\ttrain-mlogloss:0.16477\teval-mlogloss:0.29560\n",
      "[190]\ttrain-mlogloss:0.16437\teval-mlogloss:0.29562\n",
      "[191]\ttrain-mlogloss:0.16396\teval-mlogloss:0.29552\n",
      "[192]\ttrain-mlogloss:0.16371\teval-mlogloss:0.29547\n",
      "[193]\ttrain-mlogloss:0.16348\teval-mlogloss:0.29521\n",
      "[194]\ttrain-mlogloss:0.16323\teval-mlogloss:0.29503\n",
      "[195]\ttrain-mlogloss:0.16298\teval-mlogloss:0.29538\n",
      "[196]\ttrain-mlogloss:0.16261\teval-mlogloss:0.29542\n",
      "[197]\ttrain-mlogloss:0.16243\teval-mlogloss:0.29536\n",
      "[198]\ttrain-mlogloss:0.16228\teval-mlogloss:0.29546\n",
      "[199]\ttrain-mlogloss:0.16207\teval-mlogloss:0.29569\n",
      "[200]\ttrain-mlogloss:0.16170\teval-mlogloss:0.29567\n",
      "[201]\ttrain-mlogloss:0.16137\teval-mlogloss:0.29582\n",
      "[202]\ttrain-mlogloss:0.16117\teval-mlogloss:0.29584\n",
      "[203]\ttrain-mlogloss:0.16082\teval-mlogloss:0.29578\n",
      "[204]\ttrain-mlogloss:0.16051\teval-mlogloss:0.29604\n",
      "[205]\ttrain-mlogloss:0.16018\teval-mlogloss:0.29618\n",
      "[206]\ttrain-mlogloss:0.15996\teval-mlogloss:0.29624\n",
      "[207]\ttrain-mlogloss:0.15977\teval-mlogloss:0.29613\n",
      "[208]\ttrain-mlogloss:0.15952\teval-mlogloss:0.29604\n",
      "xgb now score is: [2.3884487102925775]\n",
      "[0]\ttrain-mlogloss:0.67139\teval-mlogloss:0.67102\n",
      "[1]\ttrain-mlogloss:0.65111\teval-mlogloss:0.65005\n",
      "[2]\ttrain-mlogloss:0.63188\teval-mlogloss:0.63041\n",
      "[3]\ttrain-mlogloss:0.61372\teval-mlogloss:0.61212\n",
      "[4]\ttrain-mlogloss:0.59630\teval-mlogloss:0.59435\n",
      "[5]\ttrain-mlogloss:0.58010\teval-mlogloss:0.57771\n",
      "[6]\ttrain-mlogloss:0.56475\teval-mlogloss:0.56226\n",
      "[7]\ttrain-mlogloss:0.55010\teval-mlogloss:0.54728\n",
      "[8]\ttrain-mlogloss:0.53611\teval-mlogloss:0.53295\n",
      "[9]\ttrain-mlogloss:0.52280\teval-mlogloss:0.51917\n",
      "[10]\ttrain-mlogloss:0.51024\teval-mlogloss:0.50636\n",
      "[11]\ttrain-mlogloss:0.49831\teval-mlogloss:0.49419\n",
      "[12]\ttrain-mlogloss:0.48691\teval-mlogloss:0.48255\n",
      "[13]\ttrain-mlogloss:0.47615\teval-mlogloss:0.47158\n",
      "[14]\ttrain-mlogloss:0.46558\teval-mlogloss:0.46069\n",
      "[15]\ttrain-mlogloss:0.45574\teval-mlogloss:0.45076\n",
      "[16]\ttrain-mlogloss:0.44633\teval-mlogloss:0.44103\n",
      "[17]\ttrain-mlogloss:0.43730\teval-mlogloss:0.43189\n",
      "[18]\ttrain-mlogloss:0.42868\teval-mlogloss:0.42308\n",
      "[19]\ttrain-mlogloss:0.42045\teval-mlogloss:0.41484\n",
      "[20]\ttrain-mlogloss:0.41264\teval-mlogloss:0.40681\n",
      "[21]\ttrain-mlogloss:0.40497\teval-mlogloss:0.39882\n",
      "[22]\ttrain-mlogloss:0.39787\teval-mlogloss:0.39140\n",
      "[23]\ttrain-mlogloss:0.39097\teval-mlogloss:0.38418\n",
      "[24]\ttrain-mlogloss:0.38418\teval-mlogloss:0.37729\n",
      "[25]\ttrain-mlogloss:0.37801\teval-mlogloss:0.37099\n",
      "[26]\ttrain-mlogloss:0.37200\teval-mlogloss:0.36490\n",
      "[27]\ttrain-mlogloss:0.36625\teval-mlogloss:0.35894\n",
      "[28]\ttrain-mlogloss:0.36090\teval-mlogloss:0.35356\n",
      "[29]\ttrain-mlogloss:0.35565\teval-mlogloss:0.34820\n",
      "[30]\ttrain-mlogloss:0.35058\teval-mlogloss:0.34297\n",
      "[31]\ttrain-mlogloss:0.34577\teval-mlogloss:0.33802\n",
      "[32]\ttrain-mlogloss:0.34105\teval-mlogloss:0.33321\n",
      "[33]\ttrain-mlogloss:0.33653\teval-mlogloss:0.32877\n",
      "[34]\ttrain-mlogloss:0.33219\teval-mlogloss:0.32447\n",
      "[35]\ttrain-mlogloss:0.32816\teval-mlogloss:0.32027\n",
      "[36]\ttrain-mlogloss:0.32405\teval-mlogloss:0.31609\n",
      "[37]\ttrain-mlogloss:0.32020\teval-mlogloss:0.31219\n",
      "[38]\ttrain-mlogloss:0.31652\teval-mlogloss:0.30853\n",
      "[39]\ttrain-mlogloss:0.31293\teval-mlogloss:0.30510\n",
      "[40]\ttrain-mlogloss:0.30944\teval-mlogloss:0.30170\n",
      "[41]\ttrain-mlogloss:0.30600\teval-mlogloss:0.29817\n",
      "[42]\ttrain-mlogloss:0.30294\teval-mlogloss:0.29520\n",
      "[43]\ttrain-mlogloss:0.29993\teval-mlogloss:0.29215\n",
      "[44]\ttrain-mlogloss:0.29711\teval-mlogloss:0.28921\n",
      "[45]\ttrain-mlogloss:0.29422\teval-mlogloss:0.28647\n",
      "[46]\ttrain-mlogloss:0.29164\teval-mlogloss:0.28398\n",
      "[47]\ttrain-mlogloss:0.28907\teval-mlogloss:0.28128\n",
      "[48]\ttrain-mlogloss:0.28654\teval-mlogloss:0.27874\n",
      "[49]\ttrain-mlogloss:0.28404\teval-mlogloss:0.27624\n",
      "[50]\ttrain-mlogloss:0.28162\teval-mlogloss:0.27405\n",
      "[51]\ttrain-mlogloss:0.27925\teval-mlogloss:0.27172\n",
      "[52]\ttrain-mlogloss:0.27720\teval-mlogloss:0.26962\n",
      "[53]\ttrain-mlogloss:0.27488\teval-mlogloss:0.26736\n",
      "[54]\ttrain-mlogloss:0.27271\teval-mlogloss:0.26532\n",
      "[55]\ttrain-mlogloss:0.27072\teval-mlogloss:0.26341\n",
      "[56]\ttrain-mlogloss:0.26882\teval-mlogloss:0.26186\n",
      "[57]\ttrain-mlogloss:0.26715\teval-mlogloss:0.26025\n",
      "[58]\ttrain-mlogloss:0.26522\teval-mlogloss:0.25862\n",
      "[59]\ttrain-mlogloss:0.26349\teval-mlogloss:0.25706\n",
      "[60]\ttrain-mlogloss:0.26168\teval-mlogloss:0.25541\n",
      "[61]\ttrain-mlogloss:0.26019\teval-mlogloss:0.25396\n",
      "[62]\ttrain-mlogloss:0.25877\teval-mlogloss:0.25266\n",
      "[63]\ttrain-mlogloss:0.25732\teval-mlogloss:0.25113\n",
      "[64]\ttrain-mlogloss:0.25571\teval-mlogloss:0.24975\n",
      "[65]\ttrain-mlogloss:0.25428\teval-mlogloss:0.24856\n",
      "[66]\ttrain-mlogloss:0.25303\teval-mlogloss:0.24721\n",
      "[67]\ttrain-mlogloss:0.25174\teval-mlogloss:0.24598\n",
      "[68]\ttrain-mlogloss:0.25048\teval-mlogloss:0.24485\n",
      "[69]\ttrain-mlogloss:0.24926\teval-mlogloss:0.24384\n",
      "[70]\ttrain-mlogloss:0.24807\teval-mlogloss:0.24274\n",
      "[71]\ttrain-mlogloss:0.24690\teval-mlogloss:0.24186\n",
      "[72]\ttrain-mlogloss:0.24564\teval-mlogloss:0.24120\n",
      "[73]\ttrain-mlogloss:0.24460\teval-mlogloss:0.24027\n",
      "[74]\ttrain-mlogloss:0.24355\teval-mlogloss:0.23959\n",
      "[75]\ttrain-mlogloss:0.24257\teval-mlogloss:0.23869\n",
      "[76]\ttrain-mlogloss:0.24146\teval-mlogloss:0.23790\n",
      "[77]\ttrain-mlogloss:0.24019\teval-mlogloss:0.23704\n",
      "[78]\ttrain-mlogloss:0.23929\teval-mlogloss:0.23617\n",
      "[79]\ttrain-mlogloss:0.23840\teval-mlogloss:0.23548\n",
      "[80]\ttrain-mlogloss:0.23735\teval-mlogloss:0.23480\n",
      "[81]\ttrain-mlogloss:0.23651\teval-mlogloss:0.23412\n",
      "[82]\ttrain-mlogloss:0.23552\teval-mlogloss:0.23354\n",
      "[83]\ttrain-mlogloss:0.23447\teval-mlogloss:0.23284\n",
      "[84]\ttrain-mlogloss:0.23367\teval-mlogloss:0.23226\n",
      "[85]\ttrain-mlogloss:0.23260\teval-mlogloss:0.23169\n",
      "[86]\ttrain-mlogloss:0.23180\teval-mlogloss:0.23118\n",
      "[87]\ttrain-mlogloss:0.23096\teval-mlogloss:0.23058\n",
      "[88]\ttrain-mlogloss:0.23031\teval-mlogloss:0.23002\n",
      "[89]\ttrain-mlogloss:0.22941\teval-mlogloss:0.22960\n",
      "[90]\ttrain-mlogloss:0.22870\teval-mlogloss:0.22908\n",
      "[91]\ttrain-mlogloss:0.22789\teval-mlogloss:0.22860\n",
      "[92]\ttrain-mlogloss:0.22695\teval-mlogloss:0.22833\n",
      "[93]\ttrain-mlogloss:0.22623\teval-mlogloss:0.22795\n",
      "[94]\ttrain-mlogloss:0.22559\teval-mlogloss:0.22763\n",
      "[95]\ttrain-mlogloss:0.22480\teval-mlogloss:0.22722\n",
      "[96]\ttrain-mlogloss:0.22418\teval-mlogloss:0.22690\n",
      "[97]\ttrain-mlogloss:0.22355\teval-mlogloss:0.22650\n",
      "[98]\ttrain-mlogloss:0.22287\teval-mlogloss:0.22629\n",
      "[99]\ttrain-mlogloss:0.22216\teval-mlogloss:0.22612\n",
      "[100]\ttrain-mlogloss:0.22157\teval-mlogloss:0.22584\n",
      "[101]\ttrain-mlogloss:0.22117\teval-mlogloss:0.22555\n",
      "[102]\ttrain-mlogloss:0.22037\teval-mlogloss:0.22547\n",
      "[103]\ttrain-mlogloss:0.21969\teval-mlogloss:0.22526\n",
      "[104]\ttrain-mlogloss:0.21887\teval-mlogloss:0.22519\n",
      "[105]\ttrain-mlogloss:0.21821\teval-mlogloss:0.22499\n",
      "[106]\ttrain-mlogloss:0.21735\teval-mlogloss:0.22487\n",
      "[107]\ttrain-mlogloss:0.21669\teval-mlogloss:0.22452\n",
      "[108]\ttrain-mlogloss:0.21595\teval-mlogloss:0.22438\n",
      "[109]\ttrain-mlogloss:0.21546\teval-mlogloss:0.22422\n",
      "[110]\ttrain-mlogloss:0.21473\teval-mlogloss:0.22391\n",
      "[111]\ttrain-mlogloss:0.21420\teval-mlogloss:0.22369\n",
      "[112]\ttrain-mlogloss:0.21361\teval-mlogloss:0.22372\n",
      "[113]\ttrain-mlogloss:0.21298\teval-mlogloss:0.22381\n",
      "[114]\ttrain-mlogloss:0.21237\teval-mlogloss:0.22385\n",
      "[115]\ttrain-mlogloss:0.21186\teval-mlogloss:0.22372\n",
      "[116]\ttrain-mlogloss:0.21120\teval-mlogloss:0.22360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117]\ttrain-mlogloss:0.21054\teval-mlogloss:0.22343\n",
      "[118]\ttrain-mlogloss:0.20994\teval-mlogloss:0.22339\n",
      "[119]\ttrain-mlogloss:0.20938\teval-mlogloss:0.22336\n",
      "[120]\ttrain-mlogloss:0.20892\teval-mlogloss:0.22337\n",
      "[121]\ttrain-mlogloss:0.20838\teval-mlogloss:0.22324\n",
      "[122]\ttrain-mlogloss:0.20769\teval-mlogloss:0.22332\n",
      "[123]\ttrain-mlogloss:0.20725\teval-mlogloss:0.22326\n",
      "[124]\ttrain-mlogloss:0.20673\teval-mlogloss:0.22310\n",
      "[125]\ttrain-mlogloss:0.20611\teval-mlogloss:0.22291\n",
      "[126]\ttrain-mlogloss:0.20553\teval-mlogloss:0.22260\n",
      "[127]\ttrain-mlogloss:0.20488\teval-mlogloss:0.22256\n",
      "[128]\ttrain-mlogloss:0.20429\teval-mlogloss:0.22226\n",
      "[129]\ttrain-mlogloss:0.20364\teval-mlogloss:0.22214\n",
      "[130]\ttrain-mlogloss:0.20312\teval-mlogloss:0.22213\n",
      "[131]\ttrain-mlogloss:0.20262\teval-mlogloss:0.22203\n",
      "[132]\ttrain-mlogloss:0.20223\teval-mlogloss:0.22184\n",
      "[133]\ttrain-mlogloss:0.20181\teval-mlogloss:0.22179\n",
      "[134]\ttrain-mlogloss:0.20131\teval-mlogloss:0.22199\n",
      "[135]\ttrain-mlogloss:0.20087\teval-mlogloss:0.22200\n",
      "[136]\ttrain-mlogloss:0.20043\teval-mlogloss:0.22218\n",
      "[137]\ttrain-mlogloss:0.20001\teval-mlogloss:0.22240\n",
      "[138]\ttrain-mlogloss:0.19965\teval-mlogloss:0.22256\n",
      "[139]\ttrain-mlogloss:0.19906\teval-mlogloss:0.22291\n",
      "[140]\ttrain-mlogloss:0.19848\teval-mlogloss:0.22289\n",
      "[141]\ttrain-mlogloss:0.19810\teval-mlogloss:0.22285\n",
      "[142]\ttrain-mlogloss:0.19761\teval-mlogloss:0.22286\n",
      "[143]\ttrain-mlogloss:0.19716\teval-mlogloss:0.22268\n",
      "[144]\ttrain-mlogloss:0.19671\teval-mlogloss:0.22256\n",
      "[145]\ttrain-mlogloss:0.19631\teval-mlogloss:0.22265\n",
      "[146]\ttrain-mlogloss:0.19575\teval-mlogloss:0.22292\n",
      "[147]\ttrain-mlogloss:0.19538\teval-mlogloss:0.22284\n",
      "[148]\ttrain-mlogloss:0.19495\teval-mlogloss:0.22285\n",
      "[149]\ttrain-mlogloss:0.19448\teval-mlogloss:0.22313\n",
      "[150]\ttrain-mlogloss:0.19404\teval-mlogloss:0.22312\n",
      "[151]\ttrain-mlogloss:0.19359\teval-mlogloss:0.22321\n",
      "[152]\ttrain-mlogloss:0.19311\teval-mlogloss:0.22307\n",
      "[153]\ttrain-mlogloss:0.19286\teval-mlogloss:0.22308\n",
      "[154]\ttrain-mlogloss:0.19262\teval-mlogloss:0.22310\n",
      "[155]\ttrain-mlogloss:0.19220\teval-mlogloss:0.22319\n",
      "[156]\ttrain-mlogloss:0.19173\teval-mlogloss:0.22322\n",
      "[157]\ttrain-mlogloss:0.19139\teval-mlogloss:0.22339\n",
      "[158]\ttrain-mlogloss:0.19085\teval-mlogloss:0.22333\n",
      "[159]\ttrain-mlogloss:0.19039\teval-mlogloss:0.22336\n",
      "[160]\ttrain-mlogloss:0.19006\teval-mlogloss:0.22318\n",
      "[161]\ttrain-mlogloss:0.18965\teval-mlogloss:0.22316\n",
      "[162]\ttrain-mlogloss:0.18930\teval-mlogloss:0.22322\n",
      "[163]\ttrain-mlogloss:0.18882\teval-mlogloss:0.22320\n",
      "[164]\ttrain-mlogloss:0.18833\teval-mlogloss:0.22322\n",
      "[165]\ttrain-mlogloss:0.18787\teval-mlogloss:0.22296\n",
      "[166]\ttrain-mlogloss:0.18759\teval-mlogloss:0.22320\n",
      "[167]\ttrain-mlogloss:0.18712\teval-mlogloss:0.22315\n",
      "[168]\ttrain-mlogloss:0.18685\teval-mlogloss:0.22306\n",
      "[169]\ttrain-mlogloss:0.18648\teval-mlogloss:0.22333\n",
      "[170]\ttrain-mlogloss:0.18617\teval-mlogloss:0.22314\n",
      "[171]\ttrain-mlogloss:0.18574\teval-mlogloss:0.22325\n",
      "[172]\ttrain-mlogloss:0.18526\teval-mlogloss:0.22313\n",
      "[173]\ttrain-mlogloss:0.18488\teval-mlogloss:0.22324\n",
      "[174]\ttrain-mlogloss:0.18442\teval-mlogloss:0.22351\n",
      "[175]\ttrain-mlogloss:0.18399\teval-mlogloss:0.22335\n",
      "[176]\ttrain-mlogloss:0.18352\teval-mlogloss:0.22308\n",
      "[177]\ttrain-mlogloss:0.18308\teval-mlogloss:0.22314\n",
      "[178]\ttrain-mlogloss:0.18271\teval-mlogloss:0.22292\n",
      "[179]\ttrain-mlogloss:0.18235\teval-mlogloss:0.22329\n",
      "[180]\ttrain-mlogloss:0.18197\teval-mlogloss:0.22343\n",
      "[181]\ttrain-mlogloss:0.18158\teval-mlogloss:0.22324\n",
      "[182]\ttrain-mlogloss:0.18130\teval-mlogloss:0.22308\n",
      "[183]\ttrain-mlogloss:0.18096\teval-mlogloss:0.22312\n",
      "[184]\ttrain-mlogloss:0.18058\teval-mlogloss:0.22306\n",
      "[185]\ttrain-mlogloss:0.18008\teval-mlogloss:0.22298\n",
      "[186]\ttrain-mlogloss:0.17973\teval-mlogloss:0.22281\n",
      "[187]\ttrain-mlogloss:0.17938\teval-mlogloss:0.22283\n",
      "[188]\ttrain-mlogloss:0.17902\teval-mlogloss:0.22307\n",
      "[189]\ttrain-mlogloss:0.17876\teval-mlogloss:0.22302\n",
      "[190]\ttrain-mlogloss:0.17842\teval-mlogloss:0.22291\n",
      "[191]\ttrain-mlogloss:0.17798\teval-mlogloss:0.22302\n",
      "[192]\ttrain-mlogloss:0.17770\teval-mlogloss:0.22304\n",
      "[193]\ttrain-mlogloss:0.17741\teval-mlogloss:0.22301\n",
      "[194]\ttrain-mlogloss:0.17726\teval-mlogloss:0.22279\n",
      "[195]\ttrain-mlogloss:0.17694\teval-mlogloss:0.22309\n",
      "[196]\ttrain-mlogloss:0.17660\teval-mlogloss:0.22321\n",
      "[197]\ttrain-mlogloss:0.17630\teval-mlogloss:0.22328\n",
      "[198]\ttrain-mlogloss:0.17606\teval-mlogloss:0.22327\n",
      "[199]\ttrain-mlogloss:0.17575\teval-mlogloss:0.22313\n",
      "[200]\ttrain-mlogloss:0.17558\teval-mlogloss:0.22326\n",
      "[201]\ttrain-mlogloss:0.17532\teval-mlogloss:0.22336\n",
      "[202]\ttrain-mlogloss:0.17501\teval-mlogloss:0.22320\n",
      "[203]\ttrain-mlogloss:0.17470\teval-mlogloss:0.22319\n",
      "[204]\ttrain-mlogloss:0.17443\teval-mlogloss:0.22337\n",
      "[205]\ttrain-mlogloss:0.17417\teval-mlogloss:0.22354\n",
      "[206]\ttrain-mlogloss:0.17384\teval-mlogloss:0.22355\n",
      "[207]\ttrain-mlogloss:0.17359\teval-mlogloss:0.22357\n",
      "[208]\ttrain-mlogloss:0.17329\teval-mlogloss:0.22356\n",
      "[209]\ttrain-mlogloss:0.17295\teval-mlogloss:0.22346\n",
      "[210]\ttrain-mlogloss:0.17261\teval-mlogloss:0.22356\n",
      "[211]\ttrain-mlogloss:0.17238\teval-mlogloss:0.22357\n",
      "[212]\ttrain-mlogloss:0.17215\teval-mlogloss:0.22370\n",
      "[213]\ttrain-mlogloss:0.17178\teval-mlogloss:0.22374\n",
      "[214]\ttrain-mlogloss:0.17160\teval-mlogloss:0.22359\n",
      "[215]\ttrain-mlogloss:0.17133\teval-mlogloss:0.22360\n",
      "[216]\ttrain-mlogloss:0.17101\teval-mlogloss:0.22361\n",
      "[217]\ttrain-mlogloss:0.17077\teval-mlogloss:0.22359\n",
      "[218]\ttrain-mlogloss:0.17044\teval-mlogloss:0.22362\n",
      "[219]\ttrain-mlogloss:0.17018\teval-mlogloss:0.22360\n",
      "[220]\ttrain-mlogloss:0.16996\teval-mlogloss:0.22365\n",
      "[221]\ttrain-mlogloss:0.16958\teval-mlogloss:0.22359\n",
      "[222]\ttrain-mlogloss:0.16937\teval-mlogloss:0.22353\n",
      "[223]\ttrain-mlogloss:0.16910\teval-mlogloss:0.22379\n",
      "[224]\ttrain-mlogloss:0.16879\teval-mlogloss:0.22365\n",
      "[225]\ttrain-mlogloss:0.16849\teval-mlogloss:0.22360\n",
      "[226]\ttrain-mlogloss:0.16818\teval-mlogloss:0.22352\n",
      "[227]\ttrain-mlogloss:0.16795\teval-mlogloss:0.22360\n",
      "[228]\ttrain-mlogloss:0.16768\teval-mlogloss:0.22372\n",
      "[229]\ttrain-mlogloss:0.16731\teval-mlogloss:0.22373\n",
      "[230]\ttrain-mlogloss:0.16713\teval-mlogloss:0.22368\n",
      "[231]\ttrain-mlogloss:0.16689\teval-mlogloss:0.22367\n",
      "[232]\ttrain-mlogloss:0.16652\teval-mlogloss:0.22368\n",
      "xgb now score is: [2.3884487102925775, 2.5234747439064087]\n",
      "[0]\ttrain-mlogloss:0.67115\teval-mlogloss:0.67106\n",
      "[1]\ttrain-mlogloss:0.65073\teval-mlogloss:0.65023\n",
      "[2]\ttrain-mlogloss:0.63130\teval-mlogloss:0.63045\n",
      "[3]\ttrain-mlogloss:0.61306\teval-mlogloss:0.61196\n",
      "[4]\ttrain-mlogloss:0.59568\teval-mlogloss:0.59445\n",
      "[5]\ttrain-mlogloss:0.57949\teval-mlogloss:0.57802\n",
      "[6]\ttrain-mlogloss:0.56378\teval-mlogloss:0.56207\n",
      "[7]\ttrain-mlogloss:0.54902\teval-mlogloss:0.54733\n",
      "[8]\ttrain-mlogloss:0.53514\teval-mlogloss:0.53317\n",
      "[9]\ttrain-mlogloss:0.52199\teval-mlogloss:0.51982\n",
      "[10]\ttrain-mlogloss:0.50936\teval-mlogloss:0.50694\n",
      "[11]\ttrain-mlogloss:0.49728\teval-mlogloss:0.49447\n",
      "[12]\ttrain-mlogloss:0.48574\teval-mlogloss:0.48275\n",
      "[13]\ttrain-mlogloss:0.47467\teval-mlogloss:0.47152\n",
      "[14]\ttrain-mlogloss:0.46420\teval-mlogloss:0.46093\n",
      "[15]\ttrain-mlogloss:0.45431\teval-mlogloss:0.45103\n",
      "[16]\ttrain-mlogloss:0.44479\teval-mlogloss:0.44134\n",
      "[17]\ttrain-mlogloss:0.43574\teval-mlogloss:0.43219\n",
      "[18]\ttrain-mlogloss:0.42712\teval-mlogloss:0.42356\n",
      "[19]\ttrain-mlogloss:0.41891\teval-mlogloss:0.41525\n",
      "[20]\ttrain-mlogloss:0.41098\teval-mlogloss:0.40734\n",
      "[21]\ttrain-mlogloss:0.40349\teval-mlogloss:0.39976\n",
      "[22]\ttrain-mlogloss:0.39622\teval-mlogloss:0.39235\n",
      "[23]\ttrain-mlogloss:0.38913\teval-mlogloss:0.38523\n",
      "[24]\ttrain-mlogloss:0.38250\teval-mlogloss:0.37852\n",
      "[25]\ttrain-mlogloss:0.37609\teval-mlogloss:0.37215\n",
      "[26]\ttrain-mlogloss:0.36996\teval-mlogloss:0.36602\n",
      "[27]\ttrain-mlogloss:0.36410\teval-mlogloss:0.36033\n",
      "[28]\ttrain-mlogloss:0.35842\teval-mlogloss:0.35463\n",
      "[29]\ttrain-mlogloss:0.35302\teval-mlogloss:0.34918\n",
      "[30]\ttrain-mlogloss:0.34785\teval-mlogloss:0.34399\n",
      "[31]\ttrain-mlogloss:0.34286\teval-mlogloss:0.33903\n",
      "[32]\ttrain-mlogloss:0.33815\teval-mlogloss:0.33451\n",
      "[33]\ttrain-mlogloss:0.33360\teval-mlogloss:0.32996\n",
      "[34]\ttrain-mlogloss:0.32921\teval-mlogloss:0.32560\n",
      "[35]\ttrain-mlogloss:0.32516\teval-mlogloss:0.32135\n",
      "[36]\ttrain-mlogloss:0.32094\teval-mlogloss:0.31739\n",
      "[37]\ttrain-mlogloss:0.31712\teval-mlogloss:0.31354\n",
      "[38]\ttrain-mlogloss:0.31343\teval-mlogloss:0.31008\n",
      "[39]\ttrain-mlogloss:0.30988\teval-mlogloss:0.30651\n",
      "[40]\ttrain-mlogloss:0.30630\teval-mlogloss:0.30306\n",
      "[41]\ttrain-mlogloss:0.30290\teval-mlogloss:0.29967\n",
      "[42]\ttrain-mlogloss:0.29981\teval-mlogloss:0.29656\n",
      "[43]\ttrain-mlogloss:0.29665\teval-mlogloss:0.29339\n",
      "[44]\ttrain-mlogloss:0.29373\teval-mlogloss:0.29073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45]\ttrain-mlogloss:0.29074\teval-mlogloss:0.28797\n",
      "[46]\ttrain-mlogloss:0.28787\teval-mlogloss:0.28547\n",
      "[47]\ttrain-mlogloss:0.28536\teval-mlogloss:0.28296\n",
      "[48]\ttrain-mlogloss:0.28277\teval-mlogloss:0.28077\n",
      "[49]\ttrain-mlogloss:0.28023\teval-mlogloss:0.27841\n",
      "[50]\ttrain-mlogloss:0.27788\teval-mlogloss:0.27596\n",
      "[51]\ttrain-mlogloss:0.27558\teval-mlogloss:0.27389\n",
      "[52]\ttrain-mlogloss:0.27335\teval-mlogloss:0.27197\n",
      "[53]\ttrain-mlogloss:0.27097\teval-mlogloss:0.26989\n",
      "[54]\ttrain-mlogloss:0.26882\teval-mlogloss:0.26834\n",
      "[55]\ttrain-mlogloss:0.26672\teval-mlogloss:0.26648\n",
      "[56]\ttrain-mlogloss:0.26460\teval-mlogloss:0.26494\n",
      "[57]\ttrain-mlogloss:0.26278\teval-mlogloss:0.26341\n",
      "[58]\ttrain-mlogloss:0.26104\teval-mlogloss:0.26195\n",
      "[59]\ttrain-mlogloss:0.25927\teval-mlogloss:0.26063\n",
      "[60]\ttrain-mlogloss:0.25736\teval-mlogloss:0.25943\n",
      "[61]\ttrain-mlogloss:0.25577\teval-mlogloss:0.25812\n",
      "[62]\ttrain-mlogloss:0.25423\teval-mlogloss:0.25694\n",
      "[63]\ttrain-mlogloss:0.25267\teval-mlogloss:0.25601\n",
      "[64]\ttrain-mlogloss:0.25100\teval-mlogloss:0.25488\n",
      "[65]\ttrain-mlogloss:0.24943\teval-mlogloss:0.25383\n",
      "[66]\ttrain-mlogloss:0.24794\teval-mlogloss:0.25284\n",
      "[67]\ttrain-mlogloss:0.24662\teval-mlogloss:0.25160\n",
      "[68]\ttrain-mlogloss:0.24522\teval-mlogloss:0.25079\n",
      "[69]\ttrain-mlogloss:0.24416\teval-mlogloss:0.24973\n",
      "[70]\ttrain-mlogloss:0.24270\teval-mlogloss:0.24910\n",
      "[71]\ttrain-mlogloss:0.24144\teval-mlogloss:0.24848\n",
      "[72]\ttrain-mlogloss:0.24025\teval-mlogloss:0.24777\n",
      "[73]\ttrain-mlogloss:0.23900\teval-mlogloss:0.24695\n",
      "[74]\ttrain-mlogloss:0.23769\teval-mlogloss:0.24626\n",
      "[75]\ttrain-mlogloss:0.23654\teval-mlogloss:0.24569\n",
      "[76]\ttrain-mlogloss:0.23542\teval-mlogloss:0.24502\n",
      "[77]\ttrain-mlogloss:0.23439\teval-mlogloss:0.24445\n",
      "[78]\ttrain-mlogloss:0.23330\teval-mlogloss:0.24385\n",
      "[79]\ttrain-mlogloss:0.23215\teval-mlogloss:0.24322\n",
      "[80]\ttrain-mlogloss:0.23122\teval-mlogloss:0.24263\n",
      "[81]\ttrain-mlogloss:0.23035\teval-mlogloss:0.24209\n",
      "[82]\ttrain-mlogloss:0.22904\teval-mlogloss:0.24184\n",
      "[83]\ttrain-mlogloss:0.22797\teval-mlogloss:0.24138\n",
      "[84]\ttrain-mlogloss:0.22696\teval-mlogloss:0.24108\n",
      "[85]\ttrain-mlogloss:0.22596\teval-mlogloss:0.24063\n",
      "[86]\ttrain-mlogloss:0.22490\teval-mlogloss:0.24055\n",
      "[87]\ttrain-mlogloss:0.22411\teval-mlogloss:0.24017\n",
      "[88]\ttrain-mlogloss:0.22332\teval-mlogloss:0.23976\n",
      "[89]\ttrain-mlogloss:0.22259\teval-mlogloss:0.23933\n",
      "[90]\ttrain-mlogloss:0.22174\teval-mlogloss:0.23920\n",
      "[91]\ttrain-mlogloss:0.22090\teval-mlogloss:0.23894\n",
      "[92]\ttrain-mlogloss:0.22011\teval-mlogloss:0.23876\n",
      "[93]\ttrain-mlogloss:0.21941\teval-mlogloss:0.23850\n",
      "[94]\ttrain-mlogloss:0.21852\teval-mlogloss:0.23813\n",
      "[95]\ttrain-mlogloss:0.21801\teval-mlogloss:0.23783\n",
      "[96]\ttrain-mlogloss:0.21720\teval-mlogloss:0.23786\n",
      "[97]\ttrain-mlogloss:0.21640\teval-mlogloss:0.23744\n",
      "[98]\ttrain-mlogloss:0.21581\teval-mlogloss:0.23735\n",
      "[99]\ttrain-mlogloss:0.21497\teval-mlogloss:0.23729\n",
      "[100]\ttrain-mlogloss:0.21415\teval-mlogloss:0.23730\n",
      "[101]\ttrain-mlogloss:0.21358\teval-mlogloss:0.23727\n",
      "[102]\ttrain-mlogloss:0.21300\teval-mlogloss:0.23700\n",
      "[103]\ttrain-mlogloss:0.21231\teval-mlogloss:0.23691\n",
      "[104]\ttrain-mlogloss:0.21161\teval-mlogloss:0.23686\n",
      "[105]\ttrain-mlogloss:0.21065\teval-mlogloss:0.23664\n",
      "[106]\ttrain-mlogloss:0.21013\teval-mlogloss:0.23655\n",
      "[107]\ttrain-mlogloss:0.20952\teval-mlogloss:0.23641\n",
      "[108]\ttrain-mlogloss:0.20887\teval-mlogloss:0.23642\n",
      "[109]\ttrain-mlogloss:0.20837\teval-mlogloss:0.23634\n",
      "[110]\ttrain-mlogloss:0.20765\teval-mlogloss:0.23649\n",
      "[111]\ttrain-mlogloss:0.20716\teval-mlogloss:0.23664\n",
      "[112]\ttrain-mlogloss:0.20665\teval-mlogloss:0.23663\n",
      "[113]\ttrain-mlogloss:0.20608\teval-mlogloss:0.23658\n",
      "[114]\ttrain-mlogloss:0.20549\teval-mlogloss:0.23654\n",
      "[115]\ttrain-mlogloss:0.20479\teval-mlogloss:0.23647\n",
      "[116]\ttrain-mlogloss:0.20418\teval-mlogloss:0.23643\n",
      "[117]\ttrain-mlogloss:0.20348\teval-mlogloss:0.23659\n",
      "[118]\ttrain-mlogloss:0.20297\teval-mlogloss:0.23646\n",
      "[119]\ttrain-mlogloss:0.20231\teval-mlogloss:0.23661\n",
      "[120]\ttrain-mlogloss:0.20170\teval-mlogloss:0.23654\n",
      "[121]\ttrain-mlogloss:0.20114\teval-mlogloss:0.23634\n",
      "[122]\ttrain-mlogloss:0.20035\teval-mlogloss:0.23623\n",
      "[123]\ttrain-mlogloss:0.19978\teval-mlogloss:0.23637\n",
      "[124]\ttrain-mlogloss:0.19927\teval-mlogloss:0.23631\n",
      "[125]\ttrain-mlogloss:0.19877\teval-mlogloss:0.23620\n",
      "[126]\ttrain-mlogloss:0.19809\teval-mlogloss:0.23611\n",
      "[127]\ttrain-mlogloss:0.19771\teval-mlogloss:0.23611\n",
      "[128]\ttrain-mlogloss:0.19712\teval-mlogloss:0.23612\n",
      "[129]\ttrain-mlogloss:0.19642\teval-mlogloss:0.23624\n",
      "[130]\ttrain-mlogloss:0.19588\teval-mlogloss:0.23630\n",
      "[131]\ttrain-mlogloss:0.19534\teval-mlogloss:0.23652\n",
      "[132]\ttrain-mlogloss:0.19466\teval-mlogloss:0.23654\n",
      "[133]\ttrain-mlogloss:0.19413\teval-mlogloss:0.23656\n",
      "[134]\ttrain-mlogloss:0.19363\teval-mlogloss:0.23666\n",
      "[135]\ttrain-mlogloss:0.19319\teval-mlogloss:0.23670\n",
      "[136]\ttrain-mlogloss:0.19268\teval-mlogloss:0.23674\n",
      "[137]\ttrain-mlogloss:0.19226\teval-mlogloss:0.23669\n",
      "[138]\ttrain-mlogloss:0.19171\teval-mlogloss:0.23673\n",
      "[139]\ttrain-mlogloss:0.19126\teval-mlogloss:0.23672\n",
      "[140]\ttrain-mlogloss:0.19074\teval-mlogloss:0.23674\n",
      "[141]\ttrain-mlogloss:0.19040\teval-mlogloss:0.23680\n",
      "[142]\ttrain-mlogloss:0.18982\teval-mlogloss:0.23695\n",
      "[143]\ttrain-mlogloss:0.18929\teval-mlogloss:0.23716\n",
      "[144]\ttrain-mlogloss:0.18885\teval-mlogloss:0.23712\n",
      "[145]\ttrain-mlogloss:0.18831\teval-mlogloss:0.23716\n",
      "[146]\ttrain-mlogloss:0.18796\teval-mlogloss:0.23726\n",
      "[147]\ttrain-mlogloss:0.18761\teval-mlogloss:0.23746\n",
      "[148]\ttrain-mlogloss:0.18723\teval-mlogloss:0.23756\n",
      "[149]\ttrain-mlogloss:0.18671\teval-mlogloss:0.23774\n",
      "[150]\ttrain-mlogloss:0.18630\teval-mlogloss:0.23776\n",
      "[151]\ttrain-mlogloss:0.18589\teval-mlogloss:0.23797\n",
      "[152]\ttrain-mlogloss:0.18527\teval-mlogloss:0.23794\n",
      "[153]\ttrain-mlogloss:0.18483\teval-mlogloss:0.23830\n",
      "[154]\ttrain-mlogloss:0.18439\teval-mlogloss:0.23848\n",
      "[155]\ttrain-mlogloss:0.18389\teval-mlogloss:0.23850\n",
      "[156]\ttrain-mlogloss:0.18351\teval-mlogloss:0.23851\n",
      "[157]\ttrain-mlogloss:0.18305\teval-mlogloss:0.23850\n",
      "[158]\ttrain-mlogloss:0.18258\teval-mlogloss:0.23851\n",
      "[159]\ttrain-mlogloss:0.18223\teval-mlogloss:0.23864\n",
      "[160]\ttrain-mlogloss:0.18185\teval-mlogloss:0.23871\n",
      "[161]\ttrain-mlogloss:0.18147\teval-mlogloss:0.23899\n",
      "[162]\ttrain-mlogloss:0.18121\teval-mlogloss:0.23911\n",
      "[163]\ttrain-mlogloss:0.18076\teval-mlogloss:0.23925\n",
      "[164]\ttrain-mlogloss:0.18046\teval-mlogloss:0.23916\n",
      "[165]\ttrain-mlogloss:0.18008\teval-mlogloss:0.23922\n",
      "[166]\ttrain-mlogloss:0.17969\teval-mlogloss:0.23927\n",
      "[167]\ttrain-mlogloss:0.17916\teval-mlogloss:0.23925\n",
      "[168]\ttrain-mlogloss:0.17889\teval-mlogloss:0.23924\n",
      "[169]\ttrain-mlogloss:0.17853\teval-mlogloss:0.23938\n",
      "[170]\ttrain-mlogloss:0.17809\teval-mlogloss:0.23942\n",
      "[171]\ttrain-mlogloss:0.17770\teval-mlogloss:0.23962\n",
      "[172]\ttrain-mlogloss:0.17739\teval-mlogloss:0.23986\n",
      "[173]\ttrain-mlogloss:0.17704\teval-mlogloss:0.23997\n",
      "[174]\ttrain-mlogloss:0.17679\teval-mlogloss:0.24020\n",
      "[175]\ttrain-mlogloss:0.17640\teval-mlogloss:0.24041\n",
      "[176]\ttrain-mlogloss:0.17614\teval-mlogloss:0.24045\n",
      "[177]\ttrain-mlogloss:0.17578\teval-mlogloss:0.24058\n",
      "[178]\ttrain-mlogloss:0.17535\teval-mlogloss:0.24062\n",
      "[179]\ttrain-mlogloss:0.17499\teval-mlogloss:0.24062\n",
      "[180]\ttrain-mlogloss:0.17465\teval-mlogloss:0.24083\n",
      "[181]\ttrain-mlogloss:0.17428\teval-mlogloss:0.24083\n",
      "[182]\ttrain-mlogloss:0.17393\teval-mlogloss:0.24095\n",
      "[183]\ttrain-mlogloss:0.17360\teval-mlogloss:0.24103\n",
      "[184]\ttrain-mlogloss:0.17331\teval-mlogloss:0.24103\n",
      "[185]\ttrain-mlogloss:0.17305\teval-mlogloss:0.24099\n",
      "[186]\ttrain-mlogloss:0.17269\teval-mlogloss:0.24111\n",
      "[187]\ttrain-mlogloss:0.17243\teval-mlogloss:0.24108\n",
      "[188]\ttrain-mlogloss:0.17205\teval-mlogloss:0.24118\n",
      "[189]\ttrain-mlogloss:0.17151\teval-mlogloss:0.24123\n",
      "[190]\ttrain-mlogloss:0.17108\teval-mlogloss:0.24130\n",
      "[191]\ttrain-mlogloss:0.17068\teval-mlogloss:0.24144\n",
      "[192]\ttrain-mlogloss:0.17056\teval-mlogloss:0.24129\n",
      "[193]\ttrain-mlogloss:0.17022\teval-mlogloss:0.24144\n",
      "[194]\ttrain-mlogloss:0.17000\teval-mlogloss:0.24155\n",
      "[195]\ttrain-mlogloss:0.16988\teval-mlogloss:0.24173\n",
      "[196]\ttrain-mlogloss:0.16955\teval-mlogloss:0.24178\n",
      "[197]\ttrain-mlogloss:0.16924\teval-mlogloss:0.24193\n",
      "[198]\ttrain-mlogloss:0.16890\teval-mlogloss:0.24202\n",
      "[199]\ttrain-mlogloss:0.16863\teval-mlogloss:0.24204\n",
      "[200]\ttrain-mlogloss:0.16832\teval-mlogloss:0.24214\n",
      "[201]\ttrain-mlogloss:0.16799\teval-mlogloss:0.24212\n",
      "[202]\ttrain-mlogloss:0.16778\teval-mlogloss:0.24215\n",
      "[203]\ttrain-mlogloss:0.16747\teval-mlogloss:0.24232\n",
      "[204]\ttrain-mlogloss:0.16720\teval-mlogloss:0.24235\n",
      "[205]\ttrain-mlogloss:0.16691\teval-mlogloss:0.24263\n",
      "[206]\ttrain-mlogloss:0.16659\teval-mlogloss:0.24269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207]\ttrain-mlogloss:0.16628\teval-mlogloss:0.24272\n",
      "[208]\ttrain-mlogloss:0.16598\teval-mlogloss:0.24293\n",
      "[209]\ttrain-mlogloss:0.16573\teval-mlogloss:0.24285\n",
      "[210]\ttrain-mlogloss:0.16548\teval-mlogloss:0.24294\n",
      "[211]\ttrain-mlogloss:0.16515\teval-mlogloss:0.24307\n",
      "[212]\ttrain-mlogloss:0.16476\teval-mlogloss:0.24310\n",
      "[213]\ttrain-mlogloss:0.16444\teval-mlogloss:0.24308\n",
      "[214]\ttrain-mlogloss:0.16416\teval-mlogloss:0.24312\n",
      "[215]\ttrain-mlogloss:0.16400\teval-mlogloss:0.24311\n",
      "[216]\ttrain-mlogloss:0.16376\teval-mlogloss:0.24316\n",
      "[217]\ttrain-mlogloss:0.16345\teval-mlogloss:0.24355\n",
      "[218]\ttrain-mlogloss:0.16321\teval-mlogloss:0.24380\n",
      "[219]\ttrain-mlogloss:0.16306\teval-mlogloss:0.24382\n",
      "[220]\ttrain-mlogloss:0.16279\teval-mlogloss:0.24389\n",
      "[221]\ttrain-mlogloss:0.16259\teval-mlogloss:0.24414\n",
      "[222]\ttrain-mlogloss:0.16226\teval-mlogloss:0.24422\n",
      "[223]\ttrain-mlogloss:0.16203\teval-mlogloss:0.24438\n",
      "[224]\ttrain-mlogloss:0.16171\teval-mlogloss:0.24458\n",
      "[225]\ttrain-mlogloss:0.16144\teval-mlogloss:0.24477\n",
      "[226]\ttrain-mlogloss:0.16121\teval-mlogloss:0.24483\n",
      "xgb now score is: [2.3884487102925775, 2.5234747439064087, 2.5294904569908976]\n",
      "[0]\ttrain-mlogloss:0.67068\teval-mlogloss:0.67223\n",
      "[1]\ttrain-mlogloss:0.64965\teval-mlogloss:0.65267\n",
      "[2]\ttrain-mlogloss:0.62978\teval-mlogloss:0.63405\n",
      "[3]\ttrain-mlogloss:0.61113\teval-mlogloss:0.61669\n",
      "[4]\ttrain-mlogloss:0.59349\teval-mlogloss:0.60025\n",
      "[5]\ttrain-mlogloss:0.57697\teval-mlogloss:0.58482\n",
      "[6]\ttrain-mlogloss:0.56110\teval-mlogloss:0.57016\n",
      "[7]\ttrain-mlogloss:0.54594\teval-mlogloss:0.55618\n",
      "[8]\ttrain-mlogloss:0.53148\teval-mlogloss:0.54266\n",
      "[9]\ttrain-mlogloss:0.51785\teval-mlogloss:0.53011\n",
      "[10]\ttrain-mlogloss:0.50496\teval-mlogloss:0.51833\n",
      "[11]\ttrain-mlogloss:0.49269\teval-mlogloss:0.50683\n",
      "[12]\ttrain-mlogloss:0.48073\teval-mlogloss:0.49604\n",
      "[13]\ttrain-mlogloss:0.46932\teval-mlogloss:0.48579\n",
      "[14]\ttrain-mlogloss:0.45867\teval-mlogloss:0.47616\n",
      "[15]\ttrain-mlogloss:0.44851\teval-mlogloss:0.46700\n",
      "[16]\ttrain-mlogloss:0.43874\teval-mlogloss:0.45832\n",
      "[17]\ttrain-mlogloss:0.42939\teval-mlogloss:0.44976\n",
      "[18]\ttrain-mlogloss:0.42038\teval-mlogloss:0.44171\n",
      "[19]\ttrain-mlogloss:0.41179\teval-mlogloss:0.43400\n",
      "[20]\ttrain-mlogloss:0.40366\teval-mlogloss:0.42664\n",
      "[21]\ttrain-mlogloss:0.39598\teval-mlogloss:0.41984\n",
      "[22]\ttrain-mlogloss:0.38855\teval-mlogloss:0.41334\n",
      "[23]\ttrain-mlogloss:0.38148\teval-mlogloss:0.40735\n",
      "[24]\ttrain-mlogloss:0.37462\teval-mlogloss:0.40157\n",
      "[25]\ttrain-mlogloss:0.36815\teval-mlogloss:0.39606\n",
      "[26]\ttrain-mlogloss:0.36193\teval-mlogloss:0.39059\n",
      "[27]\ttrain-mlogloss:0.35597\teval-mlogloss:0.38550\n",
      "[28]\ttrain-mlogloss:0.35020\teval-mlogloss:0.38075\n",
      "[29]\ttrain-mlogloss:0.34464\teval-mlogloss:0.37594\n",
      "[30]\ttrain-mlogloss:0.33945\teval-mlogloss:0.37168\n",
      "[31]\ttrain-mlogloss:0.33439\teval-mlogloss:0.36742\n",
      "[32]\ttrain-mlogloss:0.32962\teval-mlogloss:0.36355\n",
      "[33]\ttrain-mlogloss:0.32481\teval-mlogloss:0.35969\n",
      "[34]\ttrain-mlogloss:0.32027\teval-mlogloss:0.35605\n",
      "[35]\ttrain-mlogloss:0.31591\teval-mlogloss:0.35248\n",
      "[36]\ttrain-mlogloss:0.31161\teval-mlogloss:0.34939\n",
      "[37]\ttrain-mlogloss:0.30765\teval-mlogloss:0.34635\n",
      "[38]\ttrain-mlogloss:0.30381\teval-mlogloss:0.34341\n",
      "[39]\ttrain-mlogloss:0.30009\teval-mlogloss:0.34053\n",
      "[40]\ttrain-mlogloss:0.29658\teval-mlogloss:0.33794\n",
      "[41]\ttrain-mlogloss:0.29304\teval-mlogloss:0.33538\n",
      "[42]\ttrain-mlogloss:0.28971\teval-mlogloss:0.33275\n",
      "[43]\ttrain-mlogloss:0.28658\teval-mlogloss:0.33059\n",
      "[44]\ttrain-mlogloss:0.28345\teval-mlogloss:0.32823\n",
      "[45]\ttrain-mlogloss:0.28049\teval-mlogloss:0.32620\n",
      "[46]\ttrain-mlogloss:0.27774\teval-mlogloss:0.32424\n",
      "[47]\ttrain-mlogloss:0.27496\teval-mlogloss:0.32240\n",
      "[48]\ttrain-mlogloss:0.27231\teval-mlogloss:0.32057\n",
      "[49]\ttrain-mlogloss:0.26976\teval-mlogloss:0.31889\n",
      "[50]\ttrain-mlogloss:0.26738\teval-mlogloss:0.31736\n",
      "[51]\ttrain-mlogloss:0.26498\teval-mlogloss:0.31591\n",
      "[52]\ttrain-mlogloss:0.26274\teval-mlogloss:0.31435\n",
      "[53]\ttrain-mlogloss:0.26059\teval-mlogloss:0.31296\n",
      "[54]\ttrain-mlogloss:0.25849\teval-mlogloss:0.31166\n",
      "[55]\ttrain-mlogloss:0.25650\teval-mlogloss:0.31039\n",
      "[56]\ttrain-mlogloss:0.25442\teval-mlogloss:0.30908\n",
      "[57]\ttrain-mlogloss:0.25256\teval-mlogloss:0.30819\n",
      "[58]\ttrain-mlogloss:0.25061\teval-mlogloss:0.30693\n",
      "[59]\ttrain-mlogloss:0.24891\teval-mlogloss:0.30584\n",
      "[60]\ttrain-mlogloss:0.24727\teval-mlogloss:0.30488\n",
      "[61]\ttrain-mlogloss:0.24557\teval-mlogloss:0.30377\n",
      "[62]\ttrain-mlogloss:0.24405\teval-mlogloss:0.30296\n",
      "[63]\ttrain-mlogloss:0.24257\teval-mlogloss:0.30222\n",
      "[64]\ttrain-mlogloss:0.24106\teval-mlogloss:0.30141\n",
      "[65]\ttrain-mlogloss:0.23967\teval-mlogloss:0.30063\n",
      "[66]\ttrain-mlogloss:0.23818\teval-mlogloss:0.30001\n",
      "[67]\ttrain-mlogloss:0.23683\teval-mlogloss:0.29937\n",
      "[68]\ttrain-mlogloss:0.23552\teval-mlogloss:0.29863\n",
      "[69]\ttrain-mlogloss:0.23429\teval-mlogloss:0.29802\n",
      "[70]\ttrain-mlogloss:0.23320\teval-mlogloss:0.29749\n",
      "[71]\ttrain-mlogloss:0.23202\teval-mlogloss:0.29691\n",
      "[72]\ttrain-mlogloss:0.23084\teval-mlogloss:0.29656\n",
      "[73]\ttrain-mlogloss:0.22975\teval-mlogloss:0.29613\n",
      "[74]\ttrain-mlogloss:0.22868\teval-mlogloss:0.29575\n",
      "[75]\ttrain-mlogloss:0.22769\teval-mlogloss:0.29545\n",
      "[76]\ttrain-mlogloss:0.22670\teval-mlogloss:0.29478\n",
      "[77]\ttrain-mlogloss:0.22580\teval-mlogloss:0.29444\n",
      "[78]\ttrain-mlogloss:0.22491\teval-mlogloss:0.29410\n",
      "[79]\ttrain-mlogloss:0.22380\teval-mlogloss:0.29355\n",
      "[80]\ttrain-mlogloss:0.22304\teval-mlogloss:0.29331\n",
      "[81]\ttrain-mlogloss:0.22228\teval-mlogloss:0.29324\n",
      "[82]\ttrain-mlogloss:0.22151\teval-mlogloss:0.29300\n",
      "[83]\ttrain-mlogloss:0.22062\teval-mlogloss:0.29268\n",
      "[84]\ttrain-mlogloss:0.21990\teval-mlogloss:0.29246\n",
      "[85]\ttrain-mlogloss:0.21900\teval-mlogloss:0.29251\n",
      "[86]\ttrain-mlogloss:0.21816\teval-mlogloss:0.29211\n",
      "[87]\ttrain-mlogloss:0.21743\teval-mlogloss:0.29212\n",
      "[88]\ttrain-mlogloss:0.21663\teval-mlogloss:0.29212\n",
      "[89]\ttrain-mlogloss:0.21610\teval-mlogloss:0.29190\n",
      "[90]\ttrain-mlogloss:0.21518\teval-mlogloss:0.29165\n",
      "[91]\ttrain-mlogloss:0.21444\teval-mlogloss:0.29138\n",
      "[92]\ttrain-mlogloss:0.21369\teval-mlogloss:0.29139\n",
      "[93]\ttrain-mlogloss:0.21292\teval-mlogloss:0.29112\n",
      "[94]\ttrain-mlogloss:0.21225\teval-mlogloss:0.29108\n",
      "[95]\ttrain-mlogloss:0.21177\teval-mlogloss:0.29113\n",
      "[96]\ttrain-mlogloss:0.21107\teval-mlogloss:0.29108\n",
      "[97]\ttrain-mlogloss:0.21053\teval-mlogloss:0.29125\n",
      "[98]\ttrain-mlogloss:0.20984\teval-mlogloss:0.29112\n",
      "[99]\ttrain-mlogloss:0.20914\teval-mlogloss:0.29076\n",
      "[100]\ttrain-mlogloss:0.20858\teval-mlogloss:0.29063\n",
      "[101]\ttrain-mlogloss:0.20806\teval-mlogloss:0.29063\n",
      "[102]\ttrain-mlogloss:0.20726\teval-mlogloss:0.29032\n",
      "[103]\ttrain-mlogloss:0.20661\teval-mlogloss:0.29045\n",
      "[104]\ttrain-mlogloss:0.20594\teval-mlogloss:0.29048\n",
      "[105]\ttrain-mlogloss:0.20537\teval-mlogloss:0.29020\n",
      "[106]\ttrain-mlogloss:0.20479\teval-mlogloss:0.29024\n",
      "[107]\ttrain-mlogloss:0.20418\teval-mlogloss:0.28984\n",
      "[108]\ttrain-mlogloss:0.20384\teval-mlogloss:0.28982\n",
      "[109]\ttrain-mlogloss:0.20320\teval-mlogloss:0.28965\n",
      "[110]\ttrain-mlogloss:0.20267\teval-mlogloss:0.28977\n",
      "[111]\ttrain-mlogloss:0.20202\teval-mlogloss:0.28986\n",
      "[112]\ttrain-mlogloss:0.20148\teval-mlogloss:0.28992\n",
      "[113]\ttrain-mlogloss:0.20088\teval-mlogloss:0.28984\n",
      "[114]\ttrain-mlogloss:0.20043\teval-mlogloss:0.28994\n",
      "[115]\ttrain-mlogloss:0.19994\teval-mlogloss:0.28984\n",
      "[116]\ttrain-mlogloss:0.19939\teval-mlogloss:0.28958\n",
      "[117]\ttrain-mlogloss:0.19888\teval-mlogloss:0.28952\n",
      "[118]\ttrain-mlogloss:0.19833\teval-mlogloss:0.28943\n",
      "[119]\ttrain-mlogloss:0.19785\teval-mlogloss:0.28951\n",
      "[120]\ttrain-mlogloss:0.19748\teval-mlogloss:0.28954\n",
      "[121]\ttrain-mlogloss:0.19696\teval-mlogloss:0.28968\n",
      "[122]\ttrain-mlogloss:0.19648\teval-mlogloss:0.28977\n",
      "[123]\ttrain-mlogloss:0.19593\teval-mlogloss:0.28958\n",
      "[124]\ttrain-mlogloss:0.19551\teval-mlogloss:0.28954\n",
      "[125]\ttrain-mlogloss:0.19501\teval-mlogloss:0.28979\n",
      "[126]\ttrain-mlogloss:0.19454\teval-mlogloss:0.29003\n",
      "[127]\ttrain-mlogloss:0.19399\teval-mlogloss:0.28984\n",
      "[128]\ttrain-mlogloss:0.19351\teval-mlogloss:0.28998\n",
      "[129]\ttrain-mlogloss:0.19292\teval-mlogloss:0.29003\n",
      "[130]\ttrain-mlogloss:0.19226\teval-mlogloss:0.29018\n",
      "[131]\ttrain-mlogloss:0.19177\teval-mlogloss:0.29005\n",
      "[132]\ttrain-mlogloss:0.19126\teval-mlogloss:0.28974\n",
      "[133]\ttrain-mlogloss:0.19092\teval-mlogloss:0.28997\n",
      "[134]\ttrain-mlogloss:0.19041\teval-mlogloss:0.29018\n",
      "[135]\ttrain-mlogloss:0.19004\teval-mlogloss:0.29011\n",
      "[136]\ttrain-mlogloss:0.18966\teval-mlogloss:0.28992\n",
      "[137]\ttrain-mlogloss:0.18909\teval-mlogloss:0.29006\n",
      "[138]\ttrain-mlogloss:0.18866\teval-mlogloss:0.29005\n",
      "[139]\ttrain-mlogloss:0.18825\teval-mlogloss:0.29009\n",
      "[140]\ttrain-mlogloss:0.18786\teval-mlogloss:0.29013\n",
      "[141]\ttrain-mlogloss:0.18754\teval-mlogloss:0.29015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142]\ttrain-mlogloss:0.18707\teval-mlogloss:0.29036\n",
      "[143]\ttrain-mlogloss:0.18686\teval-mlogloss:0.29032\n",
      "[144]\ttrain-mlogloss:0.18645\teval-mlogloss:0.29029\n",
      "[145]\ttrain-mlogloss:0.18615\teval-mlogloss:0.29038\n",
      "[146]\ttrain-mlogloss:0.18566\teval-mlogloss:0.29045\n",
      "[147]\ttrain-mlogloss:0.18516\teval-mlogloss:0.29048\n",
      "[148]\ttrain-mlogloss:0.18481\teval-mlogloss:0.29043\n",
      "[149]\ttrain-mlogloss:0.18443\teval-mlogloss:0.29052\n",
      "[150]\ttrain-mlogloss:0.18409\teval-mlogloss:0.29048\n",
      "[151]\ttrain-mlogloss:0.18375\teval-mlogloss:0.29056\n",
      "[152]\ttrain-mlogloss:0.18316\teval-mlogloss:0.29039\n",
      "[153]\ttrain-mlogloss:0.18279\teval-mlogloss:0.29046\n",
      "[154]\ttrain-mlogloss:0.18250\teval-mlogloss:0.29035\n",
      "[155]\ttrain-mlogloss:0.18216\teval-mlogloss:0.29016\n",
      "[156]\ttrain-mlogloss:0.18175\teval-mlogloss:0.29002\n",
      "[157]\ttrain-mlogloss:0.18128\teval-mlogloss:0.28992\n",
      "[158]\ttrain-mlogloss:0.18096\teval-mlogloss:0.29008\n",
      "[159]\ttrain-mlogloss:0.18066\teval-mlogloss:0.29012\n",
      "[160]\ttrain-mlogloss:0.18036\teval-mlogloss:0.29012\n",
      "[161]\ttrain-mlogloss:0.17997\teval-mlogloss:0.29030\n",
      "[162]\ttrain-mlogloss:0.17964\teval-mlogloss:0.29035\n",
      "[163]\ttrain-mlogloss:0.17926\teval-mlogloss:0.29043\n",
      "[164]\ttrain-mlogloss:0.17902\teval-mlogloss:0.29069\n",
      "[165]\ttrain-mlogloss:0.17879\teval-mlogloss:0.29081\n",
      "[166]\ttrain-mlogloss:0.17841\teval-mlogloss:0.29103\n",
      "[167]\ttrain-mlogloss:0.17803\teval-mlogloss:0.29100\n",
      "[168]\ttrain-mlogloss:0.17776\teval-mlogloss:0.29110\n",
      "[169]\ttrain-mlogloss:0.17739\teval-mlogloss:0.29134\n",
      "[170]\ttrain-mlogloss:0.17703\teval-mlogloss:0.29112\n",
      "[171]\ttrain-mlogloss:0.17665\teval-mlogloss:0.29106\n",
      "[172]\ttrain-mlogloss:0.17637\teval-mlogloss:0.29117\n",
      "[173]\ttrain-mlogloss:0.17606\teval-mlogloss:0.29103\n",
      "[174]\ttrain-mlogloss:0.17565\teval-mlogloss:0.29102\n",
      "[175]\ttrain-mlogloss:0.17539\teval-mlogloss:0.29098\n",
      "[176]\ttrain-mlogloss:0.17504\teval-mlogloss:0.29097\n",
      "[177]\ttrain-mlogloss:0.17477\teval-mlogloss:0.29093\n",
      "[178]\ttrain-mlogloss:0.17460\teval-mlogloss:0.29092\n",
      "[179]\ttrain-mlogloss:0.17418\teval-mlogloss:0.29096\n",
      "[180]\ttrain-mlogloss:0.17392\teval-mlogloss:0.29088\n",
      "[181]\ttrain-mlogloss:0.17359\teval-mlogloss:0.29100\n",
      "[182]\ttrain-mlogloss:0.17329\teval-mlogloss:0.29102\n",
      "[183]\ttrain-mlogloss:0.17305\teval-mlogloss:0.29097\n",
      "[184]\ttrain-mlogloss:0.17266\teval-mlogloss:0.29087\n",
      "[185]\ttrain-mlogloss:0.17248\teval-mlogloss:0.29087\n",
      "[186]\ttrain-mlogloss:0.17212\teval-mlogloss:0.29071\n",
      "[187]\ttrain-mlogloss:0.17178\teval-mlogloss:0.29069\n",
      "[188]\ttrain-mlogloss:0.17157\teval-mlogloss:0.29078\n",
      "[189]\ttrain-mlogloss:0.17123\teval-mlogloss:0.29090\n",
      "[190]\ttrain-mlogloss:0.17080\teval-mlogloss:0.29088\n",
      "[191]\ttrain-mlogloss:0.17033\teval-mlogloss:0.29070\n",
      "[192]\ttrain-mlogloss:0.17014\teval-mlogloss:0.29067\n",
      "[193]\ttrain-mlogloss:0.16985\teval-mlogloss:0.29069\n",
      "[194]\ttrain-mlogloss:0.16966\teval-mlogloss:0.29077\n",
      "[195]\ttrain-mlogloss:0.16939\teval-mlogloss:0.29087\n",
      "[196]\ttrain-mlogloss:0.16914\teval-mlogloss:0.29094\n",
      "[197]\ttrain-mlogloss:0.16889\teval-mlogloss:0.29106\n",
      "[198]\ttrain-mlogloss:0.16859\teval-mlogloss:0.29115\n",
      "[199]\ttrain-mlogloss:0.16825\teval-mlogloss:0.29135\n",
      "[200]\ttrain-mlogloss:0.16788\teval-mlogloss:0.29144\n",
      "[201]\ttrain-mlogloss:0.16761\teval-mlogloss:0.29139\n",
      "[202]\ttrain-mlogloss:0.16743\teval-mlogloss:0.29124\n",
      "[203]\ttrain-mlogloss:0.16717\teval-mlogloss:0.29132\n",
      "[204]\ttrain-mlogloss:0.16687\teval-mlogloss:0.29158\n",
      "[205]\ttrain-mlogloss:0.16661\teval-mlogloss:0.29176\n",
      "[206]\ttrain-mlogloss:0.16638\teval-mlogloss:0.29169\n",
      "[207]\ttrain-mlogloss:0.16615\teval-mlogloss:0.29171\n",
      "[208]\ttrain-mlogloss:0.16596\teval-mlogloss:0.29165\n",
      "[209]\ttrain-mlogloss:0.16570\teval-mlogloss:0.29161\n",
      "[210]\ttrain-mlogloss:0.16533\teval-mlogloss:0.29138\n",
      "[211]\ttrain-mlogloss:0.16512\teval-mlogloss:0.29138\n",
      "[212]\ttrain-mlogloss:0.16492\teval-mlogloss:0.29146\n",
      "[213]\ttrain-mlogloss:0.16458\teval-mlogloss:0.29143\n",
      "[214]\ttrain-mlogloss:0.16438\teval-mlogloss:0.29148\n",
      "[215]\ttrain-mlogloss:0.16423\teval-mlogloss:0.29131\n",
      "[216]\ttrain-mlogloss:0.16394\teval-mlogloss:0.29120\n",
      "[217]\ttrain-mlogloss:0.16364\teval-mlogloss:0.29119\n",
      "[218]\ttrain-mlogloss:0.16330\teval-mlogloss:0.29120\n",
      "xgb now score is: [2.3884487102925775, 2.5234747439064087, 2.5294904569908976, 2.416885608518496]\n",
      "[0]\ttrain-mlogloss:0.67139\teval-mlogloss:0.67048\n",
      "[1]\ttrain-mlogloss:0.65113\teval-mlogloss:0.64952\n",
      "[2]\ttrain-mlogloss:0.63197\teval-mlogloss:0.63000\n",
      "[3]\ttrain-mlogloss:0.61384\teval-mlogloss:0.61151\n",
      "[4]\ttrain-mlogloss:0.59666\teval-mlogloss:0.59404\n",
      "[5]\ttrain-mlogloss:0.58066\teval-mlogloss:0.57744\n",
      "[6]\ttrain-mlogloss:0.56513\teval-mlogloss:0.56147\n",
      "[7]\ttrain-mlogloss:0.55069\teval-mlogloss:0.54648\n",
      "[8]\ttrain-mlogloss:0.53688\teval-mlogloss:0.53223\n",
      "[9]\ttrain-mlogloss:0.52363\teval-mlogloss:0.51864\n",
      "[10]\ttrain-mlogloss:0.51095\teval-mlogloss:0.50593\n",
      "[11]\ttrain-mlogloss:0.49910\teval-mlogloss:0.49384\n",
      "[12]\ttrain-mlogloss:0.48781\teval-mlogloss:0.48212\n",
      "[13]\ttrain-mlogloss:0.47687\teval-mlogloss:0.47096\n",
      "[14]\ttrain-mlogloss:0.46641\teval-mlogloss:0.46018\n",
      "[15]\ttrain-mlogloss:0.45663\teval-mlogloss:0.45023\n",
      "[16]\ttrain-mlogloss:0.44688\teval-mlogloss:0.44018\n",
      "[17]\ttrain-mlogloss:0.43796\teval-mlogloss:0.43090\n",
      "[18]\ttrain-mlogloss:0.42931\teval-mlogloss:0.42180\n",
      "[19]\ttrain-mlogloss:0.42110\teval-mlogloss:0.41336\n",
      "[20]\ttrain-mlogloss:0.41331\teval-mlogloss:0.40523\n",
      "[21]\ttrain-mlogloss:0.40578\teval-mlogloss:0.39737\n",
      "[22]\ttrain-mlogloss:0.39840\teval-mlogloss:0.38964\n",
      "[23]\ttrain-mlogloss:0.39138\teval-mlogloss:0.38235\n",
      "[24]\ttrain-mlogloss:0.38496\teval-mlogloss:0.37599\n",
      "[25]\ttrain-mlogloss:0.37871\teval-mlogloss:0.36965\n",
      "[26]\ttrain-mlogloss:0.37274\teval-mlogloss:0.36356\n",
      "[27]\ttrain-mlogloss:0.36700\teval-mlogloss:0.35770\n",
      "[28]\ttrain-mlogloss:0.36142\teval-mlogloss:0.35174\n",
      "[29]\ttrain-mlogloss:0.35606\teval-mlogloss:0.34630\n",
      "[30]\ttrain-mlogloss:0.35079\teval-mlogloss:0.34105\n",
      "[31]\ttrain-mlogloss:0.34590\teval-mlogloss:0.33620\n",
      "[32]\ttrain-mlogloss:0.34128\teval-mlogloss:0.33141\n",
      "[33]\ttrain-mlogloss:0.33680\teval-mlogloss:0.32680\n",
      "[34]\ttrain-mlogloss:0.33241\teval-mlogloss:0.32219\n",
      "[35]\ttrain-mlogloss:0.32825\teval-mlogloss:0.31800\n",
      "[36]\ttrain-mlogloss:0.32437\teval-mlogloss:0.31407\n",
      "[37]\ttrain-mlogloss:0.32052\teval-mlogloss:0.31008\n",
      "[38]\ttrain-mlogloss:0.31666\teval-mlogloss:0.30629\n",
      "[39]\ttrain-mlogloss:0.31302\teval-mlogloss:0.30266\n",
      "[40]\ttrain-mlogloss:0.30963\teval-mlogloss:0.29904\n",
      "[41]\ttrain-mlogloss:0.30628\teval-mlogloss:0.29598\n",
      "[42]\ttrain-mlogloss:0.30307\teval-mlogloss:0.29289\n",
      "[43]\ttrain-mlogloss:0.29983\teval-mlogloss:0.28995\n",
      "[44]\ttrain-mlogloss:0.29687\teval-mlogloss:0.28689\n",
      "[45]\ttrain-mlogloss:0.29399\teval-mlogloss:0.28407\n",
      "[46]\ttrain-mlogloss:0.29114\teval-mlogloss:0.28114\n",
      "[47]\ttrain-mlogloss:0.28834\teval-mlogloss:0.27866\n",
      "[48]\ttrain-mlogloss:0.28602\teval-mlogloss:0.27634\n",
      "[49]\ttrain-mlogloss:0.28339\teval-mlogloss:0.27376\n",
      "[50]\ttrain-mlogloss:0.28114\teval-mlogloss:0.27145\n",
      "[51]\ttrain-mlogloss:0.27866\teval-mlogloss:0.26934\n",
      "[52]\ttrain-mlogloss:0.27646\teval-mlogloss:0.26717\n",
      "[53]\ttrain-mlogloss:0.27421\teval-mlogloss:0.26508\n",
      "[54]\ttrain-mlogloss:0.27208\teval-mlogloss:0.26309\n",
      "[55]\ttrain-mlogloss:0.26996\teval-mlogloss:0.26119\n",
      "[56]\ttrain-mlogloss:0.26793\teval-mlogloss:0.25934\n",
      "[57]\ttrain-mlogloss:0.26586\teval-mlogloss:0.25755\n",
      "[58]\ttrain-mlogloss:0.26402\teval-mlogloss:0.25577\n",
      "[59]\ttrain-mlogloss:0.26224\teval-mlogloss:0.25428\n",
      "[60]\ttrain-mlogloss:0.26046\teval-mlogloss:0.25292\n",
      "[61]\ttrain-mlogloss:0.25892\teval-mlogloss:0.25152\n",
      "[62]\ttrain-mlogloss:0.25733\teval-mlogloss:0.25002\n",
      "[63]\ttrain-mlogloss:0.25566\teval-mlogloss:0.24857\n",
      "[64]\ttrain-mlogloss:0.25407\teval-mlogloss:0.24732\n",
      "[65]\ttrain-mlogloss:0.25261\teval-mlogloss:0.24591\n",
      "[66]\ttrain-mlogloss:0.25128\teval-mlogloss:0.24481\n",
      "[67]\ttrain-mlogloss:0.24981\teval-mlogloss:0.24369\n",
      "[68]\ttrain-mlogloss:0.24861\teval-mlogloss:0.24259\n",
      "[69]\ttrain-mlogloss:0.24726\teval-mlogloss:0.24136\n",
      "[70]\ttrain-mlogloss:0.24594\teval-mlogloss:0.24011\n",
      "[71]\ttrain-mlogloss:0.24477\teval-mlogloss:0.23912\n",
      "[72]\ttrain-mlogloss:0.24353\teval-mlogloss:0.23813\n",
      "[73]\ttrain-mlogloss:0.24248\teval-mlogloss:0.23723\n",
      "[74]\ttrain-mlogloss:0.24118\teval-mlogloss:0.23639\n",
      "[75]\ttrain-mlogloss:0.24002\teval-mlogloss:0.23560\n",
      "[76]\ttrain-mlogloss:0.23897\teval-mlogloss:0.23488\n",
      "[77]\ttrain-mlogloss:0.23785\teval-mlogloss:0.23421\n",
      "[78]\ttrain-mlogloss:0.23666\teval-mlogloss:0.23349\n",
      "[79]\ttrain-mlogloss:0.23582\teval-mlogloss:0.23283\n",
      "[80]\ttrain-mlogloss:0.23481\teval-mlogloss:0.23245\n",
      "[81]\ttrain-mlogloss:0.23398\teval-mlogloss:0.23187\n",
      "[82]\ttrain-mlogloss:0.23306\teval-mlogloss:0.23136\n",
      "[83]\ttrain-mlogloss:0.23227\teval-mlogloss:0.23072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84]\ttrain-mlogloss:0.23159\teval-mlogloss:0.23013\n",
      "[85]\ttrain-mlogloss:0.23088\teval-mlogloss:0.22952\n",
      "[86]\ttrain-mlogloss:0.23003\teval-mlogloss:0.22905\n",
      "[87]\ttrain-mlogloss:0.22918\teval-mlogloss:0.22864\n",
      "[88]\ttrain-mlogloss:0.22838\teval-mlogloss:0.22810\n",
      "[89]\ttrain-mlogloss:0.22764\teval-mlogloss:0.22773\n",
      "[90]\ttrain-mlogloss:0.22686\teval-mlogloss:0.22726\n",
      "[91]\ttrain-mlogloss:0.22620\teval-mlogloss:0.22690\n",
      "[92]\ttrain-mlogloss:0.22522\teval-mlogloss:0.22656\n",
      "[93]\ttrain-mlogloss:0.22444\teval-mlogloss:0.22620\n",
      "[94]\ttrain-mlogloss:0.22373\teval-mlogloss:0.22579\n",
      "[95]\ttrain-mlogloss:0.22307\teval-mlogloss:0.22548\n",
      "[96]\ttrain-mlogloss:0.22234\teval-mlogloss:0.22513\n",
      "[97]\ttrain-mlogloss:0.22150\teval-mlogloss:0.22489\n",
      "[98]\ttrain-mlogloss:0.22079\teval-mlogloss:0.22449\n",
      "[99]\ttrain-mlogloss:0.22011\teval-mlogloss:0.22420\n",
      "[100]\ttrain-mlogloss:0.21945\teval-mlogloss:0.22391\n",
      "[101]\ttrain-mlogloss:0.21885\teval-mlogloss:0.22373\n",
      "[102]\ttrain-mlogloss:0.21824\teval-mlogloss:0.22350\n",
      "[103]\ttrain-mlogloss:0.21744\teval-mlogloss:0.22335\n",
      "[104]\ttrain-mlogloss:0.21663\teval-mlogloss:0.22302\n",
      "[105]\ttrain-mlogloss:0.21604\teval-mlogloss:0.22284\n",
      "[106]\ttrain-mlogloss:0.21539\teval-mlogloss:0.22262\n",
      "[107]\ttrain-mlogloss:0.21463\teval-mlogloss:0.22250\n",
      "[108]\ttrain-mlogloss:0.21407\teval-mlogloss:0.22231\n",
      "[109]\ttrain-mlogloss:0.21339\teval-mlogloss:0.22220\n",
      "[110]\ttrain-mlogloss:0.21289\teval-mlogloss:0.22203\n",
      "[111]\ttrain-mlogloss:0.21212\teval-mlogloss:0.22189\n",
      "[112]\ttrain-mlogloss:0.21165\teval-mlogloss:0.22166\n",
      "[113]\ttrain-mlogloss:0.21119\teval-mlogloss:0.22158\n",
      "[114]\ttrain-mlogloss:0.21057\teval-mlogloss:0.22141\n",
      "[115]\ttrain-mlogloss:0.20985\teval-mlogloss:0.22123\n",
      "[116]\ttrain-mlogloss:0.20925\teval-mlogloss:0.22115\n",
      "[117]\ttrain-mlogloss:0.20872\teval-mlogloss:0.22099\n",
      "[118]\ttrain-mlogloss:0.20819\teval-mlogloss:0.22088\n",
      "[119]\ttrain-mlogloss:0.20779\teval-mlogloss:0.22069\n",
      "[120]\ttrain-mlogloss:0.20723\teval-mlogloss:0.22074\n",
      "[121]\ttrain-mlogloss:0.20670\teval-mlogloss:0.22068\n",
      "[122]\ttrain-mlogloss:0.20637\teval-mlogloss:0.22049\n",
      "[123]\ttrain-mlogloss:0.20584\teval-mlogloss:0.22055\n",
      "[124]\ttrain-mlogloss:0.20541\teval-mlogloss:0.22054\n",
      "[125]\ttrain-mlogloss:0.20488\teval-mlogloss:0.22059\n",
      "[126]\ttrain-mlogloss:0.20440\teval-mlogloss:0.22055\n",
      "[127]\ttrain-mlogloss:0.20390\teval-mlogloss:0.22065\n",
      "[128]\ttrain-mlogloss:0.20343\teval-mlogloss:0.22060\n",
      "[129]\ttrain-mlogloss:0.20281\teval-mlogloss:0.22064\n",
      "[130]\ttrain-mlogloss:0.20242\teval-mlogloss:0.22078\n",
      "[131]\ttrain-mlogloss:0.20192\teval-mlogloss:0.22065\n",
      "[132]\ttrain-mlogloss:0.20144\teval-mlogloss:0.22061\n",
      "[133]\ttrain-mlogloss:0.20100\teval-mlogloss:0.22057\n",
      "[134]\ttrain-mlogloss:0.20058\teval-mlogloss:0.22062\n",
      "[135]\ttrain-mlogloss:0.20011\teval-mlogloss:0.22065\n",
      "[136]\ttrain-mlogloss:0.19957\teval-mlogloss:0.22062\n",
      "[137]\ttrain-mlogloss:0.19907\teval-mlogloss:0.22052\n",
      "[138]\ttrain-mlogloss:0.19857\teval-mlogloss:0.22054\n",
      "[139]\ttrain-mlogloss:0.19822\teval-mlogloss:0.22051\n",
      "[140]\ttrain-mlogloss:0.19766\teval-mlogloss:0.22049\n",
      "[141]\ttrain-mlogloss:0.19710\teval-mlogloss:0.22056\n",
      "[142]\ttrain-mlogloss:0.19669\teval-mlogloss:0.22061\n",
      "[143]\ttrain-mlogloss:0.19631\teval-mlogloss:0.22059\n",
      "[144]\ttrain-mlogloss:0.19590\teval-mlogloss:0.22067\n",
      "[145]\ttrain-mlogloss:0.19540\teval-mlogloss:0.22078\n",
      "[146]\ttrain-mlogloss:0.19501\teval-mlogloss:0.22070\n",
      "[147]\ttrain-mlogloss:0.19455\teval-mlogloss:0.22080\n",
      "[148]\ttrain-mlogloss:0.19417\teval-mlogloss:0.22096\n",
      "[149]\ttrain-mlogloss:0.19377\teval-mlogloss:0.22084\n",
      "[150]\ttrain-mlogloss:0.19333\teval-mlogloss:0.22092\n",
      "[151]\ttrain-mlogloss:0.19284\teval-mlogloss:0.22098\n",
      "[152]\ttrain-mlogloss:0.19242\teval-mlogloss:0.22114\n",
      "[153]\ttrain-mlogloss:0.19202\teval-mlogloss:0.22113\n",
      "[154]\ttrain-mlogloss:0.19159\teval-mlogloss:0.22110\n",
      "[155]\ttrain-mlogloss:0.19117\teval-mlogloss:0.22092\n",
      "[156]\ttrain-mlogloss:0.19072\teval-mlogloss:0.22094\n",
      "[157]\ttrain-mlogloss:0.19039\teval-mlogloss:0.22105\n",
      "[158]\ttrain-mlogloss:0.18994\teval-mlogloss:0.22112\n",
      "[159]\ttrain-mlogloss:0.18957\teval-mlogloss:0.22096\n",
      "[160]\ttrain-mlogloss:0.18915\teval-mlogloss:0.22093\n",
      "[161]\ttrain-mlogloss:0.18875\teval-mlogloss:0.22104\n",
      "[162]\ttrain-mlogloss:0.18853\teval-mlogloss:0.22121\n",
      "[163]\ttrain-mlogloss:0.18820\teval-mlogloss:0.22100\n",
      "[164]\ttrain-mlogloss:0.18791\teval-mlogloss:0.22117\n",
      "[165]\ttrain-mlogloss:0.18754\teval-mlogloss:0.22115\n",
      "[166]\ttrain-mlogloss:0.18705\teval-mlogloss:0.22126\n",
      "[167]\ttrain-mlogloss:0.18667\teval-mlogloss:0.22133\n",
      "[168]\ttrain-mlogloss:0.18641\teval-mlogloss:0.22147\n",
      "[169]\ttrain-mlogloss:0.18597\teval-mlogloss:0.22155\n",
      "[170]\ttrain-mlogloss:0.18565\teval-mlogloss:0.22151\n",
      "[171]\ttrain-mlogloss:0.18529\teval-mlogloss:0.22155\n",
      "[172]\ttrain-mlogloss:0.18496\teval-mlogloss:0.22165\n",
      "[173]\ttrain-mlogloss:0.18459\teval-mlogloss:0.22140\n",
      "[174]\ttrain-mlogloss:0.18419\teval-mlogloss:0.22153\n",
      "[175]\ttrain-mlogloss:0.18398\teval-mlogloss:0.22167\n",
      "[176]\ttrain-mlogloss:0.18361\teval-mlogloss:0.22164\n",
      "[177]\ttrain-mlogloss:0.18328\teval-mlogloss:0.22185\n",
      "[178]\ttrain-mlogloss:0.18291\teval-mlogloss:0.22197\n",
      "[179]\ttrain-mlogloss:0.18263\teval-mlogloss:0.22206\n",
      "[180]\ttrain-mlogloss:0.18236\teval-mlogloss:0.22213\n",
      "[181]\ttrain-mlogloss:0.18208\teval-mlogloss:0.22207\n",
      "[182]\ttrain-mlogloss:0.18172\teval-mlogloss:0.22205\n",
      "[183]\ttrain-mlogloss:0.18147\teval-mlogloss:0.22198\n",
      "[184]\ttrain-mlogloss:0.18115\teval-mlogloss:0.22215\n",
      "[185]\ttrain-mlogloss:0.18096\teval-mlogloss:0.22208\n",
      "[186]\ttrain-mlogloss:0.18065\teval-mlogloss:0.22198\n",
      "[187]\ttrain-mlogloss:0.18031\teval-mlogloss:0.22216\n",
      "[188]\ttrain-mlogloss:0.18001\teval-mlogloss:0.22239\n",
      "[189]\ttrain-mlogloss:0.17977\teval-mlogloss:0.22246\n",
      "[190]\ttrain-mlogloss:0.17941\teval-mlogloss:0.22256\n",
      "[191]\ttrain-mlogloss:0.17900\teval-mlogloss:0.22260\n",
      "[192]\ttrain-mlogloss:0.17868\teval-mlogloss:0.22250\n",
      "[193]\ttrain-mlogloss:0.17844\teval-mlogloss:0.22257\n",
      "[194]\ttrain-mlogloss:0.17818\teval-mlogloss:0.22265\n",
      "[195]\ttrain-mlogloss:0.17799\teval-mlogloss:0.22265\n",
      "[196]\ttrain-mlogloss:0.17766\teval-mlogloss:0.22239\n",
      "[197]\ttrain-mlogloss:0.17729\teval-mlogloss:0.22221\n",
      "[198]\ttrain-mlogloss:0.17706\teval-mlogloss:0.22206\n",
      "[199]\ttrain-mlogloss:0.17690\teval-mlogloss:0.22215\n",
      "[200]\ttrain-mlogloss:0.17656\teval-mlogloss:0.22218\n",
      "[201]\ttrain-mlogloss:0.17625\teval-mlogloss:0.22233\n",
      "[202]\ttrain-mlogloss:0.17608\teval-mlogloss:0.22236\n",
      "[203]\ttrain-mlogloss:0.17582\teval-mlogloss:0.22234\n",
      "[204]\ttrain-mlogloss:0.17555\teval-mlogloss:0.22229\n",
      "[205]\ttrain-mlogloss:0.17535\teval-mlogloss:0.22245\n",
      "[206]\ttrain-mlogloss:0.17501\teval-mlogloss:0.22236\n",
      "[207]\ttrain-mlogloss:0.17477\teval-mlogloss:0.22258\n",
      "[208]\ttrain-mlogloss:0.17455\teval-mlogloss:0.22268\n",
      "[209]\ttrain-mlogloss:0.17432\teval-mlogloss:0.22271\n",
      "[210]\ttrain-mlogloss:0.17404\teval-mlogloss:0.22270\n",
      "[211]\ttrain-mlogloss:0.17371\teval-mlogloss:0.22293\n",
      "[212]\ttrain-mlogloss:0.17341\teval-mlogloss:0.22295\n",
      "[213]\ttrain-mlogloss:0.17317\teval-mlogloss:0.22300\n",
      "[214]\ttrain-mlogloss:0.17280\teval-mlogloss:0.22316\n",
      "[215]\ttrain-mlogloss:0.17255\teval-mlogloss:0.22315\n",
      "[216]\ttrain-mlogloss:0.17224\teval-mlogloss:0.22316\n",
      "[217]\ttrain-mlogloss:0.17203\teval-mlogloss:0.22324\n",
      "[218]\ttrain-mlogloss:0.17178\teval-mlogloss:0.22319\n",
      "[219]\ttrain-mlogloss:0.17153\teval-mlogloss:0.22331\n",
      "[220]\ttrain-mlogloss:0.17127\teval-mlogloss:0.22341\n",
      "[221]\ttrain-mlogloss:0.17109\teval-mlogloss:0.22341\n",
      "[222]\ttrain-mlogloss:0.17094\teval-mlogloss:0.22342\n",
      "xgb now score is: [2.3884487102925775, 2.5234747439064087, 2.5294904569908976, 2.416885608518496, 2.50233102417551]\n",
      "xgb_score_list: [2.3884487102925775, 2.5234747439064087, 2.5294904569908976, 2.416885608518496, 2.50233102417551]\n",
      "xgb_score_mean: 2.472126108776778\n"
     ]
    }
   ],
   "source": [
    "clf_list = clf_list\n",
    "column_list = []\n",
    "train_data_list=[]\n",
    "test_data_list=[]\n",
    "for clf in clf_list:\n",
    "    train_data,test_data,clf_name=clf(x_train, y_train, x_valid, kf, label_split=None)\n",
    "    train_data_list.append(train_data)\n",
    "    test_data_list.append(test_data)\n",
    "train_stacking = np.concatenate(train_data_list, axis=1)\n",
    "test_stacking = np.concatenate(test_data_list, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c459c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:12:48.713569Z",
     "start_time": "2022-02-14T10:12:48.703146Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stacking.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "347b5b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:12:24.183473Z",
     "start_time": "2022-02-14T10:12:24.171470Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 合并所有特征\n",
    "train = pd.DataFrame(np.concatenate([x_train, train_stacking], axis=1))\n",
    "test = np.concatenate([x_valid, test_stacking], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "273a2755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:13:11.801994Z",
     "start_time": "2022-02-14T10:13:11.795893Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_all = pd.DataFrame(train)\n",
    "df_train_all.columns = features_columns + clf_list_col\n",
    "df_test_all = pd.DataFrame(test)\n",
    "df_test_all.columns = features_columns + clf_list_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8afdb7fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:13:26.438820Z",
     "start_time": "2022-02-14T10:13:26.425808Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_all['label'] = all_data_test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23a34363",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-14T10:13:39.010640Z",
     "start_time": "2022-02-14T10:13:38.526701Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train_all.to_csv('train_all.csv',header=True,index=False)\n",
    "df_test_all.to_csv('test_all.csv',header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1cb54b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
