{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03a7e8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:21:45.478809Z",
     "start_time": "2022-01-11T02:21:45.008563Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "torch.set_default_tensor_type(torch.DoubleTensor) \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "class ExternalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model,S=64):\n",
    "        super().__init__()\n",
    "        self.mk=nn.Linear(d_model,S,bias=False)\n",
    "        self.mv=nn.Linear(S,d_model,bias=False)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries):\n",
    "        attn=self.mk(queries) #bs,n,S\n",
    "        attn=self.softmax(attn) #bs,n,S\n",
    "        attn=attn/torch.sum(attn,dim=2,keepdim=True) #bs,n,S\n",
    "        out=self.mv(attn) #bs,n,d_model\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0c00f445",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:48:56.356804Z",
     "start_time": "2022-01-11T03:48:56.333796Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    '''\n",
    "    Scaled dot-product attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, d_model, d_k, d_v, h,dropout=.1):\n",
    "        '''\n",
    "        :param d_model: Output dimensionality of the model\n",
    "        :param d_k: Dimensionality of queries and keys\n",
    "        :param d_v: Dimensionality of values\n",
    "        :param h: Number of heads\n",
    "        '''\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                init.constant_(m.weight, 1)\n",
    "                init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.normal_(m.weight, std=0.001)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values, attention_mask=None, attention_weights=None):\n",
    "        '''\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :param attention_mask: Mask over attention values (b_s, h, nq, nk). True indicates masking.\n",
    "        :param attention_weights: Multiplicative weights for attention values (b_s, h, nq, nk).\n",
    "        :return:\n",
    "        '''\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "        if attention_weights is not None:\n",
    "            att = att * attention_weights\n",
    "        if attention_mask is not None:\n",
    "            att = att.masked_fill(attention_mask, -np.inf)\n",
    "        att = torch.softmax(att, -1)\n",
    "        att=self.dropout(att)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1966b9f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:20:38.233673Z",
     "start_time": "2022-01-11T02:20:38.153720Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('poker.csv')\n",
    "dev, off = train_test_split(data, test_size=0.3, stratify=data['CLASS'],random_state=328)\n",
    "dev_y = dev.pop('CLASS')\n",
    "off_y = off.pop('CLASS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be623522",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:53:49.964235Z",
     "start_time": "2022-01-11T02:53:49.956693Z"
    }
   },
   "outputs": [],
   "source": [
    "num_emb = dev.nunique().max()+1\n",
    "num_dim =16\n",
    "#num_dim = round(num_emb**0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fc384765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:53:51.681830Z",
     "start_time": "2022-01-11T02:53:51.665695Z"
    }
   },
   "outputs": [],
   "source": [
    "x = torch.tensor(dev.to_numpy(),dtype=torch.long)\n",
    "y = torch.tensor(dev_y.to_numpy(),dtype=torch.long)\n",
    "\n",
    "val_x = torch.tensor(off.to_numpy(),dtype=torch.long)\n",
    "val_y = torch.tensor(off_y.to_numpy(),dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a00d6280",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:53:53.835845Z",
     "start_time": "2022-01-11T02:53:53.829844Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(num_emb, num_dim)\n",
    "embed_x = embedding(x)\n",
    "embed_val = embedding(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1f71cf03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T02:53:59.275194Z",
     "start_time": "2022-01-11T02:53:59.213756Z"
    }
   },
   "outputs": [],
   "source": [
    "ea =ExternalAttention(d_model=num_dim,S=2)\n",
    "at_x=ea(embed_x)\n",
    "at_val=ea(embed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1fe180c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-11T03:51:16.292783Z",
     "start_time": "2022-01-11T03:51:15.282860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69998, 10, 8])\n"
     ]
    }
   ],
   "source": [
    "sa = ScaledDotProductAttention(d_model=num_dim, d_k=num_dim, d_v=num_dim, h=2)\n",
    "output=sa(embed_x,embed_x,embed_x)\n",
    "output=sa(embed_val,embed_val,embed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e376d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
