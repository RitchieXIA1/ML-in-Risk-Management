{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df24880",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import *\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "seed_everything(456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A = pd.read_csv('A_train.csv')\n",
    "train_B = pd.read_csv('B_train.csv')\n",
    "test = pd.read_csv('B_test.csv')\n",
    "sample = pd.read_csv('submit_sample.csv')\n",
    "train_B_info = train_B.describe()\n",
    "useful_col = []\n",
    "for col in train_B_info.columns:\n",
    "    if train_B_info.ix[0,col] > train_B.shape[0]*0.01:\n",
    "        useful_col.append(col)\n",
    "train_B_1 = train_B[useful_col].copy()#train_B_1 = train_B_1.fillna(-999)\n",
    "relation = train_B_1.corr()\n",
    "train_A_1 = train_A[useful_col].copy()#train_A_1 = train_A_1.fillna(-999)\n",
    "length = relation.shape[0]\n",
    "high_corr = list()\n",
    "final_cols = []\n",
    "del_cols = []\n",
    "for i in range(length):\n",
    "    if relation.columns[i] not in del_cols:\n",
    "        final_cols.append(relation.columns[i])\n",
    "        for j in range(i+1, length):\n",
    "            if (relation.iloc[i,j] > 0.98) and (relation.columns[j] not in del_cols):\n",
    "                del_cols.append(relation.columns[j])\n",
    "train_B_1 = train_B_1[final_cols]train_A_1 = train_A_1[final_cols]\n",
    "train_B_flag = train_B_1['flag']\n",
    "train_B_1.drop('no', axis = 1, inplace = True)\n",
    "train_B_1.drop('flag', axis = 1, inplace = True)\n",
    "train_A_flag = train_A_1['flag']\n",
    "train_A_1.drop('no', axis = 1, inplace = True)\n",
    "train_A_1.drop('flag', axis = 1, inplace = True)\n",
    "train_B_1_valid,train_B_1_test,train_B_1_valid_y,train_B_1_test_y=train_test_split(train_B_1,train_B_flag,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f4f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trate = 0.25\n",
    "params = {'booster':'gbtree',\n",
    "          'eta':0.1,\n",
    "          'max_depth':20,\n",
    "          'max_delta_step':0,\n",
    "          'subsample':1,\n",
    "          'colsample_bytree':1,\n",
    "          'base_score':Trate,\n",
    "          'objective':'binary:logistic',\n",
    "          'lambda':5,\n",
    "          'alpha':8,\n",
    "          'n_estimators':500,\n",
    "          'random_seed':100,\n",
    "          'n_jobs':-1\n",
    "}\n",
    "clf=xgb.XGBClassifier(**params)\n",
    "clf.fit(train_A_1,train_A_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8323da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_A=clf.predict_proba(train_A_1)[:,1]\n",
    "y_pred_B_valid=clf.predict_proba(train_B_1_valid)[:,1]\n",
    "y_pred_B_test=clf.predict_proba(train_B_1_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceec30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" train AUC = {roc_auc_score(train_A_flag,y_pred_A)}\")\n",
    "print(f\" valid AUC = {roc_auc_score(train_B_1_valid_y,y_pred_B_valid)}\")\n",
    "print(f\" test AUC = {roc_auc_score(train_B_1_test_y,y_pred_B_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d923a6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Tradaboost(object):##针对二分类设计的tradaboost\n",
    "    def __init__(self,N=None,base_estimator=None,threshold=None,score=roc_auc_score):    \n",
    "        self.N=N\n",
    "        self.threshold=threshold\n",
    "        self.base_estimator=base_estimator\n",
    "        self.score=score\n",
    "        self.estimators=[]\n",
    "            \n",
    "    # 权重的标准化，其实差别不大，在前面的例子中也有用到过\n",
    "    def _calculate_weights(self,weights):      \n",
    "        total = np.sum(weights)      \n",
    "        return np.asarray(weights / total, order='C')      \n",
    "          \n",
    "\n",
    "          \n",
    "    #计算目标域上的错误率     \n",
    "    def _calculate_error_rate(self,y_true, y_pred, weight):      \n",
    "        total = np.sum(weight)      \n",
    "        return np.sum(weight[:, 0] / total * np.abs(y_true - y_pred))      \n",
    "          \n",
    "    #根据逻辑回归输出的score的得到标签，注意这里不能用predict直接输出标签      \n",
    "   \n",
    "          \n",
    "    def fit(self,source,target,source_label,target_label,early_stopping_rounds):#注意，输入要转为numpy格式的\n",
    "        \n",
    "        source_shape=source.shape[0]\n",
    "        target_shape=target.shape[0]\n",
    "        trans_data = np.concatenate((source, target), axis=0)      \n",
    "        trans_label = np.concatenate((source_label,target_label), axis=0)      \n",
    "        weights_source = np.ones([source_shape, 1])/source_shape      \n",
    "        weights_target = np.ones([target_shape, 1])/target_shape\n",
    "        weights = np.concatenate((weights_source, weights_target), axis=0)\n",
    "        \n",
    "        # 根据公式初始化参数，具体可见原文\n",
    "        \n",
    "        bata = 1 / (1 + np.sqrt(2 * np.log(source_shape / self.N)))    \n",
    "        bata_T = np.zeros([1, self.N])\n",
    "        result_label = np.ones([source_shape+target_shape, self.N])    \n",
    "\n",
    "        trans_data = np.asarray(trans_data, order='C')     #行优先\n",
    "        trans_label = np.asarray(trans_label, order='C')     \n",
    "        \n",
    "        score=0\n",
    "        flag=0\n",
    "        \n",
    "        for i in range(self.N):      \n",
    "            P = self._calculate_weights(weights)      #权重的标准化\n",
    "            self.base_estimator.fit(trans_data,trans_label,P*100)#这里xgb有bug，，如果权重系数太小貌似是被忽略掉了？\n",
    "            self.estimators.append(self.base_estimator)\n",
    "            y_preds=self.base_estimator.predict_proba(trans_data)[:,1] #全量数据的预测\n",
    "            result_label[:, i]=y_preds #保存全量数据的预测结果用于后面的各个模型的评价\n",
    "             \n",
    "\n",
    "            #注意，仅仅计算在目标域上的错误率 ，\n",
    "            y_target_pred=self.base_estimator.predict_proba(target)[:,1]#目标域的预测\n",
    "            error_rate = self._calculate_error_rate(target_label, (y_target_pred>self.threshold).astype(int),  \\\n",
    "                                              weights[source_shape:source_shape + target_shape, :])  \n",
    "            #根据不同的判断阈值来对二分类的标签进行判断，对于不均衡的数据集合很有效，比如100：1的数据集，不设置class_wegiht\n",
    "            #的情况下需要将正负样本的阈值提高到99%.\n",
    "            \n",
    "            # 防止过拟合     \n",
    "            if error_rate > 0.5:      \n",
    "                error_rate = 0.5      \n",
    "            if error_rate == 0:      \n",
    "                N = i      \n",
    "                break       \n",
    "\n",
    "            bata_T[0, i] = error_rate / (1 - error_rate)      \n",
    "\n",
    "            # 调整目标域样本权重      \n",
    "            for j in range(target_shape):      \n",
    "                weights[source_shape + j] = weights[source_shape + j] * \\\n",
    "                np.power(bata_T[0, i],(-np.abs(result_label[source_shape + j, i] - target_label[j])))\n",
    "\n",
    "                \n",
    "            # 调整源域样本权重      \n",
    "            for j in range(source_shape):      \n",
    "                weights[j] = weights[j] * np.power(bata,np.abs(result_label[j, i] - source_label[j]))\n",
    "                \n",
    "            tp=self.score(target_label,y_target_pred)\n",
    "            print('The '+str(i)+' rounds score is '+str(tp))\n",
    "            if tp > score :      \n",
    "                score = tp      \n",
    "                best_round = i  \n",
    "                flag=0\n",
    "            else:\n",
    "                flag+=1\n",
    "            if flag >=early_stopping_rounds:  \n",
    "                print('early stop!')\n",
    "                break  \n",
    "        self.best_round=best_round\n",
    "        self.best_score=score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c0fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=Tradaboost(N=200,base_estimator=xgb.XGBClassifier(**params), \\\n",
    "threshold=0.92975,score=roc_auc_score)\n",
    "#这里正负样本的比例决定了使用的阈值threshold，因为没有使用到class_weight='balanced'来平滑正负样本权重，因此阈值要提高到对应的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5dd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(train_A_1.values,train_B_1_valid.values,train_A_flag,train_B_1_valid_y.values,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4036266",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,estimator in enumerate(clf.estimators):    \n",
    "    print('The '+str(i+1)+' estimator:')    \n",
    "    y_pred_A=estimator.predict_proba(train_A_1.values)[:,1]    \n",
    "    y_pred_B_valid=estimator.predict_proba(train_B_1_valid.values)[:,1]    \n",
    "    y_pred_B_test=estimator.predict_proba(train_B_1_test.values)[:,1]    \n",
    "    print(f\" train AUC = {roc_auc_score(train_A_flag,y_pred_A)}\")    \n",
    "    print(f\" valid AUC = {roc_auc_score(train_B_1_valid_y,y_pred_B_valid)}\")    \n",
    "    print(f\" test AUC = {roc_auc_score(train_B_1_test_y,y_pred_B_test)}\")    \n",
    "    print('\\n')    \n",
    "    print('==============================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
