{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "560ee151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:37.627039Z",
     "start_time": "2022-03-09T05:09:36.070208Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a085a5cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T07:07:22.161455Z",
     "start_time": "2022-03-09T07:07:22.152160Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    # 数据加载部分\n",
    "    dataset = 'paws-x'\n",
    "    build_vocab = False  # 是否重构词典\n",
    "    build_wv = False  # 是否重构词向量\n",
    "    load_with_words = True  # 加载带分词的数据\n",
    "    vocab_path = 'vocab'  # 路径\n",
    "    wv_model_path = 'wv_model'  # 词向量名\n",
    "    seq_len = 27  # 句子长度\n",
    "    # 模型部分\n",
    "    model_type = 'RNN'\n",
    "    embed_dim = 100  # 词向量或embedding维度\n",
    "    vocab_size = 100  # 词典大小，读取词典后会更新\n",
    "    update_embed = True  # embedding是否随网络训练更新\n",
    "    load_model = False # 是否加载已有模型预测\n",
    "    save_model = True # 是否保存训练好的模型\n",
    "    # 训练部分\n",
    "    device = 'cpu'\n",
    "    learning_rate = 1e-3\n",
    "    batch_size = 128  # batch大小\n",
    "    epochs = 50  # 训练次数\n",
    "    print_loss = 100  # 打印loss次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf9b5fd0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:38.471861Z",
     "start_time": "2022-03-09T05:09:38.468860Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_stopwords():\n",
    "    stop_words = []\n",
    "    with open('dataprocess/baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            stop_words.append(line.replace('\\n', ''))\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18894016",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:39.019044Z",
     "start_time": "2022-03-09T05:09:39.012282Z"
    }
   },
   "outputs": [],
   "source": [
    "# jieba分词\n",
    "def cut(content, stop_words, config):\n",
    "    # 去除符号\n",
    "    content = re.sub(r\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）]\", \"\", content)\n",
    "\n",
    "    result = []\n",
    "\n",
    "    seg_list = jieba.lcut(content, cut_all=True)\n",
    "    for i in seg_list:\n",
    "        if i not in stop_words:\n",
    "            result.append(i)\n",
    "\n",
    "    if len(result) < config.seq_len:  # 小于规定长度，填充\n",
    "        new_result = ['PAD' for i in range(config.seq_len)]\n",
    "        new_result[:len(result)] = result\n",
    "        return new_result\n",
    "    else:\n",
    "        return result[:config.seq_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65fa086c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:39.348665Z",
     "start_time": "2022-03-09T05:09:39.338658Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存pickle，pickle.HIGHEST_PROTOCOL支持最高协议的对象类型以及大型对象\n",
    "def dump_pickle(obj, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "# 加载pickle\n",
    "def load_pickle(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e0e6343",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:39.754010Z",
     "start_time": "2022-03-09T05:09:39.750009Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 分词并去除停用词\n",
    "def data_anaysis(df, stop_words, config):\n",
    "    # 分词\n",
    "    df['words_a'] = df['text_a'].apply(lambda x: cut(x, stop_words, config))\n",
    "    df['words_b'] = df['text_b'].apply(lambda x: cut(x, stop_words, config))\n",
    "    return df\n",
    "\n",
    "\n",
    "# 将中文词转换为词典数字\n",
    "def word2num(content, vocab):\n",
    "    result = []\n",
    "    for word in content:\n",
    "        result.append(vocab[word])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60555b6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:40.125079Z",
     "start_time": "2022-03-09T05:09:40.110988Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构建词典\n",
    "def build_vocab(train, dev, test, config, file_name):\n",
    "    word_index = 0\n",
    "    vocab_dict = {}\n",
    "    dataset = [train, test, dev]\n",
    "    for data in tqdm(dataset, desc='构建词典'):\n",
    "        for i in range(len(data)):\n",
    "            row = data.iloc[i]\n",
    "            words_a = row['words_a']\n",
    "            words_b = row['words_b']\n",
    "            for word in words_a:\n",
    "                if word not in vocab_dict.keys():\n",
    "                    vocab_dict[word] = word_index\n",
    "                    word_index += 1\n",
    "            for word in words_b:\n",
    "                if word not in vocab_dict.keys():\n",
    "                    vocab_dict[word] = word_index\n",
    "                    word_index += 1\n",
    "    # 保存词典\n",
    "    dump_pickle(vocab_dict, config.vocab_path + '/vocab_' + file_name + '.pkl')\n",
    "    return vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0445152c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:40.562050Z",
     "start_time": "2022-03-09T05:09:40.543752Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_reader(file_name, config):\n",
    "    print('加载数据...')\n",
    "    if config.load_with_words is True:\n",
    "        train = load_pickle('data/' + file_name + '_train.pkl')\n",
    "        dev = load_pickle('data/' + file_name + '_dev.pkl')\n",
    "        test = load_pickle('data/' + file_name + '_test.pkl')\n",
    "    else:\n",
    "        # 获取停用词\n",
    "        stop_words = get_stopwords()\n",
    "        train = pd.read_csv('data/' + file_name + '/train.tsv', sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "        dev = pd.read_csv('data/' + file_name + '/dev.tsv', sep='\\t', names=['text_a', 'text_b', 'label'])\n",
    "        test = pd.read_csv('data/' + file_name + '/test.tsv', sep='\\t', names=['text_a', 'text_b'])\n",
    "\n",
    "        if len(set(train['label'])) > 2:\n",
    "            train = train[train['label'].isin(['0', '1'])]\n",
    "            train['label'] = train['label'].astype('int')\n",
    "\n",
    "        if len(set(train['label'])) > 2:\n",
    "            dev = dev[dev['label'].isin(['0', '1'])]\n",
    "            dev['label'] = dev['label'].astype('int')\n",
    "\n",
    "        test['label'] = -1\n",
    "        train = train.dropna()\n",
    "        dev = dev.dropna()\n",
    "\n",
    "        # 分词并去除停用词\n",
    "        print('分词并去除停用词...')\n",
    "        train = data_anaysis(train, stop_words, config)\n",
    "        dev = data_anaysis(dev, stop_words, config)\n",
    "        test = data_anaysis(test, stop_words, config)\n",
    "        # 保存分词后的数据\n",
    "        dump_pickle(train, 'data/' + file_name + '_train.pkl')\n",
    "        dump_pickle(train, 'data/' + file_name + '_dev.pkl')\n",
    "        dump_pickle(test, 'data/' + file_name + '_test.pkl')\n",
    "\n",
    "    # 统计词典\n",
    "    print('加载词典...')\n",
    "    if config.build_vocab is True:\n",
    "        vocab = build_vocab(train, dev, test, config, file_name)\n",
    "    else:\n",
    "        vocab = load_pickle(config.vocab_path + '/vocab_' + file_name + '.pkl')\n",
    "\n",
    "    # 将每个词转为数字\n",
    "    print('将每个词转为词典数字...')\n",
    "    train['word2num_a'] = train['words_a'].apply(lambda x: word2num(x, vocab))\n",
    "    train['word2num_b'] = train['words_b'].apply(lambda x: word2num(x, vocab))\n",
    "    dev['word2num_a'] = dev['words_a'].apply(lambda x: word2num(x, vocab))\n",
    "    dev['word2num_b'] = dev['words_b'].apply(lambda x: word2num(x, vocab))\n",
    "    test['word2num_a'] = test['words_a'].apply(lambda x: word2num(x, vocab))\n",
    "    test['word2num_b'] = test['words_b'].apply(lambda x: word2num(x, vocab))\n",
    "\n",
    "    return train, dev, test, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb538d9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:40.893489Z",
     "start_time": "2022-03-09T05:09:40.885485Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载词向量\n",
    "def load_wv(train, dev, test, config, file_name):\n",
    "    if config.build_wv is True:\n",
    "        print('构建词向量...')\n",
    "        # 若不存在词向量，则重新训练训练词向量\n",
    "        datalist = [train, dev, test]\n",
    "\n",
    "        context = []\n",
    "        for data in datalist:\n",
    "            for i in range(len(data)):\n",
    "                row = data.iloc[i]\n",
    "                context.append(row['word2num_a'])\n",
    "                context.append(row['word2num_b'])\n",
    "\n",
    "        wv_model = Word2Vec(sentences=context, vector_size=config.embed_dim, window=5, min_count=1, workers=4)\n",
    "        wv_model.train(context, total_examples=1, epochs=1)\n",
    "        wv_model.save(config.wv_model_path + '/wv_' + file_name)\n",
    "    else:\n",
    "        print('加载词向量...')\n",
    "        wv_model = Word2Vec.load(config.wv_model_path + '/wv_' + file_name)\n",
    "\n",
    "    return wv_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e827a879",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:10:58.936765Z",
     "start_time": "2022-03-09T05:10:58.922759Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "428cd7e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:10:59.293490Z",
     "start_time": "2022-03-09T05:10:59.274485Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自定义数据集\n",
    "class SimDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        super(SimDataset, self).__init__()\n",
    "        self.text_a = df['word2num_a']\n",
    "        self.text_b = df['word2num_b']\n",
    "        self.label = df['label']\n",
    "        self.len = len(df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vector_a = np.array(self.text_a.iloc[idx], dtype='int64')\n",
    "        vector_b = np.array(self.text_b.iloc[idx], dtype='int64')\n",
    "        label = np.array(self.label.iloc[idx]).astype(\"int64\")\n",
    "\n",
    "        return {'vector_a': vector_a,\n",
    "                'vector_b': vector_b,\n",
    "                'label': label}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3abe1fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:41.811944Z",
     "start_time": "2022-03-09T05:09:41.374518Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a31ebd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:41.859239Z",
     "start_time": "2022-03-09T05:09:41.845952Z"
    }
   },
   "outputs": [],
   "source": [
    "class LinModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(LinModel, self).__init__()\n",
    "\n",
    "\n",
    "        self.fc_1 = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.02)\n",
    "        )\n",
    "        self.fc_2 = nn.Sequential(\n",
    "            nn.Linear(256, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.02)\n",
    "        )\n",
    "        self.fc_3 = nn.Sequential(\n",
    "            nn.Linear(32, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.02)\n",
    "        )\n",
    "        self.fc_4 = nn.Sequential(\n",
    "            nn.Linear(4, out_features),\n",
    "        )\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.fc_1(X)\n",
    "        X = self.fc_2(X)\n",
    "        X = self.fc_3(X)\n",
    "        output = self.fc_4(X)\n",
    "\n",
    "        return self.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b4ef0ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:42.204241Z",
     "start_time": "2022-03-09T05:09:42.183762Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiamCNN(nn.Module):\n",
    "    def __init__(self, wv_mode, config):\n",
    "        super(SiamCNN, self).__init__()\n",
    "        self.device = config.device\n",
    "\n",
    "        word_vectors = torch.randn([config.vocab_size, config.embed_dim])\n",
    "        for i in range(0, config.vocab_size):\n",
    "            word_vectors[i, :] = torch.from_numpy(wv_mode.wv[i])\n",
    "        # 创建embedding层\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_vectors, freeze=config.update_embed)  # (32, 27, 100)\n",
    "        if config.update_embed is False:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.conv_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=config.seq_len, out_channels=16, kernel_size=2, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3))\n",
    "\n",
    "        self.conv_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=config.seq_len, out_channels=16, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3))\n",
    "\n",
    "        self.conv_3 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=config.seq_len, out_channels=16, kernel_size=5, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3))\n",
    "\n",
    "        self.flattern = nn.Flatten()\n",
    "\n",
    "        # 定义池化层\n",
    "        self.max_pool = nn.MaxPool1d(3)\n",
    "\n",
    "        # 定义线性层\n",
    "        self.lin_model = LinModel(1552, 2)\n",
    "\n",
    "\n",
    "    # 计算两个向量的相似度\n",
    "    def cos_sim(self, vector_a, vector_b):\n",
    "        \"\"\"\n",
    "        计算两个向量之间的余弦相似度\n",
    "        :param vector_a: 向量 a\n",
    "        :param vector_b: 向量 b\n",
    "        :return: sim\n",
    "        \"\"\"\n",
    "        return torch.tensor([torch.cosine_similarity(vector_a, vector_b, 0, 1e-8)])\n",
    "\n",
    "    def forward_one(self, text):\n",
    "        # 计算句子A\n",
    "        x = self.embedding(text)\n",
    "        conv_1 = self.conv_1(x)\n",
    "        conv_2 = self.conv_2(x)\n",
    "        conv_3 = self.conv_3(x)\n",
    "        # 合并各卷积结果取最大值\n",
    "        x = torch.cat([conv_1, conv_2, conv_3], 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.lin_model(x)\n",
    "\n",
    "    def forward(self, words_a, words_b):\n",
    "        # words_a (batch_size, seq_len)(32, 27)\n",
    "        # 计算句子A\n",
    "        x_a = self.forward_one(words_a)\n",
    "        # 计算句子B\n",
    "        x_b = self.forward_one(words_b)\n",
    "        return x_a, x_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240c80d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:42.506821Z",
     "start_time": "2022-03-09T05:09:42.490749Z"
    }
   },
   "outputs": [],
   "source": [
    "class SiamLSTM(nn.Module):\n",
    "    def __init__(self, wv_mode, config):\n",
    "        super(SiamLSTM, self).__init__()\n",
    "        self.device = config.device\n",
    "\n",
    "        word_vectors = torch.randn([config.vocab_size, config.embed_dim])\n",
    "        for i in range(0, config.vocab_size):\n",
    "            word_vectors[i, :] = torch.from_numpy(wv_mode.wv[i])\n",
    "        # 创建embedding层，因为使用的是训练好的词向量，一定要用requiresgrad来固定\n",
    "        self.embedding = nn.Embedding.from_pretrained(word_vectors, freeze=config.update_embed)  # (32, 27, 100)\n",
    "        if config.update_embed is False:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        # 创建rnn\n",
    "        self.rnn = nn.LSTM(input_size=config.embed_dim, hidden_size=10, num_layers=1)\n",
    "        # 创建线性层\n",
    "        self.lin_model = LinModel(270, 2)\n",
    "\n",
    "    def forward_one(self, text):\n",
    "        # 计算a\n",
    "        x = self.embedding(text)  # embedding转换\n",
    "        # rnn\n",
    "        x = x.transpose(0, 1)  # 交换维度，27,32,100\n",
    "        x, _ = self.rnn(x)#27,32,10，hidden_size=10\n",
    "        x = x.transpose(0, 1)  # 还原维度，32,27,10\n",
    "        x = x.contiguous().view(x.size(0), -1)#无论后面是多少，我只要前面是32，后面自然就是27*10了\n",
    "        return self.lin_model(x)\n",
    "\n",
    "    def forward(self,words_a, words_b):\n",
    "        # 计算a\n",
    "        x_a = self.forward_one(words_a) # embedding转换\n",
    "        # 计算b\n",
    "        x_b = self.forward_one(words_b)\n",
    "        return x_a, x_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2efa2b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:42.967383Z",
     "start_time": "2022-03-09T05:09:42.958295Z"
    }
   },
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = torch.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) /2 +\n",
    "        (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)/2)\n",
    "\n",
    "        y_pred = []\n",
    "        for i in euclidean_distance:\n",
    "            if i <0.5:\n",
    "                y_pred.append(0)\n",
    "            else:\n",
    "                y_pred.append(1)\n",
    "\n",
    "        return loss_contrastive, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72d3b25e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:43.491564Z",
     "start_time": "2022-03-09T05:09:43.415971Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def predict(config, model, test_df):\n",
    "    predict_labels = []\n",
    "    # 创建dataloader\n",
    "    test_dataset = SimDataset(test_df)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for mini_batch in test_dataloader:\n",
    "            mini_batch = {item: value.to(config.device) for item, value in mini_batch.items()}\n",
    "            # 获取数据\n",
    "            text_a = mini_batch['vector_a']\n",
    "            text_b = mini_batch['vector_b']\n",
    "            x_a, x_b = model(text_a, text_b)\n",
    "            euclidean_distance = torch.pairwise_distance(x_a, x_b)\n",
    "            y_pred = []\n",
    "            for i in euclidean_distance:\n",
    "                if i < 0.5:\n",
    "                    y_pred.append(0)\n",
    "                else:\n",
    "                    y_pred.append(1)\n",
    "            indices = torch.tensor(y_pred, device=config.device)\n",
    "            predict_labels += indices\n",
    "    return torch.tensor(predict_labels).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dbdeb5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:43.867904Z",
     "start_time": "2022-03-09T05:09:43.855450Z"
    }
   },
   "outputs": [],
   "source": [
    "# 校验\n",
    "def evaluation(config, model, val_dataloader, loss_fn):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    val_loss = 0.\n",
    "    # val_iterator = tqdm(val_dataloader, desc='Evaluation', total=len(val_dataloader))\n",
    "    with torch.no_grad():\n",
    "        for mini_batch in val_dataloader:\n",
    "            mini_batch = {item: value.to(config.device) for item, value in mini_batch.items()}\n",
    "            # 获取数据\n",
    "            text_a = mini_batch['vector_a']\n",
    "            text_b = mini_batch['vector_b']\n",
    "            label = mini_batch['label']\n",
    "            labels += label\n",
    "\n",
    "            x_a, x_b = model(text_a, text_b)\n",
    "\n",
    "            loss, y_pred = loss_fn(x_a, x_b, label)\n",
    "            y_pred = torch.tensor(y_pred, device=config.device)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            preds += y_pred\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_dataloader)\n",
    "    labels = torch.tensor(labels).numpy()\n",
    "    preds = torch.tensor(preds).numpy()\n",
    "    f1 = f1_score(labels, preds, average='macro')\n",
    "    # -----------new ----------------#\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    # -----------new ----------------#\n",
    "    return avg_val_loss, f1, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a597992c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T05:09:44.334174Z",
     "start_time": "2022-03-09T05:09:44.321160Z"
    }
   },
   "outputs": [],
   "source": [
    "#  训练数据\n",
    "def train(train_df, dev_df, wv_model, config):\n",
    "    # 创建数据集\n",
    "    train_dataset = SimDataset(train_df)\n",
    "    dev_dataset = SimDataset(dev_df)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=config.batch_size)\n",
    "\n",
    "    # 创建模型\n",
    "    if config.model_type == 'CNN':\n",
    "        model = SiamCNN(wv_model, config)\n",
    "    elif config.model_type == 'RNN':\n",
    "        model = SiamLSTM(wv_model, config)\n",
    "    else:\n",
    "        raise Exception('错误模型类型！')\n",
    "\n",
    "    # model = SiamCNN(wv_model, config, each_filter_num=128, filter_heights=[2, 3, 5])\n",
    "    model.to(config.device)\n",
    "    # 定义优化器\n",
    "    opt = AdamW(lr=config.learning_rate, params=model.parameters())\n",
    "    # scheduler = ExponentialLR(opt, gamma=0.9)\n",
    "    # 定义损失函数\n",
    "    loss_fn = ContrastiveLoss(1)\n",
    "\n",
    "    # 遍历epoch，开始训练\n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        for iter_id, mini_batch in enumerate(train_dataloader):\n",
    "            mini_batch = {item: value.to(config.device) for item, value in mini_batch.items()}\n",
    "            # 获取数据\n",
    "            text_a = mini_batch['vector_a']\n",
    "            text_b = mini_batch['vector_b']\n",
    "            label = mini_batch['label']\n",
    "            # 训练\n",
    "            x_a, x_b = model(text_a, text_b)\n",
    "            # 计算损失\n",
    "            loss, y_pred = loss_fn(x_a, x_b, label)\n",
    "\n",
    "            # 打印loss\n",
    "            indices = torch.tensor(y_pred, device=config.device)\n",
    "            correct = torch.sum(indices == label)\n",
    "\n",
    "            avg_loss = torch.mean(loss)\n",
    "            if iter_id % config.print_loss == 0:\n",
    "                print('epoch: {}, iter: {}, loss is: {}, acc is: {}'.format(epoch, iter_id, avg_loss,\n",
    "                                                                            correct.item() * 1.0 / len(text_a)))\n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            opt.step()\n",
    "            # scheduler.step()\n",
    "            # 清除梯度\n",
    "            model.zero_grad()\n",
    "\n",
    "        # 完成1个epoch，验证\n",
    "        avg_val_loss, f1, acc = evaluation(config, model, dev_dataloader, loss_fn)\n",
    "        print('-'*50)\n",
    "        print('epoch: {}, val_loss: {}, val_f1: {}, val_acc: {}'.format(epoch, avg_val_loss, f1,acc))\n",
    "        print('-' * 50)\n",
    "\n",
    "    return model\n",
    "\n",
    "data_list = ['paws-x']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cc79ed07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-09T07:22:20.593144Z",
     "start_time": "2022-03-09T07:07:25.458299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载数据...\n",
      "加载词典...\n",
      "将每个词转为词典数字...\n",
      "加载词向量...\n",
      "epoch: 0, iter: 0, loss is: 0.2648073136806488, acc is: 0.46875\n",
      "epoch: 0, iter: 100, loss is: 0.15295358002185822, acc is: 0.6015625\n",
      "epoch: 0, iter: 200, loss is: 0.1401657909154892, acc is: 0.6171875\n",
      "epoch: 0, iter: 300, loss is: 0.17835365235805511, acc is: 0.5078125\n",
      "--------------------------------------------------\n",
      "epoch: 0, val_loss: 0.18916058439450958, val_f1: 0.3645864254492141, val_acc: 0.5555781717519184\n",
      "--------------------------------------------------\n",
      "epoch: 1, iter: 0, loss is: 0.14154939353466034, acc is: 0.5703125\n",
      "epoch: 1, iter: 100, loss is: 0.15614436566829681, acc is: 0.546875\n",
      "epoch: 1, iter: 200, loss is: 0.14746583998203278, acc is: 0.5703125\n",
      "epoch: 1, iter: 300, loss is: 0.16400830447673798, acc is: 0.53125\n",
      "--------------------------------------------------\n",
      "epoch: 1, val_loss: 0.1946267467768242, val_f1: 0.3592157092263794, val_acc: 0.5587331311445378\n",
      "--------------------------------------------------\n",
      "epoch: 2, iter: 0, loss is: 0.15563590824604034, acc is: 0.578125\n",
      "epoch: 2, iter: 100, loss is: 0.15610697865486145, acc is: 0.5625\n",
      "epoch: 2, iter: 200, loss is: 0.1694474220275879, acc is: 0.5\n",
      "epoch: 2, iter: 300, loss is: 0.15634611248970032, acc is: 0.5390625\n",
      "--------------------------------------------------\n",
      "epoch: 2, val_loss: 0.19232219674934944, val_f1: 0.36013375344022125, val_acc: 0.5583056850332797\n",
      "--------------------------------------------------\n",
      "epoch: 3, iter: 0, loss is: 0.1314525306224823, acc is: 0.625\n",
      "epoch: 3, iter: 100, loss is: 0.15659716725349426, acc is: 0.5703125\n",
      "epoch: 3, iter: 200, loss is: 0.16347262263298035, acc is: 0.5234375\n",
      "epoch: 3, iter: 300, loss is: 0.13699544966220856, acc is: 0.578125\n",
      "--------------------------------------------------\n",
      "epoch: 3, val_loss: 0.19364940006441125, val_f1: 0.35980670503056517, val_acc: 0.5587331311445378\n",
      "--------------------------------------------------\n",
      "epoch: 4, iter: 0, loss is: 0.1649433821439743, acc is: 0.515625\n",
      "epoch: 4, iter: 100, loss is: 0.15850746631622314, acc is: 0.515625\n",
      "epoch: 4, iter: 200, loss is: 0.1596512645483017, acc is: 0.5546875\n",
      "epoch: 4, iter: 300, loss is: 0.1635778844356537, acc is: 0.5546875\n",
      "--------------------------------------------------\n",
      "epoch: 4, val_loss: 0.1950855162770798, val_f1: 0.3584376020269805, val_acc: 0.5584888762238189\n",
      "--------------------------------------------------\n",
      "epoch: 5, iter: 0, loss is: 0.1463601291179657, acc is: 0.6015625\n",
      "epoch: 5, iter: 100, loss is: 0.18167734146118164, acc is: 0.46875\n",
      "epoch: 5, iter: 200, loss is: 0.18429313600063324, acc is: 0.4921875\n",
      "epoch: 5, iter: 300, loss is: 0.13785149157047272, acc is: 0.6328125\n",
      "--------------------------------------------------\n",
      "epoch: 5, val_loss: 0.1900914489524439, val_f1: 0.3606620012227539, val_acc: 0.5582649758798266\n",
      "--------------------------------------------------\n",
      "epoch: 6, iter: 0, loss is: 0.15892592072486877, acc is: 0.5546875\n",
      "epoch: 6, iter: 100, loss is: 0.17939165234565735, acc is: 0.5234375\n",
      "epoch: 6, iter: 200, loss is: 0.15798720717430115, acc is: 0.5078125\n",
      "epoch: 6, iter: 300, loss is: 0.1514337807893753, acc is: 0.5625\n",
      "--------------------------------------------------\n",
      "epoch: 6, val_loss: 0.19182911484191814, val_f1: 0.36126055228651066, val_acc: 0.5587941948747176\n",
      "--------------------------------------------------\n",
      "epoch: 7, iter: 0, loss is: 0.166174054145813, acc is: 0.546875\n",
      "epoch: 7, iter: 100, loss is: 0.16958186030387878, acc is: 0.4921875\n",
      "epoch: 7, iter: 200, loss is: 0.15848872065544128, acc is: 0.53125\n",
      "epoch: 7, iter: 300, loss is: 0.14783114194869995, acc is: 0.515625\n",
      "--------------------------------------------------\n",
      "epoch: 7, val_loss: 0.19170012957571694, val_f1: 0.3616465353284886, val_acc: 0.558916322335077\n",
      "--------------------------------------------------\n",
      "epoch: 8, iter: 0, loss is: 0.17812205851078033, acc is: 0.5\n",
      "epoch: 8, iter: 100, loss is: 0.16833005845546722, acc is: 0.5078125\n",
      "epoch: 8, iter: 200, loss is: 0.16298262774944305, acc is: 0.5546875\n",
      "epoch: 8, iter: 300, loss is: 0.15431009232997894, acc is: 0.578125\n",
      "--------------------------------------------------\n",
      "epoch: 8, val_loss: 0.19034197305639586, val_f1: 0.36264902636643087, val_acc: 0.5590180952187099\n",
      "--------------------------------------------------\n",
      "epoch: 9, iter: 0, loss is: 0.15670883655548096, acc is: 0.53125\n",
      "epoch: 9, iter: 100, loss is: 0.16815383732318878, acc is: 0.578125\n",
      "epoch: 9, iter: 200, loss is: 0.1589696854352951, acc is: 0.5859375\n",
      "epoch: 9, iter: 300, loss is: 0.1625882387161255, acc is: 0.5234375\n",
      "--------------------------------------------------\n",
      "epoch: 9, val_loss: 0.19054374816672257, val_f1: 0.36381884641511436, val_acc: 0.5594251867532415\n",
      "--------------------------------------------------\n",
      "epoch: 10, iter: 0, loss is: 0.16275128722190857, acc is: 0.4921875\n",
      "epoch: 10, iter: 100, loss is: 0.1537497490644455, acc is: 0.53125\n",
      "epoch: 10, iter: 200, loss is: 0.1688937246799469, acc is: 0.53125\n",
      "epoch: 10, iter: 300, loss is: 0.18040503561496735, acc is: 0.4609375\n",
      "--------------------------------------------------\n",
      "epoch: 10, val_loss: 0.18882950375943133, val_f1: 0.36516153811465724, val_acc: 0.5595676687903275\n",
      "--------------------------------------------------\n",
      "epoch: 11, iter: 0, loss is: 0.17190946638584137, acc is: 0.546875\n",
      "epoch: 11, iter: 100, loss is: 0.15753844380378723, acc is: 0.546875\n",
      "epoch: 11, iter: 200, loss is: 0.15604501962661743, acc is: 0.5390625\n",
      "epoch: 11, iter: 300, loss is: 0.17025764286518097, acc is: 0.515625\n",
      "--------------------------------------------------\n",
      "epoch: 11, val_loss: 0.18925370027621588, val_f1: 0.36571430375907465, val_acc: 0.5596083779437807\n",
      "--------------------------------------------------\n",
      "epoch: 12, iter: 0, loss is: 0.1889915019273758, acc is: 0.453125\n",
      "epoch: 12, iter: 100, loss is: 0.1682552993297577, acc is: 0.53125\n",
      "epoch: 12, iter: 200, loss is: 0.15865904092788696, acc is: 0.5546875\n",
      "epoch: 12, iter: 300, loss is: 0.17454269528388977, acc is: 0.5\n",
      "--------------------------------------------------\n",
      "epoch: 12, val_loss: 0.18603692467634877, val_f1: 0.3690574084452052, val_acc: 0.559974760324859\n",
      "--------------------------------------------------\n",
      "epoch: 13, iter: 0, loss is: 0.1562240570783615, acc is: 0.5234375\n",
      "epoch: 13, iter: 100, loss is: 0.15439847111701965, acc is: 0.5625\n",
      "epoch: 13, iter: 200, loss is: 0.13943813741207123, acc is: 0.6015625\n",
      "epoch: 13, iter: 300, loss is: 0.16603028774261475, acc is: 0.4921875\n",
      "--------------------------------------------------\n",
      "epoch: 13, val_loss: 0.18563250401833406, val_f1: 0.36959591293652594, val_acc: 0.5604632701662969\n",
      "--------------------------------------------------\n",
      "epoch: 14, iter: 0, loss is: 0.16869230568408966, acc is: 0.53125\n",
      "epoch: 14, iter: 100, loss is: 0.17795193195343018, acc is: 0.4921875\n",
      "epoch: 14, iter: 200, loss is: 0.1658487468957901, acc is: 0.53125\n",
      "epoch: 14, iter: 300, loss is: 0.14252980053424835, acc is: 0.53125\n",
      "--------------------------------------------------\n",
      "epoch: 14, val_loss: 0.18294686928857118, val_f1: 0.37677131949445436, val_acc: 0.5606668159335627\n",
      "--------------------------------------------------\n",
      "epoch: 15, iter: 0, loss is: 0.14634475111961365, acc is: 0.578125\n",
      "epoch: 15, iter: 100, loss is: 0.12371876835823059, acc is: 0.625\n",
      "epoch: 15, iter: 200, loss is: 0.14927871525287628, acc is: 0.53125\n",
      "epoch: 15, iter: 300, loss is: 0.15709486603736877, acc is: 0.5625\n",
      "--------------------------------------------------\n",
      "epoch: 15, val_loss: 0.1857434849953279, val_f1: 0.371560886329881, val_acc: 0.5610739074680942\n",
      "--------------------------------------------------\n",
      "epoch: 16, iter: 0, loss is: 0.13591571152210236, acc is: 0.5859375\n",
      "epoch: 16, iter: 100, loss is: 0.1518123894929886, acc is: 0.5703125\n",
      "epoch: 16, iter: 200, loss is: 0.16660627722740173, acc is: 0.5\n",
      "epoch: 16, iter: 300, loss is: 0.16198836266994476, acc is: 0.546875\n",
      "--------------------------------------------------\n",
      "epoch: 16, val_loss: 0.18534854081614563, val_f1: 0.37231090488909446, val_acc: 0.5611349711982739\n",
      "--------------------------------------------------\n",
      "epoch: 17, iter: 0, loss is: 0.1559356302022934, acc is: 0.5703125\n",
      "epoch: 17, iter: 100, loss is: 0.17275714874267578, acc is: 0.53125\n",
      "epoch: 17, iter: 200, loss is: 0.1652085781097412, acc is: 0.5078125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17, iter: 300, loss is: 0.16025857627391815, acc is: 0.578125\n",
      "--------------------------------------------------\n",
      "epoch: 17, val_loss: 0.18714208947494626, val_f1: 0.37091617131153615, val_acc: 0.5610739074680942\n",
      "--------------------------------------------------\n",
      "epoch: 18, iter: 0, loss is: 0.152981698513031, acc is: 0.546875\n",
      "epoch: 18, iter: 100, loss is: 0.1308722198009491, acc is: 0.6171875\n",
      "epoch: 18, iter: 200, loss is: 0.15195874869823456, acc is: 0.53125\n",
      "epoch: 18, iter: 300, loss is: 0.12697762250900269, acc is: 0.6328125\n",
      "--------------------------------------------------\n",
      "epoch: 18, val_loss: 0.18397610653967908, val_f1: 0.3782008385494886, val_acc: 0.5608092979706487\n",
      "--------------------------------------------------\n",
      "epoch: 19, iter: 0, loss is: 0.1607210338115692, acc is: 0.515625\n",
      "epoch: 19, iter: 100, loss is: 0.1685093492269516, acc is: 0.5234375\n",
      "epoch: 19, iter: 200, loss is: 0.14976969361305237, acc is: 0.5234375\n",
      "epoch: 19, iter: 300, loss is: 0.18039552867412567, acc is: 0.5234375\n",
      "--------------------------------------------------\n",
      "epoch: 19, val_loss: 0.1840658705914393, val_f1: 0.37881411813540994, val_acc: 0.5619695088440636\n",
      "--------------------------------------------------\n",
      "epoch: 20, iter: 0, loss is: 0.14097057282924652, acc is: 0.6015625\n",
      "epoch: 20, iter: 100, loss is: 0.14722712337970734, acc is: 0.5390625\n",
      "epoch: 20, iter: 200, loss is: 0.1387978047132492, acc is: 0.625\n",
      "epoch: 20, iter: 300, loss is: 0.15359532833099365, acc is: 0.5703125\n",
      "--------------------------------------------------\n",
      "epoch: 20, val_loss: 0.1827814336090038, val_f1: 0.38046837793681587, val_acc: 0.5623562458018686\n",
      "--------------------------------------------------\n",
      "epoch: 21, iter: 0, loss is: 0.13700194656848907, acc is: 0.578125\n",
      "epoch: 21, iter: 100, loss is: 0.15208613872528076, acc is: 0.5625\n",
      "epoch: 21, iter: 200, loss is: 0.151350736618042, acc is: 0.578125\n",
      "epoch: 21, iter: 300, loss is: 0.1633269041776657, acc is: 0.5078125\n",
      "--------------------------------------------------\n",
      "epoch: 21, val_loss: 0.18038419471122324, val_f1: 0.38873069770406854, val_acc: 0.5635978749821897\n",
      "--------------------------------------------------\n",
      "epoch: 22, iter: 0, loss is: 0.16336296498775482, acc is: 0.5703125\n",
      "epoch: 22, iter: 100, loss is: 0.19812411069869995, acc is: 0.4453125\n",
      "epoch: 22, iter: 200, loss is: 0.13770270347595215, acc is: 0.6015625\n",
      "epoch: 22, iter: 300, loss is: 0.16019326448440552, acc is: 0.53125\n",
      "--------------------------------------------------\n",
      "epoch: 22, val_loss: 0.18040161676860103, val_f1: 0.38635845028104704, val_acc: 0.5636589387123695\n",
      "--------------------------------------------------\n",
      "epoch: 23, iter: 0, loss is: 0.14922668039798737, acc is: 0.53125\n",
      "epoch: 23, iter: 100, loss is: 0.1606108397245407, acc is: 0.5390625\n",
      "epoch: 23, iter: 200, loss is: 0.14835357666015625, acc is: 0.578125\n",
      "epoch: 23, iter: 300, loss is: 0.165121391415596, acc is: 0.5390625\n",
      "--------------------------------------------------\n",
      "epoch: 23, val_loss: 0.17880085875124982, val_f1: 0.39078648210805295, val_acc: 0.5643102851676199\n",
      "--------------------------------------------------\n",
      "epoch: 24, iter: 0, loss is: 0.1591891199350357, acc is: 0.5078125\n",
      "epoch: 24, iter: 100, loss is: 0.14920738339424133, acc is: 0.515625\n",
      "epoch: 24, iter: 200, loss is: 0.15838487446308136, acc is: 0.53125\n",
      "epoch: 24, iter: 300, loss is: 0.13451126217842102, acc is: 0.546875\n",
      "--------------------------------------------------\n",
      "epoch: 24, val_loss: 0.17913354304619133, val_f1: 0.3906843527679596, val_acc: 0.5632722017545645\n",
      "--------------------------------------------------\n",
      "epoch: 25, iter: 0, loss is: 0.1431221067905426, acc is: 0.5859375\n",
      "epoch: 25, iter: 100, loss is: 0.16452248394489288, acc is: 0.5234375\n",
      "epoch: 25, iter: 200, loss is: 0.17511892318725586, acc is: 0.515625\n",
      "epoch: 25, iter: 300, loss is: 0.11970168352127075, acc is: 0.6796875\n",
      "--------------------------------------------------\n",
      "epoch: 25, val_loss: 0.18118885702763995, val_f1: 0.3894030866432754, val_acc: 0.563679293289096\n",
      "--------------------------------------------------\n",
      "epoch: 26, iter: 0, loss is: 0.17498177289962769, acc is: 0.53125\n",
      "epoch: 26, iter: 100, loss is: 0.15754787623882294, acc is: 0.578125\n",
      "epoch: 26, iter: 200, loss is: 0.1629052758216858, acc is: 0.5546875\n",
      "epoch: 26, iter: 300, loss is: 0.18264907598495483, acc is: 0.4765625\n",
      "--------------------------------------------------\n",
      "epoch: 26, val_loss: 0.1759424969786778, val_f1: 0.40890792880101307, val_acc: 0.5656129780781208\n",
      "--------------------------------------------------\n",
      "epoch: 27, iter: 0, loss is: 0.15487337112426758, acc is: 0.578125\n",
      "epoch: 27, iter: 100, loss is: 0.17587411403656006, acc is: 0.484375\n",
      "epoch: 27, iter: 200, loss is: 0.15328539907932281, acc is: 0.5546875\n",
      "epoch: 27, iter: 300, loss is: 0.1656176894903183, acc is: 0.5546875\n",
      "--------------------------------------------------\n",
      "epoch: 27, val_loss: 0.17817613660978773, val_f1: 0.39947217241696653, val_acc: 0.5658572329988398\n",
      "--------------------------------------------------\n",
      "epoch: 28, iter: 0, loss is: 0.16229689121246338, acc is: 0.5625\n",
      "epoch: 28, iter: 100, loss is: 0.1761951446533203, acc is: 0.5078125\n",
      "epoch: 28, iter: 200, loss is: 0.1808469295501709, acc is: 0.5234375\n",
      "epoch: 28, iter: 300, loss is: 0.15109319984912872, acc is: 0.5703125\n",
      "--------------------------------------------------\n",
      "epoch: 28, val_loss: 0.17592949990648776, val_f1: 0.41082667201462864, val_acc: 0.5659997150359258\n",
      "--------------------------------------------------\n",
      "epoch: 29, iter: 0, loss is: 0.13345244526863098, acc is: 0.65625\n",
      "epoch: 29, iter: 100, loss is: 0.16143187880516052, acc is: 0.5625\n",
      "epoch: 29, iter: 200, loss is: 0.15039750933647156, acc is: 0.578125\n",
      "epoch: 29, iter: 300, loss is: 0.17825844883918762, acc is: 0.4765625\n",
      "--------------------------------------------------\n",
      "epoch: 29, val_loss: 0.17634501599241048, val_f1: 0.4014379716095716, val_acc: 0.5663050336868245\n",
      "--------------------------------------------------\n",
      "epoch: 30, iter: 0, loss is: 0.13142158091068268, acc is: 0.59375\n",
      "epoch: 30, iter: 100, loss is: 0.1545484960079193, acc is: 0.53125\n",
      "epoch: 30, iter: 200, loss is: 0.17780523002147675, acc is: 0.5078125\n",
      "epoch: 30, iter: 300, loss is: 0.1707087755203247, acc is: 0.515625\n",
      "--------------------------------------------------\n",
      "epoch: 30, val_loss: 0.17544444621307775, val_f1: 0.4088713669861626, val_acc: 0.5679741089784038\n",
      "--------------------------------------------------\n",
      "epoch: 31, iter: 0, loss is: 0.1484777331352234, acc is: 0.5703125\n",
      "epoch: 31, iter: 100, loss is: 0.16098511219024658, acc is: 0.546875\n",
      "epoch: 31, iter: 200, loss is: 0.14974422752857208, acc is: 0.53125\n",
      "epoch: 31, iter: 300, loss is: 0.14363324642181396, acc is: 0.6015625\n",
      "--------------------------------------------------\n",
      "epoch: 31, val_loss: 0.17567529319785535, val_f1: 0.4066073300820747, val_acc: 0.5681165910154898\n",
      "--------------------------------------------------\n",
      "epoch: 32, iter: 0, loss is: 0.1388154923915863, acc is: 0.5625\n",
      "epoch: 32, iter: 100, loss is: 0.165517196059227, acc is: 0.515625\n",
      "epoch: 32, iter: 200, loss is: 0.15302494168281555, acc is: 0.53125\n",
      "epoch: 32, iter: 300, loss is: 0.1514403373003006, acc is: 0.59375\n",
      "--------------------------------------------------\n",
      "epoch: 32, val_loss: 0.17480251662588367, val_f1: 0.4113408149458033, val_acc: 0.5677094994809583\n",
      "--------------------------------------------------\n",
      "epoch: 33, iter: 0, loss is: 0.17572541534900665, acc is: 0.515625\n",
      "epoch: 33, iter: 100, loss is: 0.1587417721748352, acc is: 0.546875\n",
      "epoch: 33, iter: 200, loss is: 0.15755470097064972, acc is: 0.5703125\n",
      "epoch: 33, iter: 300, loss is: 0.1728830635547638, acc is: 0.5078125\n",
      "--------------------------------------------------\n",
      "epoch: 33, val_loss: 0.1734983150769646, val_f1: 0.41603993495150393, val_acc: 0.5688290012009201\n",
      "--------------------------------------------------\n",
      "epoch: 34, iter: 0, loss is: 0.1532454490661621, acc is: 0.5625\n",
      "epoch: 34, iter: 100, loss is: 0.15110474824905396, acc is: 0.5390625\n",
      "epoch: 34, iter: 200, loss is: 0.15925438702106476, acc is: 0.5390625\n",
      "epoch: 34, iter: 300, loss is: 0.1593794822692871, acc is: 0.59375\n",
      "--------------------------------------------------\n",
      "epoch: 34, val_loss: 0.17328498106993115, val_f1: 0.4181615662127436, val_acc: 0.5691139652750921\n",
      "--------------------------------------------------\n",
      "epoch: 35, iter: 0, loss is: 0.1653209626674652, acc is: 0.53125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 35, iter: 100, loss is: 0.1554323136806488, acc is: 0.5390625\n",
      "epoch: 35, iter: 200, loss is: 0.16301649808883667, acc is: 0.5546875\n",
      "epoch: 35, iter: 300, loss is: 0.17019802331924438, acc is: 0.515625\n",
      "--------------------------------------------------\n",
      "epoch: 35, val_loss: 0.17467826531113437, val_f1: 0.4052981296877449, val_acc: 0.5683404913594822\n",
      "--------------------------------------------------\n",
      "epoch: 36, iter: 0, loss is: 0.1438448131084442, acc is: 0.5859375\n",
      "epoch: 36, iter: 100, loss is: 0.13497211039066315, acc is: 0.6171875\n",
      "epoch: 36, iter: 200, loss is: 0.17101073265075684, acc is: 0.5390625\n",
      "epoch: 36, iter: 300, loss is: 0.11851043254137039, acc is: 0.6640625\n",
      "--------------------------------------------------\n",
      "epoch: 36, val_loss: 0.17362589132972062, val_f1: 0.4209836730124235, val_acc: 0.5698263754605223\n",
      "--------------------------------------------------\n",
      "epoch: 37, iter: 0, loss is: 0.12854553759098053, acc is: 0.6640625\n",
      "epoch: 37, iter: 100, loss is: 0.16741801798343658, acc is: 0.5\n",
      "epoch: 37, iter: 200, loss is: 0.1339801400899887, acc is: 0.6328125\n",
      "epoch: 37, iter: 300, loss is: 0.16498851776123047, acc is: 0.515625\n",
      "--------------------------------------------------\n",
      "epoch: 37, val_loss: 0.17152214044472203, val_f1: 0.42726895205883486, val_acc: 0.5703759490321398\n",
      "--------------------------------------------------\n",
      "epoch: 38, iter: 0, loss is: 0.1373629868030548, acc is: 0.59375\n",
      "epoch: 38, iter: 100, loss is: 0.15490004420280457, acc is: 0.5546875\n",
      "epoch: 38, iter: 200, loss is: 0.13767442107200623, acc is: 0.578125\n",
      "epoch: 38, iter: 300, loss is: 0.1446220725774765, acc is: 0.6171875\n",
      "--------------------------------------------------\n",
      "epoch: 38, val_loss: 0.17881149627889195, val_f1: 0.40593564861724146, val_acc: 0.5665696431842699\n",
      "--------------------------------------------------\n",
      "epoch: 39, iter: 0, loss is: 0.16886956989765167, acc is: 0.53125\n",
      "epoch: 39, iter: 100, loss is: 0.16989019513130188, acc is: 0.5234375\n",
      "epoch: 39, iter: 200, loss is: 0.15085244178771973, acc is: 0.53125\n",
      "epoch: 39, iter: 300, loss is: 0.16264864802360535, acc is: 0.5234375\n",
      "--------------------------------------------------\n",
      "epoch: 39, val_loss: 0.17082684634563824, val_f1: 0.43235652104315003, val_acc: 0.5705998493761322\n",
      "--------------------------------------------------\n",
      "epoch: 40, iter: 0, loss is: 0.1399654895067215, acc is: 0.609375\n",
      "epoch: 40, iter: 100, loss is: 0.14677076041698456, acc is: 0.6015625\n",
      "epoch: 40, iter: 200, loss is: 0.1657596081495285, acc is: 0.5234375\n",
      "epoch: 40, iter: 300, loss is: 0.1794540286064148, acc is: 0.4921875\n",
      "--------------------------------------------------\n",
      "epoch: 40, val_loss: 0.17140767546758676, val_f1: 0.42696006379314033, val_acc: 0.5723096338211647\n",
      "--------------------------------------------------\n",
      "epoch: 41, iter: 0, loss is: 0.14832034707069397, acc is: 0.5703125\n",
      "epoch: 41, iter: 100, loss is: 0.15758994221687317, acc is: 0.5390625\n",
      "epoch: 41, iter: 200, loss is: 0.1548323631286621, acc is: 0.5625\n",
      "epoch: 41, iter: 300, loss is: 0.1620682030916214, acc is: 0.5703125\n",
      "--------------------------------------------------\n",
      "epoch: 41, val_loss: 0.1692732792774526, val_f1: 0.44373601967943116, val_acc: 0.5736123267316656\n",
      "--------------------------------------------------\n",
      "epoch: 42, iter: 0, loss is: 0.14843876659870148, acc is: 0.5859375\n",
      "epoch: 42, iter: 100, loss is: 0.14698426425457, acc is: 0.6015625\n",
      "epoch: 42, iter: 200, loss is: 0.16128836572170258, acc is: 0.515625\n",
      "epoch: 42, iter: 300, loss is: 0.16386613249778748, acc is: 0.546875\n",
      "--------------------------------------------------\n",
      "epoch: 42, val_loss: 0.17079587451492748, val_f1: 0.4286422026791297, val_acc: 0.5732255897738606\n",
      "--------------------------------------------------\n",
      "epoch: 43, iter: 0, loss is: 0.12371309101581573, acc is: 0.6484375\n",
      "epoch: 43, iter: 100, loss is: 0.17361323535442352, acc is: 0.4921875\n",
      "epoch: 43, iter: 200, loss is: 0.17948152124881744, acc is: 0.5\n",
      "epoch: 43, iter: 300, loss is: 0.1617276668548584, acc is: 0.5703125\n",
      "--------------------------------------------------\n",
      "epoch: 43, val_loss: 0.1684392586660882, val_f1: 0.44497527622786126, val_acc: 0.5732662989273138\n",
      "--------------------------------------------------\n",
      "epoch: 44, iter: 0, loss is: 0.14643354713916779, acc is: 0.5859375\n",
      "epoch: 44, iter: 100, loss is: 0.1519142985343933, acc is: 0.59375\n",
      "epoch: 44, iter: 200, loss is: 0.13157160580158234, acc is: 0.6640625\n",
      "epoch: 44, iter: 300, loss is: 0.1693677455186844, acc is: 0.4921875\n",
      "--------------------------------------------------\n",
      "epoch: 44, val_loss: 0.16855619471364966, val_f1: 0.44090529773228504, val_acc: 0.5740397728429237\n",
      "--------------------------------------------------\n",
      "epoch: 45, iter: 0, loss is: 0.1542215645313263, acc is: 0.5703125\n",
      "epoch: 45, iter: 100, loss is: 0.15752486884593964, acc is: 0.546875\n",
      "epoch: 45, iter: 200, loss is: 0.16185176372528076, acc is: 0.5390625\n",
      "epoch: 45, iter: 300, loss is: 0.16566933691501617, acc is: 0.5\n",
      "--------------------------------------------------\n",
      "epoch: 45, val_loss: 0.17095456978616616, val_f1: 0.4305121172639033, val_acc: 0.5737140996152985\n",
      "--------------------------------------------------\n",
      "epoch: 46, iter: 0, loss is: 0.15557080507278442, acc is: 0.5703125\n",
      "epoch: 46, iter: 100, loss is: 0.13480563461780548, acc is: 0.640625\n",
      "epoch: 46, iter: 200, loss is: 0.15865617990493774, acc is: 0.5234375\n",
      "epoch: 46, iter: 300, loss is: 0.1594020426273346, acc is: 0.53125\n",
      "--------------------------------------------------\n",
      "epoch: 46, val_loss: 0.16896421409910545, val_f1: 0.43858091073408634, val_acc: 0.5752406928697917\n",
      "--------------------------------------------------\n",
      "epoch: 47, iter: 0, loss is: 0.139947310090065, acc is: 0.59375\n",
      "epoch: 47, iter: 100, loss is: 0.15510781109333038, acc is: 0.5390625\n",
      "epoch: 47, iter: 200, loss is: 0.14592668414115906, acc is: 0.5625\n",
      "epoch: 47, iter: 300, loss is: 0.1637255996465683, acc is: 0.53125\n",
      "--------------------------------------------------\n",
      "epoch: 47, val_loss: 0.16722633315172666, val_f1: 0.4515856024192214, val_acc: 0.5770318956217305\n",
      "--------------------------------------------------\n",
      "epoch: 48, iter: 0, loss is: 0.126916766166687, acc is: 0.625\n",
      "epoch: 48, iter: 100, loss is: 0.14009077847003937, acc is: 0.578125\n",
      "epoch: 48, iter: 200, loss is: 0.16205108165740967, acc is: 0.53125\n",
      "epoch: 48, iter: 300, loss is: 0.1617945283651352, acc is: 0.5234375\n",
      "--------------------------------------------------\n",
      "epoch: 48, val_loss: 0.16844961425522342, val_f1: 0.43961776192351376, val_acc: 0.5765433857802927\n",
      "--------------------------------------------------\n",
      "epoch: 49, iter: 0, loss is: 0.1549045294523239, acc is: 0.5234375\n",
      "epoch: 49, iter: 100, loss is: 0.16296973824501038, acc is: 0.5546875\n",
      "epoch: 49, iter: 200, loss is: 0.1297561377286911, acc is: 0.578125\n",
      "epoch: 49, iter: 300, loss is: 0.1768200397491455, acc is: 0.4765625\n",
      "--------------------------------------------------\n",
      "epoch: 49, val_loss: 0.16698355708892146, val_f1: 0.4515959028927908, val_acc: 0.576869059007918\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'result/paws-x.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-d00ad2a4e8a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'result/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'.tsv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'prediction'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\t'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'result/paws-x.tsv'"
     ]
    }
   ],
   "source": [
    "for data in data_list:\n",
    "    # 配置文件\n",
    "    conf = Config()\n",
    "    conf.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    conf.dataset = data\n",
    "    # 读取数据\n",
    "    train_df, dev_df, test_df, vocab = data_reader(conf.dataset, conf)\n",
    "\n",
    "\n",
    "    if conf.load_model is False:\n",
    "        # 加载测向量\n",
    "        wv_model = load_wv(train_df, dev_df, test_df, conf, conf.dataset)\n",
    "        # 更新词典大小\n",
    "        conf.vocab_size = len(vocab)\n",
    "        # 训练模型\n",
    "        model = train(train_df, dev_df, wv_model, conf)\n",
    "        if conf.save_model is True:\n",
    "            torch.save(model, 'model/'+conf.dataset+'.pkl')\n",
    "    else:\n",
    "        model = torch.load('model/'+conf.dataset+'.pkl')\n",
    "\n",
    "\n",
    "    # 推理模型\n",
    "    predict_labels = predict(conf, model, test_df)\n",
    "    # 保存结果\n",
    "    test_df['index'] = test_df.index\n",
    "    test_df['prediction'] = predict_labels\n",
    "    test_df.to_csv('result/'+conf.dataset+'.tsv', index=False, columns=['index','prediction'], sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507093cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
