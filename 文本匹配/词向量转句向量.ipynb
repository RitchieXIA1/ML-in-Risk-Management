{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b7d148a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:04:40.438276Z",
     "start_time": "2022-03-08T08:04:40.433151Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jieba\n",
    "import textdistance as td\n",
    "from tqdm import tqdm\n",
    "from gensim import corpora,models,similarities\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec,TfidfModel\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb339318",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T07:17:51.922456Z",
     "start_time": "2022-03-08T07:17:51.906076Z"
    }
   },
   "outputs": [],
   "source": [
    "#文本处理，有些可能用不到\n",
    "import re\n",
    "import string\n",
    "import jieba\n",
    "with open(\"baidu_stopwords.txt\",encoding=\"utf-8\") as f:\n",
    "    stopword_list=f.readlines()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens=jieba.cut(text)\n",
    "    tokens=[token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    pattern=re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens=filter(None,[pattern.sub('',token) for token in tokens])\n",
    "    filtered_text=''.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "#去除停用词\n",
    "def remove_stopwords(text):\n",
    "    tokens=tokenize_text(text)\n",
    "    filtered_tokens=[token for token in tokens if token not in stopword_list]\n",
    "    filtered_text=''.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "def normalize_corpus(corpus,tokenize=False):\n",
    "    normalize_corpus=[]\n",
    "    for text in corpus:\n",
    "        text=remove_special_characters(text)\n",
    "        text=remove_stopwords(text)\n",
    "        if tokenize:\n",
    "            normalize_corpus.append(tokenize_text(text))\n",
    "        else:\n",
    "            normalize_corpus.append(text)\n",
    "    return normalize_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29ff79a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T07:18:03.575686Z",
     "start_time": "2022-03-08T07:18:03.428622Z"
    }
   },
   "outputs": [],
   "source": [
    "train_raw = pd.read_csv('paws-x-zh/paws-x-zh/train.tsv', sep='\\t',names=['text_a', 'text_b', 'label'])\n",
    "test_raw = pd.read_csv('paws-x-zh/paws-x-zh/test.tsv', sep='\\t',names=['text_a', 'text_b', 'label'])\n",
    "test_raw['label'] = -1\n",
    "train_raw = train_raw.dropna()\n",
    "test_raw = test_raw.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02682bdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T07:20:09.611493Z",
     "start_time": "2022-03-08T07:20:09.510438Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('./word2vec_data/corpus.txt', 'w',encoding='utf-8')\n",
    "for i in tokenized_corpus:\n",
    "    f.write(i + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e5dc984",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:10:49.934383Z",
     "start_time": "2022-03-08T08:10:42.146044Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_word2vec():\n",
    "    corpus=open('./word2vec_data/corpus.txt', 'r',encoding='utf-8')\n",
    "    model = Word2Vec(LineSentence(corpus), sg=0,vector_size=100, \n",
    "                     window=5, min_count=1, workers=8)\n",
    "    model.save('./word2vec_data/text_similarity.word2vec')\n",
    "\n",
    "train_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05a44e89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:10:50.592358Z",
     "start_time": "2022-03-08T08:10:50.279288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3250252\n",
      "0.11587919\n"
     ]
    }
   ],
   "source": [
    "model=Word2Vec.load('./word2vec_data/text_similarity.word2vec')\n",
    "print(model.wv.similarity('加州','美国'))\n",
    "print(model.wv.similarity('项目','第四季'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2246a8ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:26:27.020901Z",
     "start_time": "2022-03-08T08:26:27.006268Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 读取数据\n",
    "def get_stopwords():\n",
    "    stop_words = []\n",
    "    with open('baidu_stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            stop_words.append(line.replace('\\n', ''))\n",
    "    return stop_words\n",
    "\n",
    "# jieba分词\n",
    "def cut(content, stop_words):\n",
    "    # 去除符号\n",
    "    content = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\']+|[+——！，。？、~@#￥%……&*（）·]\", \"\",content)\n",
    "    \n",
    "    result = []\n",
    "    try:\n",
    "        seg_list = jieba.lcut(content)\n",
    "        for i in seg_list:\n",
    "            if i not in stop_words:\n",
    "                result.append(i)\n",
    "        \n",
    "    except AttributeError as ex:\n",
    "        print(content)\n",
    "        raise ex\n",
    "    return result\n",
    "\n",
    "def data_cut(df, stop_words):\n",
    "    df['text_a'] = df['text_a'].apply(lambda x: remove_special_characters(x))\n",
    "    df['text_b'] = df['text_b'].apply(lambda x: remove_special_characters(x))\n",
    "    df['text_a'] = df['text_a'].apply(lambda x: remove_stopwords(x))\n",
    "    df['text_b'] = df['text_b'].apply(lambda x: remove_stopwords(x))\n",
    "    # 分词\n",
    "    df['words_a'] = df['text_a'].apply(lambda x: cut(x, stop_words))\n",
    "    df['words_b'] = df['text_b'].apply(lambda x: cut(x, stop_words))\n",
    "    return df\n",
    "# 获取停用词\n",
    "stop_words = get_stopwords()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f7e18eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:27:46.168692Z",
     "start_time": "2022-03-08T08:26:27.677695Z"
    }
   },
   "outputs": [],
   "source": [
    "train = data_cut(train_raw, stop_words)\n",
    "test = data_cut(test_raw, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2c2225f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:31:05.454145Z",
     "start_time": "2022-03-08T08:30:53.611435Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 49129/49129 [00:03<00:00, 13904.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 13982.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# 训练词向量\n",
    "context = []\n",
    "for i in tqdm(range(len(train))):\n",
    "    row = train.iloc[i]\n",
    "    context.append(row['words_a'])\n",
    "    context.append(row['words_b'])\n",
    "for i in tqdm(range(len(test))):\n",
    "    row = test.iloc[i]\n",
    "    context.append(row['words_a'])\n",
    "    context.append(row['words_b'])\n",
    "\n",
    "wv_model = Word2Vec(sentences=context, vector_size=100, window=5, min_count=1, workers=8)\n",
    "wv_model.train(context, total_examples=1, epochs=5)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0ad94ee4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:31:31.898015Z",
     "start_time": "2022-03-08T08:31:24.341551Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 49129/49129 [00:07<00:00, 6982.89it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 6750.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 45002/45002 [00:00<00:00, 1363431.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# 统计全文的count\n",
    "count_list = []\n",
    "words_num = 0\n",
    "for i in tqdm(range(len(train))):\n",
    "    count_list += list(set(train.iloc[i]['words_a']))\n",
    "    count_list += list(set(train.iloc[i]['words_b']))\n",
    "    \n",
    "    words_num +=2\n",
    "    \n",
    "for i in tqdm(range(len(test))):\n",
    "    count_list += list(set(test.iloc[i]['words_a']))\n",
    "    count_list += list(set(test.iloc[i]['words_b']))\n",
    "    \n",
    "    words_num +=2\n",
    "#计算每一个元素出现的次数    \n",
    "count = Counter(count_list)\n",
    "# 计算idf列表\n",
    "idf = {}\n",
    "for k, v in tqdm(dict(count).items()):\n",
    "    idf[k] = math.log(words_num/(v+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ca38990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:09:38.854797Z",
     "start_time": "2022-03-08T08:09:38.834793Z"
    }
   },
   "outputs": [],
   "source": [
    "# 转换句向量\n",
    "def text_to_wv(model, data, operation='max_pooling',key='wv'):\n",
    "\n",
    "    full_wv_a = []\n",
    "    full_wv_b = []\n",
    "    # 每句话转词向量表达\n",
    "    for i in tqdm(range(len(data))):\n",
    "        row = data.iloc[i]\n",
    "\n",
    "        wv_a = []\n",
    "        words_a = row['words_a']\n",
    "     \n",
    "        for i in words_a:\n",
    "            wv_a.append(model.wv[i])\n",
    "        if operation == 'max_pooling':\n",
    "            full_wv_a.append(np.amax(wv_a, axis=0))\n",
    "        elif operation == 'mean_pooling':\n",
    "            full_wv_a.append(np.mean(wv_a, axis=0))\n",
    "            \n",
    "            \n",
    "        wv_b = []\n",
    "        words_b = row['words_b']\n",
    "        \n",
    "        for i in words_b:\n",
    "            wv_b.append(model.wv[i])\n",
    "        if operation == 'max_pooling':\n",
    "            full_wv_b.append(np.amax(wv_b, axis=0))\n",
    "        elif operation == 'mean_pooling':\n",
    "            full_wv_b.append(np.mean(wv_b, axis=0))\n",
    "    data[key + '_a'] = full_wv_a\n",
    "    data[key + '_b'] = full_wv_b\n",
    "\n",
    "# idf加权的句向量\n",
    "def idf_to_wv(model, data, idf):\n",
    "\n",
    "    full_wv_a = []\n",
    "    full_wv_b = []\n",
    "    # 每句话转词向量表达\n",
    "    for i in tqdm(range(len(data))):\n",
    "        row = data.iloc[i]\n",
    "        \n",
    "        wv_a = []\n",
    "        words_a = row['words_a']\n",
    "        \n",
    "        for i in words_a:\n",
    "            wv_a.append(model.wv[i] * idf[i])\n",
    "\n",
    "        full_wv_a.append(np.mean(wv_a, axis=0))\n",
    "            \n",
    "            \n",
    "        wv_b = []\n",
    "        words_b = row['words_b']\n",
    "        for i in words_b:\n",
    "            wv_b.append(model.wv[i] * idf[i])\n",
    "        \n",
    "        full_wv_b.append(np.mean(wv_b, axis=0))\n",
    "    data['idf_wv_a'] = full_wv_a\n",
    "    data['idf_wv_b'] = full_wv_b   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5c7ea340",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:31:51.368872Z",
     "start_time": "2022-03-08T08:31:51.343867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "      <th>words_a</th>\n",
       "      <th>words_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1560年10月，他在巴黎秘密会见了英国大使NicolasThrockmorton，要求他通...</td>\n",
       "      <td>1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...</td>\n",
       "      <td>0</td>\n",
       "      <td>[1560, 年, 10, 月, 巴黎, 秘密, 会见, 英国, 大使, NicolasTh...</td>\n",
       "      <td>[1560, 年, 10, 月, 巴黎, 秘密, 会见, 英国, 大使, 尼古拉斯, 斯罗,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1975年的NBA赛季76赛季是全美篮球协会的第30个赛季。</td>\n",
       "      <td>197576赛季的全国篮球协会是NBA的第30个赛季。</td>\n",
       "      <td>1</td>\n",
       "      <td>[1975, 年, NBA, 赛季, 76, 赛季, 全美, 篮球, 协会, 30, 赛季]</td>\n",
       "      <td>[197576, 赛季, 全国, 篮球, 协会, NBA, 30, 赛季]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>还有具体的讨论，公众形象辩论和项目讨论。</td>\n",
       "      <td>还有公开讨论，特定档案讨论和项目讨论。</td>\n",
       "      <td>0</td>\n",
       "      <td>[讨论, 公众形象, 辩论, 项目, 讨论]</td>\n",
       "      <td>[公开, 讨论, 特定, 档案, 讨论, 项目, 讨论]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>当可以保持相当的流速时，结果很高。</td>\n",
       "      <td>当可以保持可比较的流速时，结果很高。</td>\n",
       "      <td>1</td>\n",
       "      <td>[流速, 时, 很, 高]</td>\n",
       "      <td>[流速, 时, 很, 高]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
       "      <td>它是Akmola地区Zerendi区的所在地。</td>\n",
       "      <td>1</td>\n",
       "      <td>[Akmola, 地区, Zerendi, 区, 所在地]</td>\n",
       "      <td>[Akmola, 地区, Zerendi, 区, 所在地]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49396</th>\n",
       "      <td>我们的学校是精神和精神，热爱（时间路径）是我们的第一承诺。</td>\n",
       "      <td>我们的学校属于时间和精神，对Rehit的爱（精神之路）是我们的第一承诺。“”</td>\n",
       "      <td>0</td>\n",
       "      <td>[学校, 精神, 精神, 热爱, 时间, 路径, 第一, 承诺]</td>\n",
       "      <td>[学校, 时间, 精神, Rehit, 爱, 精神, 之路, 第一, 承诺]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49397</th>\n",
       "      <td>她于6月24日在科克，并于7月8日抵达。</td>\n",
       "      <td>她于6月24日在科克，并于7月8日抵达唐斯。</td>\n",
       "      <td>1</td>\n",
       "      <td>[6, 月, 24, 日, 科克, 并于, 7, 月, 8, 日, 抵达]</td>\n",
       "      <td>[6, 月, 24, 日, 科克, 并于, 7, 月, 8, 日, 抵达, 唐斯]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49398</th>\n",
       "      <td>CorneliaStuyvesantVanderbilt（George和EdithVande...</td>\n",
       "      <td>JohnJohnFACecil（George和CorneliaStuyvesantVande...</td>\n",
       "      <td>0</td>\n",
       "      <td>[CorneliaStuyvesantVanderbiltGeorge, EdithVand...</td>\n",
       "      <td>[JohnJohnFACecilGeorge, CorneliaStuyvesantVand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49399</th>\n",
       "      <td>第三季于2010年6月7日首播，第四季是混合情侣竞赛系统。</td>\n",
       "      <td>第四季于2010年6月7日首播。就像第三季一样，比赛系统是混合情侣。</td>\n",
       "      <td>0</td>\n",
       "      <td>[第三季, 2010, 年, 6, 月, 7, 日, 首播, 第四季, 混合, 情侣, 竞赛...</td>\n",
       "      <td>[第四季, 2010, 年, 6, 月, 7, 日, 首播, 第三季, 比赛, 系统, 混合...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49400</th>\n",
       "      <td>它也位于洛杉矶县大陆海岸的葡萄牙弯曲自然保护区，被称为加州帕洛斯佛得角半岛。</td>\n",
       "      <td>它也位于加利福尼亚大陆海岸的一个位置，位于洛杉矶县帕洛斯弗迪斯半岛的葡萄牙本德自然保护区。</td>\n",
       "      <td>0</td>\n",
       "      <td>[位于, 洛杉矶, 县, 大陆, 海岸, 葡萄牙, 弯曲, 自然保护区, 称为, 加州, 帕...</td>\n",
       "      <td>[位于, 加利福尼亚, 大陆, 海岸, 一个, 位置, 位于, 洛杉矶, 县, 帕洛斯, 弗...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49129 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text_a  \\\n",
       "0      1560年10月，他在巴黎秘密会见了英国大使NicolasThrockmorton，要求他通...   \n",
       "1                         1975年的NBA赛季76赛季是全美篮球协会的第30个赛季。   \n",
       "2                                   还有具体的讨论，公众形象辩论和项目讨论。   \n",
       "3                                      当可以保持相当的流速时，结果很高。   \n",
       "4                                它是Akmola地区Zerendi区的所在地。   \n",
       "...                                                  ...   \n",
       "49396                      我们的学校是精神和精神，热爱（时间路径）是我们的第一承诺。   \n",
       "49397                               她于6月24日在科克，并于7月8日抵达。   \n",
       "49398  CorneliaStuyvesantVanderbilt（George和EdithVande...   \n",
       "49399                      第三季于2010年6月7日首播，第四季是混合情侣竞赛系统。   \n",
       "49400             它也位于洛杉矶县大陆海岸的葡萄牙弯曲自然保护区，被称为加州帕洛斯佛得角半岛。   \n",
       "\n",
       "                                                  text_b  label  \\\n",
       "0      1560年10月，他在巴黎秘密会见了英国大使尼古拉斯·斯罗克莫顿，并要求他通过英格兰返回苏格...      0   \n",
       "1                            197576赛季的全国篮球协会是NBA的第30个赛季。      1   \n",
       "2                                    还有公开讨论，特定档案讨论和项目讨论。      0   \n",
       "3                                     当可以保持可比较的流速时，结果很高。      1   \n",
       "4                                它是Akmola地区Zerendi区的所在地。      1   \n",
       "...                                                  ...    ...   \n",
       "49396             我们的学校属于时间和精神，对Rehit的爱（精神之路）是我们的第一承诺。“”      0   \n",
       "49397                             她于6月24日在科克，并于7月8日抵达唐斯。      1   \n",
       "49398  JohnJohnFACecil（George和CorneliaStuyvesantVande...      0   \n",
       "49399                 第四季于2010年6月7日首播。就像第三季一样，比赛系统是混合情侣。      0   \n",
       "49400      它也位于加利福尼亚大陆海岸的一个位置，位于洛杉矶县帕洛斯弗迪斯半岛的葡萄牙本德自然保护区。      0   \n",
       "\n",
       "                                                 words_a  \\\n",
       "0      [1560, 年, 10, 月, 巴黎, 秘密, 会见, 英国, 大使, NicolasTh...   \n",
       "1         [1975, 年, NBA, 赛季, 76, 赛季, 全美, 篮球, 协会, 30, 赛季]   \n",
       "2                                 [讨论, 公众形象, 辩论, 项目, 讨论]   \n",
       "3                                          [流速, 时, 很, 高]   \n",
       "4                          [Akmola, 地区, Zerendi, 区, 所在地]   \n",
       "...                                                  ...   \n",
       "49396                   [学校, 精神, 精神, 热爱, 时间, 路径, 第一, 承诺]   \n",
       "49397              [6, 月, 24, 日, 科克, 并于, 7, 月, 8, 日, 抵达]   \n",
       "49398  [CorneliaStuyvesantVanderbiltGeorge, EdithVand...   \n",
       "49399  [第三季, 2010, 年, 6, 月, 7, 日, 首播, 第四季, 混合, 情侣, 竞赛...   \n",
       "49400  [位于, 洛杉矶, 县, 大陆, 海岸, 葡萄牙, 弯曲, 自然保护区, 称为, 加州, 帕...   \n",
       "\n",
       "                                                 words_b  \n",
       "0      [1560, 年, 10, 月, 巴黎, 秘密, 会见, 英国, 大使, 尼古拉斯, 斯罗,...  \n",
       "1                  [197576, 赛季, 全国, 篮球, 协会, NBA, 30, 赛季]  \n",
       "2                           [公开, 讨论, 特定, 档案, 讨论, 项目, 讨论]  \n",
       "3                                          [流速, 时, 很, 高]  \n",
       "4                          [Akmola, 地区, Zerendi, 区, 所在地]  \n",
       "...                                                  ...  \n",
       "49396             [学校, 时间, 精神, Rehit, 爱, 精神, 之路, 第一, 承诺]  \n",
       "49397          [6, 月, 24, 日, 科克, 并于, 7, 月, 8, 日, 抵达, 唐斯]  \n",
       "49398  [JohnJohnFACecilGeorge, CorneliaStuyvesantVand...  \n",
       "49399  [第四季, 2010, 年, 6, 月, 7, 日, 首播, 第三季, 比赛, 系统, 混合...  \n",
       "49400  [位于, 加利福尼亚, 大陆, 海岸, 一个, 位置, 位于, 洛杉矶, 县, 帕洛斯, 弗...  \n",
       "\n",
       "[49129 rows x 5 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebf23919",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:32:22.475923Z",
     "start_time": "2022-03-08T08:32:15.599645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 49129/49129 [00:06<00:00, 7478.68it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 7040.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# 最大池化句向量\n",
    "text_to_wv(wv_model, train, 'max_pooling','max_wv')\n",
    "text_to_wv(wv_model, test, 'max_pooling','max_wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "267f01a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:33:13.218497Z",
     "start_time": "2022-03-08T08:33:05.872061Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 49129/49129 [00:07<00:00, 6993.09it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 6665.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# 平均池化句向量\n",
    "text_to_wv(wv_model, train, 'mean_pooling','mean_wv')\n",
    "text_to_wv(wv_model, test, 'mean_pooling','mean_wv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b541a92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:33:23.040416Z",
     "start_time": "2022-03-08T08:33:13.805328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 49129/49129 [00:08<00:00, 5568.85it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 5247.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# idf加权平均句向量\n",
    "idf_to_wv(wv_model, train, idf)\n",
    "idf_to_wv(wv_model, test, idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "95ffc781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:33:36.648088Z",
     "start_time": "2022-03-08T08:33:26.899520Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 49129/49129 [00:08<00:00, 5524.35it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:00<00:00, 5286.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# sif词向量\n",
    "\n",
    "# 计算主成分，npc为需要计算的主成分的个数\n",
    "def compute_pc(X, npc):\n",
    "    svd = TruncatedSVD(n_components=npc, n_iter=5, random_state=0)\n",
    "    svd.fit(X)\n",
    "    return svd.components_\n",
    "\n",
    "\n",
    "# 去除主成分\n",
    "def remove_pc(X, npc=1):\n",
    "    pc = compute_pc(X, npc)\n",
    "    if npc == 1:\n",
    "        XX = X - X.dot(pc.transpose()) * pc\n",
    "    else:\n",
    "        XX = X - X.dot(pc.transpose()).dot(pc)\n",
    "    return XX\n",
    "\n",
    "# 更新词权重\n",
    "def sif_weight(count, a=3e-5):\n",
    "    # 统计所有词频\n",
    "    word_num = 0\n",
    "    for k,v in dict(count).items():\n",
    "        word_num += v\n",
    "    # 更新权重\n",
    "    sif = {}\n",
    "    for k,v in dict(count).items():\n",
    "        sif[k] = a / (a + v/word_num)\n",
    "    return sif\n",
    "\n",
    "# sif加权的句向量\n",
    "def sif_to_wv(model, data, sif):\n",
    "\n",
    "    full_wv_a = []\n",
    "    full_wv_b = []\n",
    "    # 每句话转词向量表达\n",
    "    for i in tqdm(range(len(data))):\n",
    "        row = data.iloc[i]\n",
    "        wv_a = []\n",
    "        words_a = row['words_a']\n",
    "        # 统计词向量\n",
    "        for i in words_a:\n",
    "            wv_a.append(model.wv[i] * sif[i])\n",
    "        # 记录结果\n",
    "        full_wv_a.append(np.mean(wv_a, axis=0))\n",
    "            \n",
    "            \n",
    "        wv_b = []\n",
    "        words_b = row['words_b']\n",
    "        for i in words_b:\n",
    "            wv_b.append(model.wv[i] * sif[i])\n",
    "        full_wv_b.append(np.mean(wv_b, axis=0))    \n",
    "    # 扣除第一主成分\n",
    "    full_wv_a = remove_pc(np.array(full_wv_a))\n",
    "    full_wv_b = remove_pc(np.array(full_wv_b))\n",
    "\n",
    "    data['sif_wv_a'] = list(full_wv_a)\n",
    "    data['sif_wv_b'] = list(full_wv_b)\n",
    "\n",
    "# 更新词权重\n",
    "sif = sif_weight(count)\n",
    "sif_to_wv(wv_model, train, sif)\n",
    "sif_to_wv(wv_model, test, sif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa1ea39d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:33:42.414739Z",
     "start_time": "2022-03-08T08:33:42.310729Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_wv_a</th>\n",
       "      <th>max_wv_b</th>\n",
       "      <th>mean_wv_a</th>\n",
       "      <th>mean_wv_b</th>\n",
       "      <th>idf_wv_a</th>\n",
       "      <th>idf_wv_b</th>\n",
       "      <th>sif_wv_a</th>\n",
       "      <th>sif_wv_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.10586922, 3.19763, 0.869081, 1.5040182, 1.9...</td>\n",
       "      <td>[0.6495797, 3.19763, 0.869081, 1.5040182, 1.98...</td>\n",
       "      <td>[-0.5706834, 0.3842089, -0.28422257, 0.1185452...</td>\n",
       "      <td>[-0.3717652, 0.31164643, -0.22815046, 0.202109...</td>\n",
       "      <td>[-2.2659285, 1.3924878, -0.99648845, 0.2462786...</td>\n",
       "      <td>[-1.2555038, 1.1040108, -1.0089166, 0.7555666,...</td>\n",
       "      <td>[-0.003121443, -0.010070443, -0.012975168, -0....</td>\n",
       "      <td>[0.03999223, -0.027868468, -0.037669625, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-0.25663307, 1.2137681, 1.4089204, 1.3859515,...</td>\n",
       "      <td>[-0.025835479, 1.2137681, 1.4089204, 1.3859515...</td>\n",
       "      <td>[-1.3674251, 0.46689332, 0.344108, 0.4812583, ...</td>\n",
       "      <td>[-1.483204, 0.5218263, 0.3872711, 0.33815396, ...</td>\n",
       "      <td>[-6.5485535, 2.4744925, 1.617031, 2.2298937, 7...</td>\n",
       "      <td>[-7.3397818, 2.6218858, 1.9737866, 1.4896796, ...</td>\n",
       "      <td>[-0.031989053, -0.00019693375, 0.0023369621, 0...</td>\n",
       "      <td>[-0.033897623, -0.0057500843, 0.007091727, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.07471452, 0.561395, 1.2111696, 0.17724654,...</td>\n",
       "      <td>[0.058350448, 0.6155718, 1.2111696, -0.0134789...</td>\n",
       "      <td>[-0.26919073, 0.35985965, 0.69544286, -0.14496...</td>\n",
       "      <td>[-0.31866485, 0.38964024, 0.72183657, -0.22135...</td>\n",
       "      <td>[-1.8132522, 2.332793, 4.360469, -0.83756256, ...</td>\n",
       "      <td>[-1.9721477, 2.4456043, 4.5141783, -1.4147881,...</td>\n",
       "      <td>[0.012638889, 0.0077733584, 0.075452946, -0.03...</td>\n",
       "      <td>[0.033889085, -0.0025461167, 0.07547951, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.1274478, 0.829338, 1.8819296, 1.0075006, 3...</td>\n",
       "      <td>[-0.1274478, 0.829338, 1.8819296, 1.0075006, 3...</td>\n",
       "      <td>[-1.7261013, -0.008885518, 0.84366155, 0.59414...</td>\n",
       "      <td>[-1.7261013, -0.008885518, 0.84366155, 0.59414...</td>\n",
       "      <td>[-7.7408185, 0.47007996, 3.9157085, 2.9004803,...</td>\n",
       "      <td>[-7.7408185, 0.47007996, 3.9157085, 2.9004803,...</td>\n",
       "      <td>[-0.029917978, -0.007971641, 0.026406996, 0.02...</td>\n",
       "      <td>[-0.030010313, -0.00794249, 0.026392894, 0.025...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0.26174036, 2.8882825, 0.9130776, -0.01055472...</td>\n",
       "      <td>[0.26174036, 2.8882825, 0.9130776, -0.01055472...</td>\n",
       "      <td>[-0.119692065, 0.68623275, 0.282225, -1.289105...</td>\n",
       "      <td>[-0.119692065, 0.68623275, 0.282225, -1.289105...</td>\n",
       "      <td>[-0.4849108, 2.8154886, 1.2123942, -5.3210306,...</td>\n",
       "      <td>[-0.4849108, 2.8154886, 1.2123942, -5.3210306,...</td>\n",
       "      <td>[0.029387828, 0.0056593996, -0.011569629, -0.0...</td>\n",
       "      <td>[0.029377524, 0.0056323744, -0.011605375, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            max_wv_a  \\\n",
       "0  [0.10586922, 3.19763, 0.869081, 1.5040182, 1.9...   \n",
       "1  [-0.25663307, 1.2137681, 1.4089204, 1.3859515,...   \n",
       "2  [-0.07471452, 0.561395, 1.2111696, 0.17724654,...   \n",
       "3  [-0.1274478, 0.829338, 1.8819296, 1.0075006, 3...   \n",
       "4  [0.26174036, 2.8882825, 0.9130776, -0.01055472...   \n",
       "\n",
       "                                            max_wv_b  \\\n",
       "0  [0.6495797, 3.19763, 0.869081, 1.5040182, 1.98...   \n",
       "1  [-0.025835479, 1.2137681, 1.4089204, 1.3859515...   \n",
       "2  [0.058350448, 0.6155718, 1.2111696, -0.0134789...   \n",
       "3  [-0.1274478, 0.829338, 1.8819296, 1.0075006, 3...   \n",
       "4  [0.26174036, 2.8882825, 0.9130776, -0.01055472...   \n",
       "\n",
       "                                           mean_wv_a  \\\n",
       "0  [-0.5706834, 0.3842089, -0.28422257, 0.1185452...   \n",
       "1  [-1.3674251, 0.46689332, 0.344108, 0.4812583, ...   \n",
       "2  [-0.26919073, 0.35985965, 0.69544286, -0.14496...   \n",
       "3  [-1.7261013, -0.008885518, 0.84366155, 0.59414...   \n",
       "4  [-0.119692065, 0.68623275, 0.282225, -1.289105...   \n",
       "\n",
       "                                           mean_wv_b  \\\n",
       "0  [-0.3717652, 0.31164643, -0.22815046, 0.202109...   \n",
       "1  [-1.483204, 0.5218263, 0.3872711, 0.33815396, ...   \n",
       "2  [-0.31866485, 0.38964024, 0.72183657, -0.22135...   \n",
       "3  [-1.7261013, -0.008885518, 0.84366155, 0.59414...   \n",
       "4  [-0.119692065, 0.68623275, 0.282225, -1.289105...   \n",
       "\n",
       "                                            idf_wv_a  \\\n",
       "0  [-2.2659285, 1.3924878, -0.99648845, 0.2462786...   \n",
       "1  [-6.5485535, 2.4744925, 1.617031, 2.2298937, 7...   \n",
       "2  [-1.8132522, 2.332793, 4.360469, -0.83756256, ...   \n",
       "3  [-7.7408185, 0.47007996, 3.9157085, 2.9004803,...   \n",
       "4  [-0.4849108, 2.8154886, 1.2123942, -5.3210306,...   \n",
       "\n",
       "                                            idf_wv_b  \\\n",
       "0  [-1.2555038, 1.1040108, -1.0089166, 0.7555666,...   \n",
       "1  [-7.3397818, 2.6218858, 1.9737866, 1.4896796, ...   \n",
       "2  [-1.9721477, 2.4456043, 4.5141783, -1.4147881,...   \n",
       "3  [-7.7408185, 0.47007996, 3.9157085, 2.9004803,...   \n",
       "4  [-0.4849108, 2.8154886, 1.2123942, -5.3210306,...   \n",
       "\n",
       "                                            sif_wv_a  \\\n",
       "0  [-0.003121443, -0.010070443, -0.012975168, -0....   \n",
       "1  [-0.031989053, -0.00019693375, 0.0023369621, 0...   \n",
       "2  [0.012638889, 0.0077733584, 0.075452946, -0.03...   \n",
       "3  [-0.029917978, -0.007971641, 0.026406996, 0.02...   \n",
       "4  [0.029387828, 0.0056593996, -0.011569629, -0.0...   \n",
       "\n",
       "                                            sif_wv_b  \n",
       "0  [0.03999223, -0.027868468, -0.037669625, -0.00...  \n",
       "1  [-0.033897623, -0.0057500843, 0.007091727, -0....  \n",
       "2  [0.033889085, -0.0025461167, 0.07547951, -0.06...  \n",
       "3  [-0.030010313, -0.00794249, 0.026392894, 0.025...  \n",
       "4  [0.029377524, 0.0056323744, -0.011605375, -0.0...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[['max_wv_a', 'max_wv_b', 'mean_wv_a', 'mean_wv_b', 'idf_wv_a',\n",
    "       'idf_wv_b', 'sif_wv_a', 'sif_wv_b']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "10d84b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-08T08:34:30.871062Z",
     "start_time": "2022-03-08T08:34:30.866061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0586922e-01,  3.1976299e+00,  8.6908102e-01,  1.5040182e+00,\n",
       "        1.9806354e+00, -9.7959228e-02,  7.2109714e-02,  2.5499647e+00,\n",
       "        8.3875918e-01,  8.7200344e-01,  1.4692152e+00, -9.0140373e-02,\n",
       "        9.6664131e-01,  1.8361543e+00,  2.7515805e+00,  1.8958132e+00,\n",
       "        1.4467839e-02,  2.4397149e+00,  6.4852113e-01,  2.9717381e+00,\n",
       "        1.1150166e+00,  1.4817405e+00,  1.7503228e+00,  1.4074761e+00,\n",
       "        3.0135412e+00,  2.6730828e+00,  8.9562899e-01,  1.8291825e-03,\n",
       "        1.2178862e+00,  1.0519068e+00,  1.9449003e+00,  1.9600861e-01,\n",
       "        1.9734142e+00, -5.2139156e-02,  3.0239861e+00,  3.2439635e+00,\n",
       "        3.2252681e+00,  8.9331657e-01,  2.6336231e+00,  1.6503750e-01,\n",
       "        7.8748679e-01,  2.3942892e-01,  1.2094382e+00,  2.1470168e+00,\n",
       "        5.5393434e-01, -2.8689490e-03,  3.3924723e-01,  5.7517117e-01,\n",
       "        2.4884396e+00,  6.8192053e-01,  2.3844910e+00,  6.4880383e-01,\n",
       "        1.4977551e+00,  1.9493902e+00,  1.2040800e+00,  2.1248360e+00,\n",
       "        3.2096500e+00,  1.6183609e+00,  2.2866987e-01,  1.6004553e+00,\n",
       "        2.8653318e-01,  1.6493994e+00,  1.8228570e+00,  1.4020123e+00,\n",
       "        6.4742404e-01,  2.3854449e+00,  2.3380661e+00,  2.1096208e+00,\n",
       "        5.7805574e-01,  4.2898407e+00,  1.1133350e+00,  4.5859671e-01,\n",
       "        1.4759450e+00,  1.0780449e+00,  2.8463163e+00,  2.5631311e+00,\n",
       "        2.0624404e+00,  2.2667398e+00,  2.4272203e+00,  7.0158702e-01,\n",
       "       -5.1370289e-02,  1.1232605e+00,  1.3567977e+00,  1.1829876e+00,\n",
       "        1.4048271e-01,  2.0221574e+00,  7.4864453e-01,  1.4296633e+00,\n",
       "        3.0227914e+00,  1.2792163e+00,  1.5981007e+00,  9.1338521e-01,\n",
       "        2.6052210e+00, -1.3374792e-02,  3.2856033e+00,  1.8851242e+00,\n",
       "        1.5906037e+00, -5.3454001e-02,  2.2522130e+00,  2.2599392e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['max_wv_a'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d768410",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
