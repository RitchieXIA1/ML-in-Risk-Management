{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6beeb0ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T02:01:42.957995Z",
     "start_time": "2022-01-13T02:01:40.399506Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "seed = 3407\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871fc026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T02:01:53.248819Z",
     "start_time": "2022-01-13T02:01:53.239797Z"
    }
   },
   "outputs": [],
   "source": [
    "class argparse():\n",
    "    pass\n",
    "\n",
    "args = argparse()\n",
    "args.epochs, args.learning_rate, args.patience = [30, 0.001, 4]\n",
    "args.hidden_size, args.input_size= [40, 30]\n",
    "args.device, = [torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9284147f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T02:02:14.729516Z",
     "start_time": "2022-01-13T02:02:14.722430Z"
    }
   },
   "outputs": [],
   "source": [
    "class Your_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Your_model, self).__init__()\n",
    "        pass\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pass\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da4ed36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T02:02:25.129555Z",
     "start_time": "2022-01-13T02:02:25.117551Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "    def __init__(self,patience=7,verbose=False,delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "    def __call__(self,val_loss,model,path):\n",
    "        print(\"val_loss={}\".format(val_loss))\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "        elif score < self.best_score+self.delta:\n",
    "            self.counter+=1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter>=self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss,model,path)\n",
    "            self.counter = 0\n",
    "    def save_checkpoint(self,val_loss,model,path):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path+'/'+'model_checkpoint.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f792e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-13T02:03:10.660391Z",
     "start_time": "2022-01-13T02:03:10.357324Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__load_data__() missing 1 required positional argument: 'csv_paths'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e4d2584c8b84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             .format(self.train_X.shape, self.train_Y.shape, self.valid_X.shape, self.valid_Y.shape))\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'valid'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-e4d2584c8b84>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, flag)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mflag\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'train'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'valid'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load_data__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __load_data__() missing 1 required positional argument: 'csv_paths'"
     ]
    }
   ],
   "source": [
    "#ÂÅödataset\n",
    "class Dataset_name(Dataset):\n",
    "    def __init__(self, flag='train'):\n",
    "        assert flag in ['train', 'test', 'valid']\n",
    "        self.flag = flag\n",
    "        self.__load_data__()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        pass\n",
    "\n",
    "    def __load_data__(self, csv_paths: list):\n",
    "        pass\n",
    "        print(\n",
    "            \"train_X.shape:{}\\ntrain_Y.shape:{}\\nvalid_X.shape:{}\\nvalid_Y.shape:{}\\n\"\n",
    "            .format(self.train_X.shape, self.train_Y.shape, self.valid_X.shape, self.valid_Y.shape))\n",
    "\n",
    "train_dataset = Dataset_name(flag='train')\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataset = Dataset_name(flag='valid')\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ff0583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Your_model().to(args.device)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(Your_model.parameters(),lr=args.learning_rate)\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_epochs_loss = []\n",
    "valid_epochs_loss = []\n",
    "\n",
    "early_stopping = EarlyStopping(patience=args.patience,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a215c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    Your_model.train()\n",
    "    train_epoch_loss = []\n",
    "    for idx,(data_x,data_y) in enumerate(train_dataloader,0):\n",
    "        data_x = data_x.to(torch.float32).to(args.device)\n",
    "        data_y = data_y.to(torch.float32).to(args.device)\n",
    "        outputs = Your_model(data_x)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(data_y,outputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_epoch_loss.append(loss.item())\n",
    "        train_loss.append(loss.item())\n",
    "        if idx%(len(train_dataloader)//2)==0:\n",
    "            print(\"epoch={}/{},{}/{}of train, loss={}\".format(\n",
    "                epoch, args.epochs, idx, len(train_dataloader),loss.item()))\n",
    "    train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "    \n",
    "    #=====================valid============================\n",
    "    Your_model.eval()\n",
    "    valid_epoch_loss = []\n",
    "    for idx,(data_x,data_y) in enumerate(valid_dataloader,0):\n",
    "        data_x = data_x.to(torch.float32).to(args.device)\n",
    "        data_y = data_y.to(torch.float32).to(args.device)\n",
    "        outputs = Your_model(data_x)\n",
    "        loss = criterion(outputs,data_y)\n",
    "        valid_epoch_loss.append(loss.item())\n",
    "        valid_loss.append(loss.item())\n",
    "    valid_epochs_loss.append(np.average(valid_epoch_loss))\n",
    "    #==================early stopping======================\n",
    "    early_stopping(valid_epochs_loss[-1],model=Your_model,path=r'c:\\\\your_model_to_save')\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    #====================adjust lr========================\n",
    "    lr_adjust = {\n",
    "            2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "            10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "        }\n",
    "    if epoch in lr_adjust.keys():\n",
    "        lr = lr_adjust[epoch]\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        print('Updating learning rate to {}'.format(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb1917d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lossÁöÑÂèØËßÜÂåñ\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(121)\n",
    "plt.plot(train_loss[:])\n",
    "plt.title(\"train_loss\")\n",
    "plt.subplot(122)\n",
    "plt.plot(train_epochs_loss[1:],'-o',label=\"train_loss\")\n",
    "plt.plot(valid_epochs_loss[1:],'-o',label=\"valid_loss\")\n",
    "plt.title(\"epochs_loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc720ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ê≠§Â§ÑÂèØÂÆö‰πâ‰∏Ä‰∏™È¢ÑÊµãÈõÜÁöÑDataloader„ÄÇ‰πüÂèØ‰ª•Áõ¥Êé•Â∞Ü‰Ω†ÁöÑÈ¢ÑÊµãÊï∞ÊçÆreshape,Ê∑ªÂä†batch_size=1\n",
    "Your_model.eval()\n",
    "predict = Your_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57734e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor.type())  # Êï∞ÊçÆÁ±ªÂûã\n",
    "print(tensor.size())  # Âº†ÈáèÁöÑshapeÔºåÊòØ‰∏™ÂÖÉÁªÑ\n",
    "print(tensor.dim())   # Áª¥Â∫¶ÁöÑÊï∞Èáè"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d1035",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Âú®Â∞ÜÂç∑ÁßØÂ±ÇËæìÂÖ•ÂÖ®ËøûÊé•Â±ÇÁöÑÊÉÖÂÜµ‰∏ãÈÄöÂ∏∏ÈúÄË¶ÅÂØπÂº†ÈáèÂÅöÂΩ¢ÂèòÂ§ÑÁêÜÔºå\n",
    "# Áõ∏ÊØîtorch.viewÔºåtorch.reshapeÂèØ‰ª•Ëá™Âä®Â§ÑÁêÜËæìÂÖ•Âº†Èáè‰∏çËøûÁª≠ÁöÑÊÉÖÂÜµ„ÄÇ\n",
    "tensor = torch.rand(2,3,4)\n",
    "shape = (6, 4)\n",
    "tensor = torch.reshape(tensor, shape)\n",
    "Êâì‰π±È°∫Â∫è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b8b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ê≥®ÊÑètorch.catÂíåtorch.stackÁöÑÂå∫Âà´Âú®‰∫étorch.catÊ≤øÁùÄÁªôÂÆöÁöÑÁª¥Â∫¶ÊãºÊé•Ôºå\n",
    "ËÄåtorch.stack‰ºöÊñ∞Â¢û‰∏ÄÁª¥„ÄÇ‰æãÂ¶ÇÂΩìÂèÇÊï∞ÊòØ3‰∏™10x5ÁöÑÂº†ÈáèÔºåtorch.catÁöÑÁªìÊûúÊòØ30x5ÁöÑÂº†ÈáèÔºå\n",
    "ËÄåtorch.stackÁöÑÁªìÊûúÊòØ3x10x5ÁöÑÂº†Èáè„ÄÇ\n",
    "'''\n",
    "tensor = torch.cat(list_of_tensors, dim=0)\n",
    "tensor = torch.stack(list_of_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorchÁöÑÊ†áËÆ∞ÈªòËÆ§‰ªé0ÂºÄÂßã\n",
    "tensor = torch.tensor([0, 2, 1, 3])\n",
    "N = tensor.size(0)\n",
    "num_classes = 4\n",
    "one_hot = torch.zeros(N, num_classes).long()\n",
    "one_hot.scatter_(dim=1, index=torch.unsqueeze(tensor, dim=1), src=torch.ones(N, num_classes).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Âà§Êñ≠‰∏§‰∏™Âº†ÈáèÁõ∏Á≠â\n",
    "torch.allclose(tensor1, tensor2)  # float tensor\n",
    "torch.equal(tensor1, tensor2)     # int tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06107c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ËÆ°ÁÆó‰∏§ÁªÑÊï∞ÊçÆ‰πãÈó¥ÁöÑ‰∏§‰∏§Ê¨ßÂºèË∑ùÁ¶ª Âà©Áî®broadcastÊú∫Âà∂\n",
    "dist = torch.sqrt(torch.sum((X1[:,None,:] - X2) ** 2, dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplcation: (m*n) * (n*p) * -> (m*p).\n",
    "result = torch.mm(tensor1, tensor2)\n",
    "\n",
    "# Batch matrix multiplication: (b*m*n) * (b*n*p) -> (b*m*p)\n",
    "result = torch.bmm(tensor1, tensor2)\n",
    "\n",
    "# Element-wise multiplication.\n",
    "result = tensor1 * tensor2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8423fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional neural network (2 convolutional layers)\n",
    "#‰∏Ä‰∏™Âç∑ÁßØÂ±ÇÁ§∫ÊÑè\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "model = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d8157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â¶ÇÊûúË¶ÅÂÆûÁé∞Á±ª‰ºº BN ÊªëÂä®Âπ≥ÂùáÁöÑÊìç‰ΩúÔºåÂú® forward ÂáΩÊï∞‰∏≠Ë¶Å‰ΩøÁî®ÂéüÂú∞ÔºàinplaceÔºâÊìç‰ΩúÁªôÊªëÂä®Âπ≥ÂùáËµãÂÄº„ÄÇ\n",
    "class BN(torch.nn.Module)\n",
    "    def __init__(self):\n",
    "        ...\n",
    "        self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "\n",
    "    def forward(self, X):\n",
    "        ...\n",
    "        self.running_mean += momentum * (current - self.running_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a94d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ËÆ°ÁÆóÊï¥‰ΩìÂèÇÊï∞Èáè\n",
    "num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0598ca11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Êü•ÁúãÁΩëÁªú‰∏≠ÁöÑÂèÇÊï∞\n",
    "#ÂèØ‰ª•ÈÄöËøámodel.state_dict()ÊàñËÄÖmodel.named_parameters()ÂáΩÊï∞Êü•ÁúãÁé∞Âú®ÁöÑÂÖ®ÈÉ®ÂèØËÆ≠ÁªÉÂèÇÊï∞ÔºàÂåÖÊã¨ÈÄöËøáÁªßÊâøÂæóÂà∞ÁöÑÁà∂Á±ª‰∏≠ÁöÑÂèÇÊï∞Ôºâ\n",
    "params = list(model.named_parameters())\n",
    "(name, param) = params[28]\n",
    "print(name)\n",
    "print(param.grad)\n",
    "print('-------------------------------------------------')\n",
    "(name2, param2) = params[29]\n",
    "print(name2)\n",
    "print(param2.grad)\n",
    "print('----------------------------------------------------')\n",
    "(name1, param1) = params[30]\n",
    "print(name1)\n",
    "print(param1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂèñÊ®°Âûã‰∏≠ÁöÑÂâç‰∏§Â±Ç\n",
    "new_model = nn.Sequential(*list(model.children())[:2] \n",
    "# Â¶ÇÊûúÂ∏åÊúõÊèêÂèñÂá∫Ê®°Âûã‰∏≠ÁöÑÊâÄÊúâÂç∑ÁßØÂ±ÇÔºåÂèØ‰ª•ÂÉè‰∏ãÈù¢ËøôÊ†∑Êìç‰ΩúÔºö\n",
    "for layer in model.named_modules():\n",
    "    if isinstance(layer[1],nn.Conv2d):\n",
    "         conv_model.add_module(layer[0],layer[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÈÉ®ÂàÜÂ±Ç‰ΩøÁî®È¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
    "#Ê≥®ÊÑèÂ¶ÇÊûú‰øùÂ≠òÁöÑÊ®°ÂûãÊòØ torch.nn.DataParallelÔºåÂàôÂΩìÂâçÁöÑÊ®°Âûã‰πüÈúÄË¶ÅÊòØ\n",
    "model.load_state_dict(torch.load('model.pth'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f274b839",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ê®°ÂûãÂØºÂÖ•ÂèÇÊï∞Êó∂ÔºåÂ¶ÇÊûú‰∏§‰∏™Ê®°ÂûãÁªìÊûÑ‰∏ç‰∏ÄËá¥ÔºåÂàôÁõ¥Êé•ÂØºÂÖ•ÂèÇÊï∞‰ºöÊä•Èîô„ÄÇÁî®‰∏ãÈù¢ÊñπÊ≥ïÂèØ‰ª•ÊääÂè¶‰∏Ä‰∏™Ê®°ÂûãÁöÑÁõ∏ÂêåÁöÑÈÉ®ÂàÜÂØºÂÖ•Âà∞Êñ∞ÁöÑÊ®°Âûã‰∏≠„ÄÇ\n",
    "# model_new‰ª£Ë°®Êñ∞ÁöÑÊ®°Âûã\n",
    "# model_saved‰ª£Ë°®ÂÖ∂‰ªñÊ®°ÂûãÔºåÊØîÂ¶ÇÁî®torch.loadÂØºÂÖ•ÁöÑÂ∑≤‰øùÂ≠òÁöÑÊ®°Âûã\n",
    "model_new_dict = model_new.state_dict()\n",
    "model_common_dict = {k:v for k, v in model_saved.items() if k in model_new_dict.keys()}\n",
    "model_new_dict.update(model_common_dict)\n",
    "model_new.load_state_dict(model_new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfe02ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂàÜÁ±ªÊ®°ÂûãËÆ≠ÁªÉ‰ª£Á†Å\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i ,(images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimizer\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Loss: {}'\n",
    "                  .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17668ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂàÜÁ±ªÊ®°ÂûãÊµãËØï‰ª£Á†Å\n",
    "# Test the model\n",
    "model.eval()  # eval mode(batch norm uses moving mean/variance \n",
    "              #instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test accuracy of the model on the 10000 test images: {} %'\n",
    "          .format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8372ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ëá™ÂÆö‰πâÊçüÂ§±ÂáΩÊï∞\n",
    "class MyLoss(torch.nn.Moudle):\n",
    "    def __init__(self):\n",
    "        super(MyLoss, self).__init__()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        loss = torch.mean((x - y) ** 2)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0080a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ê†áÁ≠æÂπ≥ÊªëÔºàlabel smoothingÔºâÂÜô‰∏Ä‰∏™label_smoothing.pyÁöÑÊñá‰ª∂ÔºåÁÑ∂ÂêéÂú®ËÆ≠ÁªÉ‰ª£Á†ÅÈáåÂºïÁî®ÔºåÁî®LSR‰ª£Êõø‰∫§ÂèâÁÜµÊçüÂ§±Âç≥ÂèØ„ÄÇlabel_smoothing.pyÂÜÖÂÆπÂ¶Ç‰∏ã\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSR(nn.Module):\n",
    "\n",
    "    def __init__(self, e=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.e = e\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def _one_hot(self, labels, classes, value=1):\n",
    "        \"\"\"\n",
    "            Convert labels to one hot vectors\n",
    "\n",
    "        Args:\n",
    "            labels: torch tensor in format [label1, label2, label3, ...]\n",
    "            classes: int, number of classes\n",
    "            value: label value in one hot vector, default to 1\n",
    "\n",
    "        Returns:\n",
    "            return one hot format labels in shape [batchsize, classes]\n",
    "        \"\"\"\n",
    "\n",
    "        one_hot = torch.zeros(labels.size(0), classes)\n",
    "\n",
    "        #labels and value_added  size must match\n",
    "        labels = labels.view(labels.size(0), -1)\n",
    "        value_added = torch.Tensor(labels.size(0), 1).fill_(value)\n",
    "\n",
    "        value_added = value_added.to(labels.device)\n",
    "        one_hot = one_hot.to(labels.device)\n",
    "\n",
    "        one_hot.scatter_add_(1, labels, value_added)\n",
    "\n",
    "        return one_hot\n",
    "\n",
    "    def _smooth_label(self, target, length, smooth_factor):\n",
    "        \"\"\"convert targets to one-hot format, and smooth\n",
    "        them.\n",
    "        Args:\n",
    "            target: target in form with [label1, label2, label_batchsize]\n",
    "            length: length of one-hot format(number of classes)\n",
    "            smooth_factor: smooth factor for label smooth\n",
    "\n",
    "        Returns:\n",
    "            smoothed labels in one hot format\n",
    "        \"\"\"\n",
    "        one_hot = self._one_hot(target, length, value=1 - smooth_factor)\n",
    "        one_hot += smooth_factor / (length - 1)\n",
    "\n",
    "        return one_hot.to(target.device)\n",
    "\n",
    "    def forward(self, x, target):\n",
    "\n",
    "        if x.size(0) != target.size(0):\n",
    "            raise ValueError('Expected input batchsize ({}) to match target batch_size({})'\n",
    "                    .format(x.size(0), target.size(0)))\n",
    "\n",
    "        if x.dim() < 2:\n",
    "            raise ValueError('Expected input tensor to have least 2 dimensions(got {})'\n",
    "                    .format(x.size(0)))\n",
    "\n",
    "        if x.dim() != 2:\n",
    "            raise ValueError('Only 2 dimension tensor are implemented, (got {})'\n",
    "                    .format(x.size()))\n",
    "\n",
    "\n",
    "        smoothed_target = self._smooth_label(target, x.size(1), self.e)\n",
    "        x = self.log_softmax(x)\n",
    "        loss = torch.sum(- x * smoothed_target, dim=1)\n",
    "\n",
    "        if self.reduction == 'none':\n",
    "            return loss\n",
    "\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(loss)\n",
    "\n",
    "        elif self.reduction == 'mean':\n",
    "            return torch.mean(loss)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('unrecognized option, expect reduction to be one of none, mean, sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffdb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÊàñËÄÖÁõ¥Êé•Âú®ËÆ≠ÁªÉÊ®°ÂûãÈáåÂÅö\n",
    "for images, labels in train_loader:\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "    N = labels.size(0)\n",
    "    # C is the number of classes.\n",
    "    smoothed_labels = torch.full(size=(N, C), fill_value=0.1 / (C - 1)).cuda()\n",
    "    smoothed_labels.scatter_(dim=1, index=torch.unsqueeze(labels, dim=1), value=0.9)\n",
    "\n",
    "    score = model(images)\n",
    "    log_prob = torch.nn.functional.log_softmax(score, dim=1)\n",
    "    loss = -torch.sum(log_prob * smoothed_labels) / N\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4131285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â≠¶‰π†ÁéáË°∞Âáè\n",
    "# Reduce learning rate when validation accuarcy plateau.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "for t in range(0, 80):\n",
    "    train(...)\n",
    "    val(...)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "# Cosine annealing learning rate.\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=80)\n",
    "# Reduce learning rate by 10 at given epochs.\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[50, 70], gamma=0.1)\n",
    "for t in range(0, 80):\n",
    "    scheduler.step()    \n",
    "    train(...)\n",
    "    val(...)\n",
    "\n",
    "# Learning rate warmup by 10 epochs.\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: t / 10)\n",
    "for t in range(0, 10):\n",
    "    scheduler.step()\n",
    "    train(...)\n",
    "    val(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7fcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÂæóÂà∞ÂΩìÂâçÂ≠¶‰π†Áéá\n",
    "\n",
    "# If there is one global learning rate (which is the common case).\n",
    "lr = next(iter(optimizer.param_groups))['lr']\n",
    "\n",
    "# If there are multiple learning rates for different layers.\n",
    "all_lr = []\n",
    "for param_group in optimizer.param_groups:\n",
    "    all_lr.append(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82d2be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#‰øùÂ≠òÂíåÂä†ËΩΩ\n",
    "#Ê≥®ÊÑè‰∏∫‰∫ÜËÉΩÂ§üÊÅ¢Â§çËÆ≠ÁªÉÔºåÊàë‰ª¨ÈúÄË¶ÅÂêåÊó∂‰øùÂ≠òÊ®°ÂûãÂíå‰ºòÂåñÂô®ÁöÑÁä∂ÊÄÅÔºå‰ª•ÂèäÂΩìÂâçÁöÑËÆ≠ÁªÉËΩÆÊï∞„ÄÇ\n",
    "start_epoch = 0\n",
    "# Load checkpoint.\n",
    "if resume: # resume‰∏∫ÂèÇÊï∞ÔºåÁ¨¨‰∏ÄÊ¨°ËÆ≠ÁªÉÊó∂ËÆæ‰∏∫0Ôºå‰∏≠Êñ≠ÂÜçËÆ≠ÁªÉÊó∂ËÆæ‰∏∫1\n",
    "    model_path = os.path.join('model', 'best_checkpoint.pth.tar')\n",
    "    assert os.path.isfile(model_path)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    print('Load checkpoint at epoch {}.'.format(start_epoch))\n",
    "    print('Best accuracy so far {}.'.format(best_acc))\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(start_epoch, num_epochs): \n",
    "    ... \n",
    "\n",
    "    # Test the model\n",
    "    ...\n",
    "\n",
    "    # save checkpoint\n",
    "    is_best = current_acc > best_acc\n",
    "    best_acc = max(current_acc, best_acc)\n",
    "    checkpoint = {\n",
    "        'best_acc': best_acc,\n",
    "        'epoch': epoch + 1,\n",
    "        'model': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }\n",
    "    model_path = os.path.join('model', 'checkpoint.pth.tar')\n",
    "    best_model_path = os.path.join('model', 'best_checkpoint.pth.tar')\n",
    "    torch.save(checkpoint, model_path)\n",
    "    if is_best:\n",
    "        shutil.copy(model_path, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7c09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
