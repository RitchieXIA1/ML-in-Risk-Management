{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23889e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a052811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "(32, 10, 64)\n"
     ]
    }
   ],
   "source": [
    ">>> model = tf.keras.Sequential()\n",
    ">>> model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))\n",
    ">>> # The model will take as input an integer matrix of size (batch,\n",
    ">>> # input_length), and the largest integer (i.e. word index) in the input\n",
    ">>> # should be no larger than 999 (vocabulary size).\n",
    ">>> # Now model.output_shape is (None, 10, 64), where `None` is the batch\n",
    ">>> # dimension.\n",
    ">>> input_array = np.random.randint(100,size=(32, 10))\n",
    ">>> model.compile('rmsprop', 'mse')\n",
    ">>> output_array = model.predict(input_array)\n",
    ">>> print(output_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d690631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff17d372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 32)\n"
     ]
    }
   ],
   "source": [
    ">>> input_shape = (4, 10, 128)\n",
    ">>> x = tf.random.normal(input_shape)\n",
    ">>> y = tf.keras.layers.Conv1D(\n",
    "... 32, 3, activation='relu',input_shape=input_shape[1:])(x)\n",
    ">>> print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8fa89f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0808bb9",
   "metadata": {},
   "source": [
    "只能embedding 0和1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8a3f31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 5.1000, 6.3000]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n",
    "embedding = torch.nn.Embedding.from_pretrained(weight)\n",
    " # Get embeddings for index 1\n",
    "input = torch.LongTensor([1])  ##表示取表第二个，即结果[4, 5.1, 6.3]\n",
    "embedding(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "620a1979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e284fd72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.3000, 3.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.LongTensor([0])  ##表示取表第二个，即结果[4, 5.1, 6.3]\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0051fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b1899ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 8, 16)\n",
      "(None, 2, 8, 8)\n"
     ]
    }
   ],
   "source": [
    ">>> layer = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n",
    ">>> target = tf.keras.Input(shape=[8, 16])\n",
    ">>> source = tf.keras.Input(shape=[8, 16])\n",
    ">>> output_tensor, weights = layer(target, source,\n",
    "...                                return_attention_scores=True)\n",
    ">>> print(output_tensor.shape)\n",
    ">>> print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce1d6850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 2, 8, 8) dtype=float32 (created by layer 'multi_head_attention_1')>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e85d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable-length int sequences.\n",
    "query_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "value_input = tf.keras.Input(shape=(None,), dtype='int32')\n",
    "\n",
    "# Embedding lookup.\n",
    "token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)\n",
    "# Query embeddings of shape [batch_size, Tq, dimension].\n",
    "query_embeddings = token_embedding(query_input)\n",
    "# Value embeddings of shape [batch_size, Tv, dimension].\n",
    "value_embeddings = token_embedding(value_input)\n",
    "\n",
    "# CNN layer.\n",
    "cnn_layer = tf.keras.layers.Conv1D(\n",
    "    filters=100,\n",
    "    kernel_size=4,\n",
    "    # Use 'same' padding so outputs have the same shape as inputs.\n",
    "    padding='same')\n",
    "# Query encoding of shape [batch_size, Tq, filters].\n",
    "query_seq_encoding = cnn_layer(query_embeddings)\n",
    "# Value encoding of shape [batch_size, Tv, filters].\n",
    "value_seq_encoding = cnn_layer(value_embeddings)\n",
    "\n",
    "# Query-value attention of shape [batch_size, Tq, filters].\n",
    "query_value_attention_seq = tf.keras.layers.Attention()(\n",
    "    [query_seq_encoding, value_seq_encoding])\n",
    "\n",
    "# Reduce over the sequence axis to produce encodings of shape\n",
    "# [batch_size, filters].\n",
    "query_encoding = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_seq_encoding)\n",
    "query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(\n",
    "    query_value_attention_seq)\n",
    "\n",
    "# Concatenate query and document encodings to produce a DNN input layer.\n",
    "input_layer = tf.keras.layers.Concatenate()(\n",
    "    [query_encoding, query_value_attention])\n",
    "\n",
    "# Add DNN layers, and create Model.\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a19c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
