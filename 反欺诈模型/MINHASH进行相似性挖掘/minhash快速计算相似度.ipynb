{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "464dc879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Jaccard for data1 and data2 is 0.7109375\n",
      "Actual Jaccard for data1 and data2 is 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash\n",
    "\n",
    "data1 = ['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "        'estimating', 'the', 'similarity', 'between', 'datasets']\n",
    "data2 = ['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "        'estimating', 'the', 'similarity', 'between', 'documents']\n",
    "\n",
    "m1, m2 = MinHash(), MinHash()\n",
    "for d in data1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in data2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "print(\"Estimated Jaccard for data1 and data2 is\", m1.jaccard(m2))\n",
    "\n",
    "s1 = set(data1)\n",
    "s2 = set(data2)\n",
    "actual_jaccard = float(len(s1.intersection(s2)))/float(len(s1.union(s2)))\n",
    "print(\"Actual Jaccard for data1 and data2 is\", actual_jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30044599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate neighbours with Jaccard similarity > 0.5 ['m3', 'm2']\n"
     ]
    }
   ],
   "source": [
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "set1 = set(['minhash', 'is', 'a', 'probabilistic', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'datasets'])\n",
    "set2 = set(['minhash', 'is', 'a', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "set3 = set(['minhash', 'is', 'probability', 'data', 'structure', 'for',\n",
    "            'estimating', 'the', 'similarity', 'between', 'documents'])\n",
    "\n",
    "m1 = MinHash(num_perm=128)\n",
    "m2 = MinHash(num_perm=128)\n",
    "m3 = MinHash(num_perm=128)\n",
    "for d in set1:\n",
    "    m1.update(d.encode('utf8'))\n",
    "for d in set2:\n",
    "    m2.update(d.encode('utf8'))\n",
    "for d in set3:\n",
    "    m3.update(d.encode('utf8'))\n",
    "\n",
    "# Create LSH index\n",
    "lsh = MinHashLSH(threshold=0.5, num_perm=128)\n",
    "lsh.insert(\"m2\", m2)\n",
    "lsh.insert(\"m3\", m3)\n",
    "result = lsh.query(m1)\n",
    "print(\"Approximate neighbours with Jaccard similarity > 0.5\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4ab1308",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from random import randint, seed, choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d05cdee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(output_path):\n",
    "    \"\"\"\n",
    "    :param output_path: 数据保存路径\n",
    "    :return: 格式为 id \\t attr1,attr2,attrn\n",
    "    \"\"\"\n",
    "    # n个属性\n",
    "    n_attr = 100\n",
    "    # 属性维度 2-1000个\n",
    "    n_attr_list = random.sample(range(2, 1000), n_attr)\n",
    "    # 样本个数\n",
    "    n_sample = 1000\n",
    "    ids = []\n",
    "    attrs = []\n",
    "\n",
    "    for i in range(n_sample):\n",
    "        ids.append('id_' + str(i))\n",
    "        attr = ['attr%d_%d' % (j, random.randint(1, n_attr_list[j])) for j in range(n_attr)]\n",
    "        attrs.append(attr)\n",
    "    # 簇个数\n",
    "    n_cluster = 4\n",
    "    # 簇最大的大小\n",
    "    max_cluster_size = 12\n",
    "    # 随机生成簇大小\n",
    "    cluster_size = random.sample(range(2, max_cluster_size), n_cluster)\n",
    "    for i in range(n_cluster):\n",
    "        for j in range(cluster_size[i]):\n",
    "            if j == 0:\n",
    "                attr = ['attr%d_%d' % (j, random.randint(1, n_attr_list[j])) for j in range(n_attr)]\n",
    "                attrs.append(attr)\n",
    "            else:\n",
    "                sim_attr = copy.deepcopy(attr)\n",
    "                # 随机篡改3-10个元素\n",
    "                for k in random.sample(range(n_attr), random.randint(3, 10)):\n",
    "                    sim_attr[k] = sim_attr[k] + str(random.randint(0, 1000))\n",
    "                attrs.append(sim_attr)\n",
    "            ids.append('c%d_%d_%d' % (i, cluster_size[i], j))\n",
    "    df = pd.DataFrame({'id': ids, 'attributes': [','.join(l) for l in attrs]})\n",
    "    df.to_csv(output_path, index=False, encoding=\"utf-8\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac61da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_set(input):\n",
    "    input = [list(l) for l in input]\n",
    "\n",
    "    lenth = len(input)\n",
    "    position = [-1 for _ in range(lenth)]\n",
    "    dict1 = {}\n",
    "    hash = {}\n",
    "    res = []\n",
    "    for i in range(lenth):\n",
    "        for j in input[i]:\n",
    "            if j not in dict1:  # 如果元素没有出现过，建立元素与所属集合的键值对\n",
    "                if i not in hash:  # 如果所属集合没有出现过。认为所属集合是父集合，建立连接关系\n",
    "                    hash[i] = i\n",
    "                    dict1[j] = i\n",
    "                else:\n",
    "                    dict1[j] = i\n",
    "            else:\n",
    "                temp = dict1[j]  # 如果元素已经出现过，建立父子集合之间的连接关系\n",
    "                hash[i] = temp  # 确保一个集合在同时有未出现过元素和已出现过元素的情况下，能够正确与之前集合建立联系\n",
    "    temphash = {}\n",
    "    for i in range(lenth):\n",
    "        j = i\n",
    "        while hash[j] != j:\n",
    "            j = hash[j]\n",
    "        position[i] = j  # 将子集合指向根集合\n",
    "        if j not in temphash:\n",
    "            temphash[j] = 1\n",
    "    for i in range(lenth):\n",
    "        if position[i] == i:\n",
    "            continue\n",
    "        else:\n",
    "            input[position[i]].extend(input[i])  # 合并集合\n",
    "            input[position[i]] = list(set(input[position[i]]))  # 合并后去重\n",
    "    for key in temphash:\n",
    "        res.append(input[key])  # temphash中存储的都是并查集的父集合（没有上一级的集合），根据temphash生成结果\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f4d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_num = 100  # 哈希函数个数 约高约精准\n",
    "band_size = 10  # LSH 哈希桶大小 r\n",
    "band_num = int(hash_num / band_size)  # LSH 哈希桶的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a84365",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsh = MinHashLSH(threshold=0.8, num_perm=hash_num, params=(band_num, band_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4af878c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'cluster_dataset.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f95794b",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_chunks = pd.read_csv(data_path, encoding='utf-8', iterator=True, chunksize=10000, delimiter='\\t',\n",
    "                              dtype={'id': str, 'attributes': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68c7df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in read_chunks:\n",
    "    chunk.fillna('')\n",
    "    for id, attribute in zip(chunk.id, chunk.attributes):\n",
    "        if isinstance(attribute, str):\n",
    "            attribute_list = attribute.split(',')\n",
    "            m = MinHash(num_perm=hash_num)\n",
    "            m.update_batch([s.encode('utf-8') for s in attribute_list])\n",
    "            lsh.insert(id, m)\n",
    "        else:\n",
    "            print('id=%s attr=%s' % (id, attribute))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9828c07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['c0_5_0', 'c0_5_3', 'c0_5_4', 'c0_5_1', 'c0_5_2'], ['c1_10_0', 'c1_10_6', 'c1_10_7', 'c1_10_8', 'c1_10_4'], ['c1_10_0', 'c1_10_1', 'c1_10_6', 'c1_10_3', 'c1_10_7', 'c1_10_8', 'c1_10_4', 'c1_10_9', 'c1_10_2'], ['c2_6_1', 'c2_6_5', 'c2_6_4', 'c2_6_3', 'c2_6_0', 'c2_6_2'], ['c3_2_0', 'c3_2_1']]\n"
     ]
    }
   ],
   "source": [
    "clusters_list = []\n",
    "for i, band_i in enumerate(lsh.hashtables):\n",
    "    for band_hash in band_i:\n",
    "        if len(band_i[band_hash]) > 1:\n",
    "            clusters_list.append(band_i[band_hash])\n",
    "    # 合并集合\n",
    "clusters_combination = merge_set(clusters_list)\n",
    "print(clusters_combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7516ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
